{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91548549-f872-4a66-baef-f87a396c44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6328b747-55c9-4edc-80a2-b5639f0be74e",
   "metadata": {
    "id": "6cdpG2ZoV1ig"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './'\n",
    "\n",
    "TRAIN_DIR = f'{DATA_DIR}/train'\n",
    "TEST_DIR = f'{DATA_DIR}/test'\n",
    "\n",
    "PLOTS_DIR = f'{DATA_DIR}/plots'\n",
    "\n",
    "OUTPUT_DIR = f'{DATA_DIR}/output'\n",
    "MODEL_CHECKPOINT_DIR = f'{DATA_DIR}/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582aecf4-5edb-4873-bf6f-283408af4214",
   "metadata": {},
   "source": [
    "## Base Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12be5e-18a3-41a1-8edf-1c11d355aa5c",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ac5e40-0217-46dc-b9c1-dbc2e041e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train = pd.read_csv(f'{TRAIN_DIR}/Train.csv')\n",
    "desc = pd.read_csv(f'{DATA_DIR}/DataDictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3225aa83-9537-4413-890b-356368f581d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CAT = desc[36:]['Column Name'].tolist()\n",
    "train_df = train.drop(columns=CAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c6953e-bc77-4002-817d-28998d1ca0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fossil.preprocessing import FossilPreprocessor, LabelEncoder\n",
    "from fossil.config import ModelsConfig\n",
    "\n",
    "np.random.seed(ModelsConfig.SEED)\n",
    "sku_encoder = LabelEncoder(train.sku_name.sample(frac=0.95, random_state=ModelsConfig.SEED).unique())\n",
    "\n",
    "fossil_preproc = FossilPreprocessor(sku_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baba318f-5ce5-47d4-bc62-8b5c552191b2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1991ed253846318dfd8ddd321eb365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f413a7473c4370b5f9bfd54d6d9249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dc5332ce684a9bb3e064dc951bc500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96394b1e53674942845cc652666fe715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c037a5b678d4f56abc6951ed10998ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a79406e22b47a1b5ececfa35d46972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff09014f97994f9f8aed5c02c941f0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2a6bdadeed4555b8026716f621eeed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_data = fossil_preproc.prepare_primary_data(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8787d673-2122-4f91-a4b1-f09b52d88022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAszUlEQVR4nO3deXxU9b3/8dc7CSFhB8Mmu4ICKgKmIO5rq63WrbeVWrXWltJKrb1t77X319v19t5ut62tVi5V3K4Xq1YtttS14FJcWGRfwx5AEtYEQtb5/P44JzqGkJxEJjOT+Twfj3lk5sw5Z94TwnzmfM/3+z0yM5xzzmWurGQHcM45l1xeCJxzLsN5IXDOuQznhcA55zKcFwLnnMtwOckO0FIFBQU2dOjQZMdwzrm0smjRot1m1rux59KuEAwdOpSFCxcmO4ZzzqUVSVuO9pw3DTnnXIbzQuCccxnOC4FzzmU4LwTOOZfhvBA451yGS1ghkDRTUomkFUd5XpJ+K6lI0jJJ4xOVxTnn3NEl8ojgQeCyJp6/HBgR3qYA9yYwi3POuaNI2DgCM3tV0tAmVrkKeNiCebDflNRDUn8z25moTM65tmVmVNfFqKqNUVUTo6q2jsqaGNW1wf2q2uB+TV39zaipi1FbZ9SZURd7/xaz+hvUxQwzwwwMiIX3P/Da74do43edOIVDe3HeSY2OCftQkjmgbACwLe5xcbjsiEIgaQrBUQODBw9uk3DOuUB1bYx9FdXsPljFvkM17K2oZt+havaFPw8cruHA4RrKKms5cLiGiqpaKmtjVNbUcbimLiU+h6VkJzg2pp5/YrsrBI390zT6J2NmM4AZAIWFhSnwZ+VcejMz9lXUsKuskl1llZSUVbGrrJLSg1XsPljF7vLq9+6XV9YedT/d8nLo3qkD3fODW5+uXeiUm0N+bhZ5Odnk52bTMSeLvA7Bz471P3Pqf2aRG95ysrLIzRE5WVnkZIvsLJGt8GeWyMoSWQqWSZAV/5Pgw17t5RO/jSWzEBQDg+IeDwR2JCmLc+1G/Yf8jv2H378dqGTngUp2HahkZ9lhdpVVUV0bO2Lb7vkdKOiSS++uHTnl+G4UdOlIr8659OqcS0GXXHp2Cu737JxLj/wO5GR7x8P2IJmFYDYwTdJjwETggJ8fcK55sZhRUl5F8b4Ktu8/TPG+4LZ9/2G276tgx/5KDtfUfWCb3Jws+nfPo2+3PMYP7km/bsH9ft3z6NutI3265tG7a0fyOmQn6V25ZEpYIZA0C7gAKJBUDHwf6ABgZtOBOcDHgSKgArglUVmcSzd1MWP7vsNs3H2QzbsPsXlPBVv3VrB5zyGK9x6muu6D3+Z7dc5lQI98RvTpygUn9+H4HvkMCG/9e+RxXOdcbzZxR5XIXkOTm3negNsS9frOpYNYzNi6t4I175az5t0y1uwsp6j0IFv3VHzgw75zbjaDj+vMyX27cunovgzq2YkBPfMZ1DOf43vk0yk37SYSdinE/3qcayMV1bWs3lnGqp3lrN5ZxuqdZax9t5yK6qAZR4Khx3VmeJ8uXDyqDycUdGZYQReGFnSid5eO/o3eJYwXAucSoC5mrN5ZxuKt+1hWfIDlxQdYX1JOLOzz1i0vh1H9u/HpwkGM6t+Vkf26MaJvF/9m75LC/+qcOwbMjKKSg8zfsIf5G3bz5sa9HDhcA8BxnXMZM7A7Hzu1H6cN6M7o47txfPc8/4bvUoYXAudaaX9FNa8X7ebVdaW8tn43Ow9UAjCwZz6XndKPs4YfR+HQXv6h71KeFwLnIqqLGUu27efVdaW8sq6UZcX7iRl0zcvhnOEF3H5xb84ZXsCgXp2SHdW5FvFC4FwTqmrreGVtKX9ZtpNX1pVy4HANEpw+sAfTLhrB+ScVcPrAHj6wyqU1LwTONVAXM/5RtJtnl+7guZXvUl5ZS89OHbh0dF/OPyn41t+zc26yYzp3zHghcC60sfQgTywq5qnFxewqq6Jrxxw+eko/rjy9P2cPL6CDf+t37ZQXApfRKmvqeHbpDh5bsI1FW/aRJbjg5D58/8qBXDSyj0+54DKCFwKXkbbuqeDRt7bwx4Xb2F9Rw4m9O3Pn5SO5dtwA+nTLS3Y859qUFwKXMcyM19bv5sH5m5m7toQsiY+O7suNk4Yw6YTjvIuny1heCFy7V1Fdy58Wb+eh+ZspKjlIQZdcvnbhcCZPHEz/7vnJjudc0nkhcO3Wjv2HeWj+Zma9vZWyylpOG9CdX336dD4xpj8dc7zt37l6Xghcu7N46z5mvr6Jv614FzPj8lP7c8vZQzljSE9v/nGuEV4IXLsQixkvrt7FjFc3smjLPrrm5XDrOcO4adIQBvb0kb7ONcULgUtrlTV1PP3Odv7w2kY2lh5iYM98vn/laP6pcBBdOvqft3NR+P8Ul5Yqa+qY9fZW7p23gZLyKk4d0I3fTR7H5af28+kenGshLwQurVTV1vHHBdu4Z24Ru8qqmDisF7/+zFjOOtG7fzrXWpEKgaQhwAgze0lSPpBjZuWJjebc+2rqYjy5qJjfvryenQcq+cjQnmEBKEh2NOfSXrOFQNKXgClAL+BEYCAwHbg4wraXAXcB2cB9ZvbTBs/3BGaG+60EvmBmK1r4Hlw7FosZzy7bwa9eXMeWPRWMG9yDX3zqdM4e7kcAzh0rUY4IbgMmAG8BmNl6SX2a20hSNnAPcClQDCyQNNvMVsWt9m/AEjO7RtLIcP1mC4xr/8yMF1ft4r9fWMfaXeWM6t+N+28u5KKRfbwAOHeMRSkEVWZWXf+fT1IOYBG2mwAUmdnGcLvHgKuA+EIwGvgvADNbI2mopL5mtqsF78G1M/OLdvPz59eyZNt+hhV05neTx/GJ0/qTleUFwLlEiFIIXpH0b0C+pEuBrwLPRthuALAt7nExMLHBOkuBa4HXJU0AhhA0PX2gEEiaQtA8xeDBgyO8tEtHS7ft5+fPr+EfRXvo3z2Pn113GteNH+i9gJxLsCiF4E7gVmA58GVgDnBfhO0a+/rW8Ejip8BdkpaE+38HqD1iI7MZwAyAwsLCKEcjLo2UlFfys7+t5U+Li+nVOZd/v2I0N0wc7FNAO9dGohSCfGCmmf0B3mv7zwcqmtmuGBgU93ggsCN+BTMrA24J9ytgU3hzGaCmLsZD8zfzm5fWU1Vbx9TzT2TaRcN9IJhzbSzK/7iXgUuAg+HjfOAF4KxmtlsAjJA0DNgOXA98Nn4FST2ACjOrBr4IvBoWB9fOvblxD//+zArWlxzk/JN68/0rR3NC7y7JjuVcRopSCPLMrL4IYGYHJTU7eYuZ1UqaBjxP0H10ppmtlDQ1fH46MAp4WFIdwUnkW1vzJlz62Huomv+cs5onFxUzsGc+f7ipkEtGeU8g55IpSiE4JGm8mS0GkHQGcDjKzs1sDsE5hfhl0+PuvwGMiB7XpSsz40+Lt/OTv66ivLKWr1xwIrdfNIL8XD8P4FyyRSkEdwBPSKpv3+8PfCZhiVy7s3n3Ib7z1HLe2LiHwiE9+ck1p3Fyv67JjuWcCzVbCMxsQTjY62SCnkBrzKwm4clc2quti3Hf65v49YvryM3J4j+vOY3rPzLIxwM4l2Kids/4CDA0XH+cJMzs4YSlcmlvxfYD3PnUMlZsL+Njp/TlR1edSl+/KLxzKSnKXEOPEMwFtASoCxcb4IXAHaG6Nsbv/r6e38/bQK/Oudx7w3guP61/smM555oQ5YigEBhtZj6QyzVpxfYDfOuJpax5t5xrxw/g+1ecQvdOHZIdyznXjCiFYAXQD9iZ4CwuTVXXxrh7bhG/n1tEr8653H9zIReP6pvsWM65iKIUggJglaS3gar6hWb2yYSlcmljy55D3D7rHZYWH+DacQP43pWj6dEpN9mxnHMtEKUQ/CDRIVx6evqdYr779Aqys+TnApxLY1G6j77SFkFc+jhYVcv3nlnBU+9s5yNDe/Kb68cxoEd+smM551opSq+hM4HfEUwHkUswXcQhM+uW4GwuBRWVHGTKIwvZvPsQd1wygmkXDvdpop1Lc1Gahu4mmDDuCYIeRDfh00JkpJdW7eKOPy6hY04Wj37xTCadeFyyIznnjoFIA8rMrEhStpnVAQ9Imp/gXC6FxGLG3XOL+NWL6zh1QDf+58ZCbwpyrh2JUggqJOUCSyT9nKAbaefExnKp4lBVLd98fCnPrXyXa8YN4L+uPc0vGONcOxOlENxIcF5gGvANgovNXJfIUC41bNtbwZceXsi6XeV89xOjuPWcYT5dtHPtUJReQ1vCu4eBHyY2jksV8zfs5rZHF1MXMx76wgTOHdE72ZGccwly1EIg6XEz+7Sk5Rx5rWHMbExCk7mkMDMeeXMLP3x2FcMKOvOHmwoZVuAtgc61Z00dEXw9/HlFWwRxyVcXM3747EoefmMLF43sw13Xj6Vrns8V5Fx7d9RCYGY7wwvV329ml7RhJpcEFdW13D7rHV5aXcKXzh3GnZePItuvG+BcRmjyHIGZ1UmqkNTdzA60VSjXtkrLq7j1oQWs2H6AH111CjdNGprsSM65NhSl11AlsFzSi8Ch+oVmdntzG0q6DLiLoNfRfWb20wbPdwf+FxgcZvmlmT0QPb77sIpKDvL5B95mz8FqZtxYyCWjfdZQ5zJNlELw1/DWImGz0j3ApUAxsEDSbDNbFbfabcAqM7tSUm9graRHzay6pa/nWm7F9gPceP9bZGeJP375TMYM7JHsSM65JIjSffShVu57AlBkZhsBJD0GXAXEFwIDuironN4F2AvUtvL1XAss3rqPm2e+Tbe8Djz6xYkM9Z5BzmWsKJPOjQD+CxgNvHfRWTM7oZlNBwDb4h4XAxMbrHM3MBvYAXQFPmNmsUYyTAGmAAwePLi5yK4Zb27cw60PLqCga0ce/eJEBvbslOxIzrkkijJt5APAvQTf1C8kuFbxIxG2a6zLScPxCB8juBby8cBY4G5JR8xqamYzzKzQzAp79/aBTR/GK+tK+fwDb9O/Rz6Pf3mSFwHnXKRCkG9mLwMysy1m9gPgogjbFRNMR1FvIME3/3i3AE9ZoAjYBIyMsG/XCm9u3MOXHlrIsIIu/HHKmfTtltf8Rs65di9KIaiUlAWslzRN0jVAnwjbLQBGSBoWTlp3PUEzULytwMUAkvoCJwMbI6d3kRWVlDPl4YUM6pXPrC9N5LguHZMdyTmXIqL0GroD6ATcDvyYoHno5uY2MrNaSdOA5wm6j840s5WSpobPTw/392A4jYWAfzWz3a15I+7oSsoruXnmAnJzsnnwlgl+TWHn3AdEKQS1ZnYQOEjQlBOZmc0B5jRYNj3u/g7goy3Zp2uZQ1W1fOHBBew9VM3jX57EoF5+TsA590FRmoZ+JWmNpB9LOiXhidwxU1sX42uz3mHVjjLuuWEcpw3snuxIzrkU1GwhMLMLgQuAUmCGpOWSvpvoYO7D+4+/rubva0r48dWnctFIHzHsnGtcpKuOm9m7ZvZbYCpBd8/vJTKU+/AeX7iNB+dv5tZzhnHDxCHJjuOcS2HNFgJJoyT9QNIKggFg8wm6groU9c7WfXz36RWcM7yA71zuvXGdc02LcrL4AWAW8NHw5K5LYbvKKvnyI4vo270jv5s8jpzsSAd9zrkMFmWuoTPbIoj78Kpq65j6v4s4WFXLw7eeRc/O3k3UOde8KEcELg2YGd97ZiXvbN3PvTeMZ2S/I2bqcM65Rnm7QTvxf29v5Y8LtzHtwuFcflr/ZMdxzqURLwTtwOKt+/jB7JWcf1JvvnHpScmO45xLM0dtGpL0LEfOFvoeM/tkQhK5Fiktr+Kr/7uYft3zuOv6sX6dYedcizV1juCX4c9rgX4El5QEmAxsTmAmF1FNXYxp/7eY/YereeorZ/scQs65VjlqITCzVwAk/djMzot76llJryY8mWvWT/+2hrc27eXXnzmd0cf7yWHnXOtEOUfQW9J7VyOTNAzwq8Mk2ZzlO7n/9U18/qyhXDPOx/c551ovSvfRbwDzJNVfJ2Ao8OWEJXLN2rqngn99chmnD+rBv318VLLjOOfSXJQBZc+F1y2un6tgjZlVJTaWO5rq2hhfm7UYBHdPHkdujnf8cs59OFHmGuoEfBuYZmZLgcGSrkh4Mteonz23hqXFB/jFp8b4tQWcc8dE1IvXVwOTwsfFwH8kLJE7qhdX7eL+1zdx86QhXHaqDxpzzh0bUQrBiWb2c6AGwMwOE1xW0rWh7fsP860nlnLK8d34jp8XcM4dQ1EKQbWkfMLBZZJOBCKdI5B0maS1kook3dnI89+WtCS8rZBUJ6lXi95BBojFjG8+voTauhh3f3Y8eR2ykx3JOdeORCkE3weeAwZJehR4GfiX5jaSlA3cA1wOjAYmSxodv46Z/cLMxprZWOA7wCtmtrdlb6H9e+iNzby5cS/fu3I0wwo6JzuOc66didJr6EVJi4EzCZqEvm5muyPsewJQZGYbASQ9BlwFrDrK+pMJrnvg4mwoPchP/7aGi0b24dOFg5IdxznXDkXte5gH7APKgNGSzmtmfYABwLa4x8XhsiOEPZMuA/4UMU9GqK2L8c3Hl5LXIZufXnsakp+acc4de80eEUj6GfAZYCUQCxcb0Nw0E419ah1tErsrgX8crVlI0hRgCsDgwYObi9xu/M+rG1mybT+/nTyOPt3ykh3HOddORRlZfDVwcisGkRUD8W0ZA4GjXeryeppoFjKzGcAMgMLCwqPOiNqerNpRxm9eWscnxvTnk6cfn+w4zrl2LErT0EagQyv2vQAYIWmYpFyCD/vZDVeS1B04H/hzK16jXaqLGd9+cind83P58VWnJjuOc66di3JEUAEskfQycd1Gzez2pjYys1pJ04DngWxgppmtlDQ1fH56uOo1wAtmdqg1b6A9enLRNlbuKON3k8fRy6877JxLsCiFYDaNfJOPwszmAHMaLJve4PGDwIOt2X97dLCqll88v44zhvTkijE+etg5l3hRuo8+1BZBXOD3c4vYfbCK+24u9F5Czrk20dSlKh83s09LWk4jvX3MbExCk2WgbXsruO/1TVwzbgBjB/VIdhznXIZo6ojg6+FPn2m0jfzsuTVkCf7lspOTHcU5l0GaulTlzvDnlraLk7kWbdnLX5bt5PaLR9C/e36y4zjnMkiU6xGcKWmBpIOSqsOJ4craIlymiMWMH/1lNX27dWTq+Sc0v4Fzzh1DUcYR3E0wD9B6IB/4IvC7RIbKNH9b8S5Lt+3nWx89mU65UTpyOefcsRPpU8fMiiRlm1kd8ICk+QnOlTHqYsavX1rHiD5duHa8X4TeOdf2Ig0oC0cGL5H0c2An4HMhHyPPLt1BUclB7vnseLKzvLuoc67tRWkaupFgZPA04BDB/EHXJTJUpqiti/Gbl9Yxsl9XLj+1X7LjOOcyVJQBZfW9hg4DP0xsnMzy1OLtbN5TwYwbzyDLjwacc0nS1ICyRgeS1fMBZR9OdW2Mu15ez5iB3bl0dN9kx3HOZbCmjgh8IFkCPb5wG9v3H+Yn15zqU0k455KqqQFl7w0kk9SP4NKTBiwws3fbIFu7VVlTx91/L+KMIT05/6TeyY7jnMtwUQaUfRF4G7gW+BTwpqQvJDpYezbr7a28W1bJNy89yY8GnHNJF6X76LeBcWa2B0DSccB8YGYig7VXlTV13DtvAxOH9eKs4QXJjuOcc5G6jxYD5XGPy/ngReldC8x6eysl5VXccclJyY7inHNAtCOC7cBbkv5McI7gKuBtSf8MYGa/SmC+diX+aGDSicclO45zzgHRCsGG8Fav/trCXY99nPbtsfBo4DfXj012FOece0+UQvAzM6uMXyCpwMx2JyhTu1RZU8e9r2xgwrBeTDrBjwacc6kjyjmCtyWdWf9A0nUEJ4ubJekySWslFUm68yjrXCBpiaSVkl6JFjv9/HHBNnaVVXHHxSO8p5BzLqVEOSK4AZgpaR5wPHAccFFzG0nKBu4BLiU44bxA0mwzWxW3Tg/g98BlZrZVUp8Wv4M0UFlTx+/nFTFhqJ8bcM6lnihzDS2X9BPgEYIeQ+eZWXGEfU8AisxsI4CkxwhONK+KW+ezwFNmtjV8rZIW5k8Ljy8MjgZ+9emxfjTgnEs5UQaU3Q/cAYwBbgGelXRbhH0P4IPdTIvDZfFOAnpKmidpkaSbjpJhiqSFkhaWlpZGeOnUUVUb9BQqHNKTs/xowDmXgqKcI1gBXGhmm8zseeBMYHyE7Rr76ttwErsc4AzgE8DHgH+XdEQHezObYWaFZlbYu3d6Tcnw1OLt7DxQye1+bsA5l6KaLQRm9mtgsKRLwkXVBEcIzSkmuHZBvYHAjkbWec7MDoW9kF4FTo+w77RQWxfj3nkbGDOwO+eO8FHEzrnUFKVp6EvAk8D/hIsGAs9E2PcCYISkYeEVzq4HZjdY58/AuZJyJHUCJgKrI2ZPec8u28HWvRVMu3C4Hw0451JWlF5DtxGc+H0LwMzWR+ndY2a1kqYBzxNc4Wymma2UNDV8frqZrZb0HLAMiAH3mdmKVr6XlBKLGffM3cDIfl25ZJRfb8A5l7qiFIIqM6uu/0YrKYcmLlgTz8zmAHMaLJve4PEvgF9ESptGnl/5LkUlB/nt5HF+9THnXEqLcrL4FUn/BuRLuhR4Ang2sbHSm5lx99wihhV05hOn9U92HOeca1KUQnAnUAosB75M8A3/u4kMle7mrS1l5Y4yvnLBiWT70YBzLsVFGVAWA/4Q3lwzzIzf/X09A3rkc824hsMmnHMu9UQ5InAt8NamvSzeup+p559Ah2z/9TrnUp9/Uh1jM1/fRM9OHfinwkHNr+yccykgciGQ1DmRQdqDrXsqeHH1Lm6YOIS8DtnJjuOcc5FEGVB2lqRVhAO9JJ0u6fcJT5aGHpy/mWyJGycNSXYU55yLLMoRwa8J5gHaA2BmS4HzEhkqHZVX1vD4wm1cMaY/fbvlJTuOc85FFqlpyMwaXqy+LgFZ0toTC4s5WFXLF84ZluwozjnXIlFGFm+TdBZg4ZxBt9OO5gM6FupixkNvbKZwSE/GDOyR7DjOOdciUY4IphLMNzSAYLbQseFjF/r7mhK27KnwowHnXFqKckQgM7sh4UnS2MzXNzGgRz4fHe2Tyznn0k+UI4L5kl6QdGt4jWEXZ9WOMt7YuIebJg0hxweQOefSUJQL04wgmFvoFGCxpL9I+lzCk6WJB+dvIr9DNtd/ZHCyozjnXKtE7TX0tpn9M8F1CfYCDyU0VZo4UFHDn5fs4OpxA+jeqUOy4zjnXKtEGVDWTdLNkv4GzAd2EhSEjPfEom1U1cb43Jl+NOCcS19RThYvJbg05Y/M7I3ExkkfsZjx6FtbOWNIT045vnuy4zjnXKtFKQQnmFmkK5Jlkvkb9rBp9yG+fvGIZEdxzrkP5aiFQNJvzOwOYLakIwqBmX0ykcFS3SNvbqZX51wuP61fsqM459yH0tQRwSPhz1+2dueSLgPuIrh4/X1m9tMGz18A/BnYFC56ysx+1NrXays7DxzmxVW7+PL5J9Ixx2cZdc6lt6MWAjNbFN4da2Z3xT8n6evAK03tWFI2cA9wKcGI5AWSZpvZqgarvmZmV7Q4eRLNemsrBnx2gp8kds6lvyjdR29uZNnnI2w3ASgys41mVg08BlzVgmwpqbo2xqwF27jw5D4M6tUp2XGcc+5Da+ocwWTgs8AwSbPjnupKOCV1MwYA8bOWFgMTG1lvkqSlwA7gW2a2spEsU4ApAIMHJ/db+Aur3qW0vIobz/RrDjjn2oemzhHUjxkoAP47bnk5sCzCvtXIsoYnnRcDQ8zsoKSPE3RTPaIbjpnNAGYAFBYWJrUH0yNvbGFQr3zOO6l3MmM459wx09Q5gi3AFmBSK/ddDMRfuHcgwbf++Ncoi7s/R9LvJRWY2e5WvmZCbdp9iLc27eXbHzuZ7KzG6pxzzqWfKCOLz5S0QNJBSdWS6iSVNbcdsAAYIWlYeB2D64H4JiYk9ZOk8P6EME+UZqekeHLRNrIEnzpjYLKjOOfcMRNlQNndBB/iTwCFwE3A8OY2MrNaSdOA5wm6j840s5WSpobPTwc+BXxFUi1wGLg+VQev1cWMPy3aznkn9fZLUTrn2pUohQAzK5KUbWZ1wAOS5kfcbg4wp8Gy6XH37yYoNCnv9aLdvFtWyfeuHJ3sKM45d0xFKQQVYdPOEkk/JziB3DmxsVLPEwu30aNTBy4e1SfZUZxz7piKMo7gRoKmnWnAIYITwNclMlSqOVBRwwurdnH12AE+ktg51+40e0QQ9h6CoA3/h4mNk5pmL91OdW3MTxI759qlpgaULefIfv/vMbMxCUmUgp5YVMyo/t04dYBPN+2ca3+aOiJIq/l/EmXtu+UsKz7A967wk8TOufapuQFlGe+JhdvokC2uHjcg2VGccy4hmj1HIKmc95uIcoEOwCEz65bIYKmgpi7GM0u2c/HIvvTqnJvsOM45lxBRThZ3jX8s6Woy5JrFr60vZffBav6p0E8SO+faryjdRz/AzJ4BLjr2UVLPi6t20aVjDueO8AnmnHPtV5SmoWvjHmYRTDORktNAHEuxmPHy6hLOP6k3uTktrpfOOZc2oowsvjLufi2wmXZwgZnmLN9+gJLyKi4Z7SOJnXPtW5RzBLe0RZBU89LqXWRniQtP9kLgnGvfojQNDQO+BgyNX9/MPpm4WMn30uoSzhjSkx6dvLeQc659i9I09AxwP/AsEEtomhRRvK+C1TvL+H8fH5XsKM45l3BRCkGlmf024UlSyMurSwC4ZHTfJCdxzrnEi1II7pL0feAFoKp+oZktTliqJHtp9S5O6N2ZYQUZN9u2cy4DRSkEpxFMRX0R7zcNGe10LEF5ZQ1vbtzDF84eluwozjnXJqIUgmuAE8ysOtFhUsGr63ZTU2feLOScyxhRRkotBXokOEfKeGn1Lnp26sD4wT2THcU559pElELQF1gj6XlJs+tvUXYu6TJJayUVSbqzifU+IqlO0qeiBk+E2roYc9eWcOHIPmRnKZlRnHOuzURpGvp+a3YsKRu4B7gUKAYWSJptZqsaWe9nwPOteZ1jadGWfeyvqOHSUd4s5JzLHFFGFr/Syn1PAIrMbCOApMcIpqZY1WC9rwF/Aj7Sytc5Zl5avYvc7CzOPcknmXPOZY5mm4YklUsqC2+VYRNOWYR9DwC2xT0uDpfF73sAwcno6c1kmCJpoaSFpaWlEV66deauLWXiCb3o0jHKgZJzzrUPzRYCM+tqZt3CWx5wHXB3hH031sjecNbS3wD/amZ1zWSYYWaFZlbYu3divq1v21tBUclBn1vIOZdxWvzV18yeaerEb5xiYFDc44HAjgbrFAKPSQIoAD4uqTa85kGbmrs2GE184UgvBM65zJLI6xEsAEaEk9ZtB64HPhu/gpm9N2pL0oPAX5JRBADmrilh6HGdfDSxcy7jJOx6BGZWK2kaQW+gbGCmma2UNDV8vsnzAm2psqaO+Rv2MHnC4GRHcc65NpfQ6xGY2RxgToNljRYAM/t8a1/nw3pj4x6qamPeLOScy0hReg09JKlH3OOekmYmNFUbm7emhLwOWUwc1ivZUZxzrs1FGVk8xsz21z8ws33AuIQlamNmxty1pZx9YgF5HbKTHcc559pclEKQJem9iXck9aIVvY1S1cbdh9i6t4ILvFnIOZehonyg/zcwX9KTBL2FPg38JKGp2tDcNUG30Qt8NLFzLkNFOVn8sKSFBNcfEHBtw/mC0tm8taWM6NOFQb06JTuKc84lRaQmnvCDv918+Nc7VFXLW5v2cItfhMY5l8GinCNot/5RFFyE5oKTvVnIOZe5MroQzF1bSpeOORQO8W6jzrnMlbGFwMyYt7aEc4YXkJuTsb8G55zL3EJQVHKQnQcqOc97CznnMlzGFoLX1u8G4NwRBUlO4pxzyZXBhaCUEwo6e7dR51zGy8hCUFVbx5sb93KOHw0451xmFoJFW/ZxuKaOc0f4+QHnnMvIQvD6+t3kZIkzT/Buo845l5GF4LX1uxk/uCdd8zokO4pzziVdxhWCPQerWLHjgJ8fcM65UMYVgn9s2IOZdxt1zrl6CS0Eki6TtFZSkaQ7G3n+KknLJC2RtFDSOYnMA/D6+lK65eUwZmCPRL+Uc86lhYRdYEZSNnAPcClQDCyQNLvBFNYvA7PNzCSNAR4HRiYqk5nx2vrdnD28gOwsJeplnHMurSTyiGACUGRmG82sGngMuCp+BTM7aGYWPuxMcOGbhNlQGkwr4d1GnXPufYksBAOAbXGPi8NlHyDpGklrgL8CX0hgHl5d59NKOOdcQ4ksBI21vRzxjd/MnjazkcDVwI8b3ZE0JTyHsLC0tLTVgV4v2s0wn1bCOec+IJGFoBgYFPd4ILDjaCub2avAiZKO+LpuZjPMrNDMCnv3bl2zTlVtHW9s2MM5w/1owDnn4iWyECwARkgaJikXuB6YHb+CpOGSFN4fD+QCexIRZvGW/eG0El4InHMuXsJ6DZlZraRpwPNANjDTzFZKmho+Px24DrhJUg1wGPhM3MnjYyonW1xwcm8mnXhcInbvnHNpSwn63E2YwsJCW7hwYbJjOOdcWpG0yMwKG3su40YWO+ec+yAvBM45l+G8EDjnXIbzQuCccxnOC4FzzmU4LwTOOZfhvBA451yG80LgnHMZLu0GlEkqBba0cvMCYPcxjJNo6ZQ3nbJCeuVNp6yQXnnTKSt8uLxDzKzRydrSrhB8GJIWHm1kXSpKp7zplBXSK286ZYX0yptOWSFxeb1pyDnnMpwXAuecy3CZVghmJDtAC6VT3nTKCumVN52yQnrlTaeskKC8GXWOwDnn3JEy7YjAOedcA14InHMuw2VMIZB0maS1kook3ZnsPA1JmimpRNKKuGW9JL0oaX34s2cyM9aTNEjSXEmrJa2U9PVwecrllZQn6W1JS8OsP0zVrPUkZUt6R9JfwsepnHWzpOWSlkhaGC5L5bw9JD0paU349zspFfNKOjn8ndbfyiTdkaisGVEIJGUD9wCXA6OByZJGJzfVER4ELmuw7E7gZTMbAbwcPk4FtcA3zWwUcCZwW/j7TMW8VcBFZnY6MBa4TNKZpGbWel8HVsc9TuWsABea2di4/u2pnPcu4DkzGwmcTvB7Trm8ZrY2/J2OBc4AKoCnSVRWM2v3N2AS8Hzc4+8A30l2rkZyDgVWxD1eC/QP7/cH1iY741Fy/xm4NNXzAp2AxcDEVM0KDAz/g18E/CXV/w6AzUBBg2UpmRfoBmwi7CST6nnj8n0U+Ecis2bEEQEwANgW97g4XJbq+prZToDwZ58k5zmCpKHAOOAtUjRv2NSyBCgBXjSzlM0K/Ab4FyAWtyxVswIY8IKkRZKmhMtSNe8JQCnwQNj0dp+kzqRu3nrXA7PC+wnJmimFQI0s836zH5KkLsCfgDvMrCzZeY7GzOosOMQeCEyQdGqSIzVK0hVAiZktSnaWFjjbzMYTNLveJum8ZAdqQg4wHrjXzMYBh0iBZqCmSMoFPgk8kcjXyZRCUAwMins8ENiRpCwtsUtSf4DwZ0mS87xHUgeCIvComT0VLk7ZvABmth+YR3AuJhWzng18UtJm4DHgIkn/S2pmBcDMdoQ/SwjasCeQunmLgeLwiBDgSYLCkKp5ISiwi81sV/g4IVkzpRAsAEZIGhZW2OuB2UnOFMVs4Obw/s0EbfFJJ0nA/cBqM/tV3FMpl1dSb0k9wvv5wCXAGlIwq5l9x8wGmtlQgr/Rv5vZ50jBrACSOkvqWn+foC17BSma18zeBbZJOjlcdDGwihTNG5rM+81CkKisyT4R0oYnXD4OrAM2AP8v2XkayTcL2AnUEHxzuRU4juDE4frwZ69k5wyznkPQtLYMWBLePp6KeYExwDth1hXA98LlKZe1Qe4LeP9kcUpmJWhzXxreVtb/v0rVvGG2scDC8O/hGaBnquYl6NywB+getywhWX2KCeecy3CZ0jTknHPuKLwQOOdchvNC4JxzGc4LgXPOZTgvBM45l+G8ELi0J2mepIRfgFzS7eGMlY8m+rWSKZyh86vJzuHajhcCl9Ek5bRg9a8CHzezGxKVJ0X0IHivLkN4IXBtQtLQ8Nv0H8LrArwQjvT9wDd6SQXhFAtI+rykZyQ9K2mTpGmS/jmcMOxNSb3iXuJzkuZLWiFpQrh9ZwXXeVgQbnNV3H6fkPQs8EIjWf853M8KSXeEy6YTDKCaLekbDdbPlvTLcF7+ZZK+Fi6/OHzd5WGOjuHyzZL+U9IbkhZKGi/peUkbJE0N17lA0quSnpa0StJ0SVnhc5PDfa6Q9LO4HAcl/UTBtRfelNQ3XN5b0p/C38MCSWeHy38Q5ponaaOk28Nd/RQ4UcE8+L+Q1D/MsiR8zXNb+3fgUlSyR8/5LTNuBFNs1wJjw8ePA58L788DCsP7BcDm8P7ngSKgK9AbOABMDZ/7NcFkd/Xb/yG8fx7hVN7Af8a9Rg+CkeWdw/0W08ioTIK535eH63UhGDE7LnxuMw2mXA6Xf4Vg3qWc8HEvII9gxtuTwmUPx+XdDHwl7n0si3uPJeHyC4BKguKTDbwIfAo4HtgarpsD/B24OtzGgCvD+z8Hvhve/z/gnPD+YIKpQQB+AMwHOoa/9z1AB46cDv2bvD9qOBvomuy/J78d21tLDoud+7A2mdmS8P4igg+c5sw1s3KgXNIB4Nlw+XKC6SPqzQIws1cldQvnF/oowSRu3wrXySP4IIRgOuq9jbzeOcDTZnYIQNJTwLkE01QczSXAdDOrDTPslXR6+H7Xhes8BNxGMM00vD/X1XKgS9x7rKyfGwl428w2hjlmhdlqgHlmVhouf5Sg+D0DVAN/CbddRHCNiPp8o4MpogDoVj9HEPBXM6sCqiSVAH0beX8LgJkKJhp8Ju7f0LUTXghcW6qKu18H5If3a3m/mTKviW1icY9jfPDvt+FcKUYw/fh1ZrY2/glJEwmmIG5MY1OWN0eNvH5z+4l/Hw3fY/37Otp7OpoaM6vfpi5uP1nAJDM7/IGAQWFo+G9yxGdCWFzPAz4BPCLpF2b2cBM5XJrxcwQuFWwmaJKBoPmjNT4DIOkc4ICZHQCeB74WzpaKpHER9vMqcLWkTuGMmtcArzWzzQvA1PoTz+G5izXAUEnDw3VuBF5p4XuaoGDG3CyC9/c6wQWAzg/PpWQTzE7Z3H5fAKbVP5A0tpn1ywmaqurXH0LQZPUHgllnx7fwfbgU50cELhX8Enhc0o0Ebd6tsU/SfILLEX4hXPZjgqaYZWEx2Axc0dROzGyxpAeBt8NF95lZU81CAPcBJ4WvU0NwvuJuSbcAT4QFYgEwvYXv6Q2CE7enERSop80sJuk7wFyCo4M5ZtbcVMS3A/dIWkbwf/5VYOrRVjazPZL+IWkF8DeCWVu/Hb63g8BNLXwfLsX57KPOpSBJFwDfMrMmC5dzx4I3DTnnXIbzIwLnnMtwfkTgnHMZzguBc85lOC8EzjmX4bwQOOdchvNC4JxzGe7/A2OFC0kt9Fw7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fossil_preproc.pca_feature_selection(base_data, eda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f949a20-cfb6-4f8c-86cc-3814ade79290",
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_features = fossil_preproc.pca_feature_selection(base_data, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b1eba-5500-435d-ae44-71278f558cb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adf64584-25d2-4649-af02-cf40fbd84798",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for timestep 1 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[239]\ttraining's l1: 113999\tvalid_1's l1: 155863\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 88194.1\tvalid_1's l1: 118728\n",
      "Early stopping, best iteration is:\n",
      "[675]\ttraining's l1: 79765\tvalid_1's l1: 117185\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 86117.6\tvalid_1's l1: 129252\n",
      "Early stopping, best iteration is:\n",
      "[673]\ttraining's l1: 78092.4\tvalid_1's l1: 127827\n",
      "Elapsed 0.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 83360.2\tvalid_1's l1: 150188\n",
      "Early stopping, best iteration is:\n",
      "[629]\ttraining's l1: 77315.8\tvalid_1's l1: 148814\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 85279.4\tvalid_1's l1: 134225\n",
      "Early stopping, best iteration is:\n",
      "[650]\ttraining's l1: 78336.7\tvalid_1's l1: 133285\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[439]\ttraining's l1: 95652.6\tvalid_1's l1: 127419\n",
      "Elapsed 0.20 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[423]\ttraining's l1: 87024\tvalid_1's l1: 143375\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 84882.8\tvalid_1's l1: 116310\n",
      "Early stopping, best iteration is:\n",
      "[625]\ttraining's l1: 78730.9\tvalid_1's l1: 115586\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 81774.8\tvalid_1's l1: 131857\n",
      "Early stopping, best iteration is:\n",
      "[625]\ttraining's l1: 76026.1\tvalid_1's l1: 131154\n",
      "Elapsed 0.30 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttraining's l1: 84214.7\tvalid_1's l1: 130017\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 78268.7\tvalid_1's l1: 125213\n",
      "Early stopping, best iteration is:\n",
      "[678]\ttraining's l1: 70685.3\tvalid_1's l1: 123767\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 77678.2\tvalid_1's l1: 126836\n",
      "Early stopping, best iteration is:\n",
      "[660]\ttraining's l1: 70499.4\tvalid_1's l1: 125723\n",
      "Elapsed 0.40 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Making timestep 1 predictions\n",
      "Val MAE: 133795.3755917538\n",
      "\n",
      "\n",
      "Making timestep 2 predictions\n",
      "Val MAE: 136597.2491148936\n",
      "\n",
      "\n",
      "Making timestep 3 predictions\n",
      "Val MAE: 130142.7268334461\n",
      "\n",
      "\n",
      "Making timestep 4 predictions\n",
      "Val MAE: 126529.39100255941\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:207431.57888\tvalidation_1-mae:225583.30728\n",
      "[116]\tvalidation_0-mae:116135.66555\tvalidation_1-mae:135339.85672\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:211643.81541\tvalidation_1-mae:217225.77590\n",
      "[99]\tvalidation_0-mae:120919.27846\tvalidation_1-mae:121745.02386\n",
      "Elapsed 0.11 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:221391.59001\tvalidation_1-mae:197450.63821\n",
      "[89]\tvalidation_0-mae:124805.57495\tvalidation_1-mae:116343.74766\n",
      "Elapsed 0.16 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 203978.1506026\ttest: 203978.1506026\ttest1: 222338.3178592\tbest: 222338.3178592 (0)\ttotal: 8.23ms\tremaining: 1m 22s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 129860.9426\n",
      "bestIteration = 270\n",
      "\n",
      "Shrink model to first 271 iterations.\n",
      "Elapsed 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 208464.0775641\ttest: 208464.0775641\ttest1: 213324.6876496\tbest: 213324.6876496 (0)\ttotal: 8.16ms\tremaining: 1m 21s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 118084.0537\n",
      "bestIteration = 287\n",
      "\n",
      "Shrink model to first 288 iterations.\n",
      "Elapsed 0.12 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 217691.0062575\ttest: 217691.0062575\ttest1: 194405.8863634\tbest: 194405.8863634 (0)\ttotal: 12.2ms\tremaining: 2m 2s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 111797.8323\n",
      "bestIteration = 287\n",
      "\n",
      "Shrink model to first 288 iterations.\n",
      "Elapsed 0.18 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Elapsed 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Elapsed 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 204074.7374324\ttest: 204074.7374324\ttest1: 222537.9198484\tbest: 222537.9198484 (0)\ttotal: 7.94ms\tremaining: 1m 19s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 129343.2699\n",
      "bestIteration = 316\n",
      "\n",
      "Shrink model to first 317 iterations.\n",
      "Elapsed 0.09 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 208435.7678512\ttest: 208435.7678512\ttest1: 213294.8962217\tbest: 213294.8962217 (0)\ttotal: 31.8ms\tremaining: 5m 17s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 117842.1106\n",
      "bestIteration = 287\n",
      "\n",
      "Shrink model to first 288 iterations.\n",
      "Elapsed 0.15 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 217952.3293234\ttest: 217952.3293234\ttest1: 194690.1394439\tbest: 194690.1394439 (0)\ttotal: 13.2ms\tremaining: 2m 12s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 111757.1903\n",
      "bestIteration = 279\n",
      "\n",
      "Shrink model to first 280 iterations.\n",
      "Elapsed 0.21 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:207431.11639\tvalidation_1-mae:225793.61717\n",
      "[140]\tvalidation_0-mae:115754.31737\tvalidation_1-mae:136161.83213\n",
      "Elapsed 0.08 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:211642.87447\tvalidation_1-mae:217206.75509\n",
      "[94]\tvalidation_0-mae:120311.82966\tvalidation_1-mae:122045.42066\n",
      "Elapsed 0.13 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:221387.67628\tvalidation_1-mae:197421.39075\n",
      "[86]\tvalidation_0-mae:124340.45067\tvalidation_1-mae:116348.02592\n",
      "Elapsed 0.19 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Elapsed 0.00 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Elapsed 0.00 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:207430.15896\tvalidation_1-mae:225634.57797\n",
      "[115]\tvalidation_0-mae:116030.08338\tvalidation_1-mae:135768.61398\n",
      "Elapsed 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:211643.96345\tvalidation_1-mae:217230.01072\n",
      "[95]\tvalidation_0-mae:120951.32012\tvalidation_1-mae:121799.04873\n",
      "Elapsed 0.13 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:221391.36908\tvalidation_1-mae:197441.68387\n",
      "[89]\tvalidation_0-mae:124723.00464\tvalidation_1-mae:117145.34150\n",
      "Elapsed 0.19 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 204010.3835147\ttest: 204010.3835147\ttest1: 222450.8242861\tbest: 222450.8242861 (0)\ttotal: 8.71ms\tremaining: 1m 27s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 129381.9721\n",
      "bestIteration = 289\n",
      "\n",
      "Shrink model to first 290 iterations.\n",
      "Elapsed 0.08 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 208439.3196689\ttest: 208439.3196689\ttest1: 213315.8547320\tbest: 213315.8547320 (0)\ttotal: 8.75ms\tremaining: 1m 27s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 117967.5891\n",
      "bestIteration = 282\n",
      "\n",
      "Shrink model to first 283 iterations.\n",
      "Elapsed 0.13 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 217865.7642529\ttest: 217865.7642529\ttest1: 194535.5007119\tbest: 194535.5007119 (0)\ttotal: 7.85ms\tremaining: 1m 18s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 112044.9706\n",
      "bestIteration = 287\n",
      "\n",
      "Shrink model to first 288 iterations.\n",
      "Elapsed 0.20 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:226722.86759\tvalidation_1-mae:242414.33359\n",
      "[115]\tvalidation_0-mae:104186.45949\tvalidation_1-mae:134813.91163\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:233102.56838\tvalidation_1-mae:229895.56934\n",
      "[143]\tvalidation_0-mae:106595.67835\tvalidation_1-mae:121929.41130\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:236219.94678\tvalidation_1-mae:223693.21574\n",
      "[131]\tvalidation_0-mae:105065.31242\tvalidation_1-mae:130940.04469\n",
      "Elapsed 0.13 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:212349.15299\tvalidation_1-mae:231927.56350\n",
      "[115]\tvalidation_0-mae:103585.98796\tvalidation_1-mae:142907.28631\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:204857.27246\tvalidation_1-mae:247621.49507\n",
      "[175]\tvalidation_0-mae:102760.49479\tvalidation_1-mae:138291.56271\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:239011.13853\tvalidation_1-mae:176871.25405\n",
      "[62]\tvalidation_0-mae:127641.06603\tvalidation_1-mae:109313.13256\n",
      "Elapsed 0.26 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:207141.34849\tvalidation_1-mae:203042.18216\n",
      "[100]\tvalidation_0-mae:103487.02307\tvalidation_1-mae:126811.11987\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:212280.82757\tvalidation_1-mae:191201.39192\n",
      "[75]\tvalidation_0-mae:113049.44352\tvalidation_1-mae:115948.91062\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:197499.28079\tvalidation_1-mae:222866.16638\n",
      "[172]\tvalidation_0-mae:97186.39181\tvalidation_1-mae:132489.91425\n",
      "Elapsed 0.40 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:210992.35660\tvalidation_1-mae:171533.67072\n",
      "[78]\tvalidation_0-mae:110373.47224\tvalidation_1-mae:110112.17821\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:189761.79391\tvalidation_1-mae:214111.76247\n",
      "[136]\tvalidation_0-mae:95237.32662\tvalidation_1-mae:128045.75537\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:192598.98986\tvalidation_1-mae:208334.92391\n",
      "[122]\tvalidation_0-mae:96293.76920\tvalidation_1-mae:120654.61063\n",
      "Elapsed 0.53 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Making timestep 1 predictions\n",
      "Val MAE: 129077.55716990317\n",
      "\n",
      "\n",
      "Making timestep 2 predictions\n",
      "Val MAE: 129424.05056602039\n",
      "\n",
      "\n",
      "Making timestep 3 predictions\n",
      "Val MAE: 124236.86073255153\n",
      "\n",
      "\n",
      "Making timestep 4 predictions\n",
      "Val MAE: 119287.44099464316\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 124473\tvalid_1's l1: 144464\n",
      "Early stopping, best iteration is:\n",
      "[666]\ttraining's l1: 122878\tvalid_1's l1: 143135\n",
      "Elapsed 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttraining's l1: 129070\tvalid_1's l1: 137933\n",
      "Elapsed 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttraining's l1: 133662\tvalid_1's l1: 128279\n",
      "Elapsed 0.08 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 203878.0666120\ttest: 203878.0666120\ttest1: 222232.1067609\tbest: 222232.1067609 (0)\ttotal: 14.3ms\tremaining: 2m 23s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 131236.2274\n",
      "bestIteration = 289\n",
      "\n",
      "Shrink model to first 290 iterations.\n",
      "Elapsed 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 208398.6196769\ttest: 208398.6196769\ttest1: 213310.8079834\tbest: 213310.8079834 (0)\ttotal: 13.7ms\tremaining: 2m 17s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 120526.9332\n",
      "bestIteration = 315\n",
      "\n",
      "Shrink model to first 316 iterations.\n",
      "Elapsed 0.11 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 218183.1420740\ttest: 218183.1420740\ttest1: 194810.7078859\tbest: 194810.7078859 (0)\ttotal: 14.7ms\tremaining: 2m 27s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 111498.7931\n",
      "bestIteration = 271\n",
      "\n",
      "Shrink model to first 272 iterations.\n",
      "Elapsed 0.16 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Elapsed 0.00 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 204226.2591926\ttest: 204226.2591926\ttest1: 222620.6433903\tbest: 222620.6433903 (0)\ttotal: 8ms\tremaining: 1m 20s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 130188.0728\n",
      "bestIteration = 305\n",
      "\n",
      "Shrink model to first 306 iterations.\n",
      "Elapsed 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 208753.8668896\ttest: 208753.8668896\ttest1: 213663.5461358\tbest: 213663.5461358 (0)\ttotal: 14.5ms\tremaining: 2m 24s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 119662.0423\n",
      "bestIteration = 421\n",
      "\n",
      "Shrink model to first 422 iterations.\n",
      "Elapsed 0.20 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 218081.2043321\ttest: 218081.2043321\ttest1: 194787.1260703\tbest: 194787.1260703 (0)\ttotal: 8.44ms\tremaining: 1m 24s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 111640.4999\n",
      "bestIteration = 254\n",
      "\n",
      "Shrink model to first 255 iterations.\n",
      "Elapsed 0.31 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 119929\tvalid_1's l1: 139890\n",
      "Early stopping, best iteration is:\n",
      "[642]\ttraining's l1: 118537\tvalid_1's l1: 139206\n",
      "Elapsed 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[481]\ttraining's l1: 124045\tvalid_1's l1: 133424\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 128478\tvalid_1's l1: 125448\n",
      "Early stopping, best iteration is:\n",
      "[569]\ttraining's l1: 127588\tvalid_1's l1: 125217\n",
      "Elapsed 0.08 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Elapsed 0.00 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Elapsed 0.00 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 120166\tvalid_1's l1: 138491\n",
      "Early stopping, best iteration is:\n",
      "[613]\ttraining's l1: 118973\tvalid_1's l1: 137883\n",
      "Elapsed 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 124042\tvalid_1's l1: 132999\n",
      "Early stopping, best iteration is:\n",
      "[518]\ttraining's l1: 123779\tvalid_1's l1: 132880\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 128819\tvalid_1's l1: 125467\n",
      "Early stopping, best iteration is:\n",
      "[630]\ttraining's l1: 127299\tvalid_1's l1: 125044\n",
      "Elapsed 0.09 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 203816.2153584\ttest: 203816.2153584\ttest1: 222170.7866434\tbest: 222170.7866434 (0)\ttotal: 149ms\tremaining: 24m 54s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 130251.9212\n",
      "bestIteration = 324\n",
      "\n",
      "Shrink model to first 325 iterations.\n",
      "Elapsed 0.09 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 208397.6965246\ttest: 208397.6965246\ttest1: 213305.8099640\tbest: 213305.8099640 (0)\ttotal: 101ms\tremaining: 16m 46s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 119837.4387\n",
      "bestIteration = 362\n",
      "\n",
      "Shrink model to first 363 iterations.\n",
      "Elapsed 0.17 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 218189.4900052\ttest: 218189.4900052\ttest1: 194809.0002665\tbest: 194809.0002665 (0)\ttotal: 8.89ms\tremaining: 1m 28s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 111982.4455\n",
      "bestIteration = 221\n",
      "\n",
      "Shrink model to first 222 iterations.\n",
      "Elapsed 0.23 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 222103.7619990\ttest: 222103.7619990\ttest1: 237162.3004707\tbest: 237162.3004707 (0)\ttotal: 41.5ms\tremaining: 6m 54s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 129012.6772\n",
      "bestIteration = 302\n",
      "\n",
      "Shrink model to first 303 iterations.\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 228202.2135119\ttest: 228202.2135119\ttest1: 225369.9556292\tbest: 225369.9556292 (0)\ttotal: 13.4ms\tremaining: 2m 14s\n",
      "500:\tlearn: 108801.9054118\ttest: 108801.9054118\ttest1: 113659.9259474\tbest: 113650.9590984 (499)\ttotal: 1.88s\tremaining: 35.6s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 110053.1267\n",
      "bestIteration = 940\n",
      "\n",
      "Shrink model to first 941 iterations.\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 231268.6198495\ttest: 231268.6198495\ttest1: 218997.0452563\tbest: 218997.0452563 (0)\ttotal: 6.48ms\tremaining: 1m 4s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 124003.6725\n",
      "bestIteration = 302\n",
      "\n",
      "Shrink model to first 303 iterations.\n",
      "Elapsed 0.18 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 208652.5452516\ttest: 208652.5452516\ttest1: 228060.5228095\tbest: 228060.5228095 (0)\ttotal: 12.7ms\tremaining: 2m 7s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 131573.9296\n",
      "bestIteration = 285\n",
      "\n",
      "Shrink model to first 286 iterations.\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 201348.3167719\ttest: 201348.3167719\ttest1: 243287.6085939\tbest: 243287.6085939 (0)\ttotal: 9.71ms\tremaining: 1m 37s\n",
      "500:\tlearn: 105350.9911313\ttest: 105350.9911313\ttest1: 132758.8868526\tbest: 132758.8754317 (499)\ttotal: 1.96s\tremaining: 37.1s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 130712.6204\n",
      "bestIteration = 828\n",
      "\n",
      "Shrink model to first 829 iterations.\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 235150.1219398\ttest: 235150.1219398\ttest1: 173852.3516845\tbest: 173852.3516845 (0)\ttotal: 12.8ms\tremaining: 2m 8s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 108149.5228\n",
      "bestIteration = 160\n",
      "\n",
      "Shrink model to first 161 iterations.\n",
      "Elapsed 0.31 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 204024.3619800\ttest: 204024.3619800\ttest1: 199338.7445679\tbest: 199338.7445679 (0)\ttotal: 14.1ms\tremaining: 2m 21s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 118184.0592\n",
      "bestIteration = 246\n",
      "\n",
      "Shrink model to first 247 iterations.\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 209344.8996422\ttest: 209344.8996422\ttest1: 189424.9838514\tbest: 189424.9838514 (0)\ttotal: 16.3ms\tremaining: 2m 42s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 106460.998\n",
      "bestIteration = 414\n",
      "\n",
      "Shrink model to first 415 iterations.\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 194507.6463408\ttest: 194507.6463408\ttest1: 219211.9091099\tbest: 219211.9091099 (0)\ttotal: 15ms\tremaining: 2m 29s\n",
      "500:\tlearn: 101158.5932848\ttest: 101158.5932848\ttest1: 126789.0567241\tbest: 126789.0567241 (500)\ttotal: 2.39s\tremaining: 45.3s\n",
      "1000:\tlearn: 94470.0391071\ttest: 94470.0391071\ttest1: 123597.9148823\tbest: 123597.9148823 (1000)\ttotal: 4.55s\tremaining: 40.9s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 123554.9122\n",
      "bestIteration = 1012\n",
      "\n",
      "Shrink model to first 1013 iterations.\n",
      "Elapsed 0.49 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 208749.6636480\ttest: 208749.6636480\ttest1: 168388.6405319\tbest: 168388.6405319 (0)\ttotal: 13.4ms\tremaining: 2m 13s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 101725.0302\n",
      "bestIteration = 176\n",
      "\n",
      "Shrink model to first 177 iterations.\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 187183.9047911\ttest: 187183.9047911\ttest1: 212725.2624996\tbest: 212725.2624996 (0)\ttotal: 14.4ms\tremaining: 2m 24s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 118313.4398\n",
      "bestIteration = 474\n",
      "\n",
      "Shrink model to first 475 iterations.\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 190375.0606295\ttest: 190375.0606295\ttest1: 206039.8764035\tbest: 206039.8764035 (0)\ttotal: 4.91ms\tremaining: 49.1s\n",
      "500:\tlearn: 98404.0693311\ttest: 98404.0693311\ttest1: 115359.4001130\tbest: 115340.3456260 (498)\ttotal: 1.92s\tremaining: 36.4s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 113586.1182\n",
      "bestIteration = 907\n",
      "\n",
      "Shrink model to first 908 iterations.\n",
      "Elapsed 0.69 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Making timestep 1 predictions\n",
      "Val MAE: 121086.88319545754\n",
      "\n",
      "\n",
      "Making timestep 2 predictions\n",
      "Val MAE: 123534.34580895759\n",
      "\n",
      "\n",
      "Making timestep 3 predictions\n",
      "Val MAE: 116086.79840263138\n",
      "\n",
      "\n",
      "Making timestep 4 predictions\n",
      "Val MAE: 111135.43180686029\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttraining's l1: 124025\tvalid_1's l1: 143597\n",
      "Elapsed 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttraining's l1: 128619\tvalid_1's l1: 137630\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttraining's l1: 133871\tvalid_1's l1: 127939\n",
      "Elapsed 0.08 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:207416.86475\tvalidation_1-mae:225520.89458\n",
      "[97]\tvalidation_0-mae:115165.37896\tvalidation_1-mae:132018.01371\n",
      "Elapsed 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:211626.51724\tvalidation_1-mae:217260.61749\n",
      "[91]\tvalidation_0-mae:119393.83655\tvalidation_1-mae:122928.61711\n",
      "Elapsed 0.13 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:221370.56224\tvalidation_1-mae:197389.63385\n",
      "[83]\tvalidation_0-mae:123648.20868\tvalidation_1-mae:115452.07362\n",
      "Elapsed 0.18 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Elapsed 0.00 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:207417.11270\tvalidation_1-mae:225496.94007\n",
      "[97]\tvalidation_0-mae:115801.81319\tvalidation_1-mae:131796.70049\n",
      "Elapsed 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:211625.48283\tvalidation_1-mae:217305.05147\n",
      "[93]\tvalidation_0-mae:120080.96078\tvalidation_1-mae:122775.38862\n",
      "Elapsed 0.12 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:221366.78578\tvalidation_1-mae:197429.00736\n",
      "[83]\tvalidation_0-mae:124491.81755\tvalidation_1-mae:114794.10135\n",
      "Elapsed 0.18 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 119207\tvalid_1's l1: 140071\n",
      "Early stopping, best iteration is:\n",
      "[622]\ttraining's l1: 117848\tvalid_1's l1: 139518\n",
      "Elapsed 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 123703\tvalid_1's l1: 132510\n",
      "Early stopping, best iteration is:\n",
      "[495]\ttraining's l1: 123783\tvalid_1's l1: 132509\n",
      "Elapsed 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 128890\tvalid_1's l1: 124429\n",
      "Early stopping, best iteration is:\n",
      "[579]\ttraining's l1: 127862\tvalid_1's l1: 124044\n",
      "Elapsed 0.09 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Elapsed 0.00 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Elapsed 0.00 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 119603\tvalid_1's l1: 138758\n",
      "Early stopping, best iteration is:\n",
      "[662]\ttraining's l1: 118009\tvalid_1's l1: 137956\n",
      "Elapsed 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 124105\tvalid_1's l1: 131250\n",
      "Early stopping, best iteration is:\n",
      "[564]\ttraining's l1: 123317\tvalid_1's l1: 131013\n",
      "Elapsed 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 129313\tvalid_1's l1: 125521\n",
      "Early stopping, best iteration is:\n",
      "[586]\ttraining's l1: 128324\tvalid_1's l1: 125107\n",
      "Elapsed 0.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:207417.10320\tvalidation_1-mae:225524.02047\n",
      "[96]\tvalidation_0-mae:115642.20113\tvalidation_1-mae:131805.58586\n",
      "Elapsed 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:211625.39068\tvalidation_1-mae:217347.75252\n",
      "[95]\tvalidation_0-mae:119991.95446\tvalidation_1-mae:122806.88868\n",
      "Elapsed 0.13 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:221366.76192\tvalidation_1-mae:197351.09699\n",
      "[79]\tvalidation_0-mae:124536.45612\tvalidation_1-mae:114916.74340\n",
      "Elapsed 0.19 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Elapsed 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Making timestep 1 predictions\n",
      "Val MAE: 155965.10597026738\n",
      "\n",
      "\n",
      "Making timestep 2 predictions\n",
      "Val MAE: 166951.1657728608\n",
      "\n",
      "\n",
      "Making timestep 3 predictions\n",
      "Val MAE: 163621.77972184005\n",
      "\n",
      "\n",
      "Making timestep 4 predictions\n",
      "Val MAE: 225412.8502687466\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[330]\ttraining's l1: 152425\tvalid_1's l1: 179471\n",
      "Elapsed 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[379]\ttraining's l1: 154966\tvalid_1's l1: 164756\n",
      "Elapsed 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 155880\tvalid_1's l1: 159094\n",
      "Early stopping, best iteration is:\n",
      "[596]\ttraining's l1: 154139\tvalid_1's l1: 158172\n",
      "Elapsed 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:207526.39994\tvalidation_1-mae:225823.00031\n",
      "[75]\tvalidation_0-mae:139515.23208\tvalidation_1-mae:162601.90555\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:211728.05470\tvalidation_1-mae:217508.16329\n",
      "[76]\tvalidation_0-mae:143519.30697\tvalidation_1-mae:149457.69519\n",
      "Elapsed 0.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:221453.98121\tvalidation_1-mae:197214.57565\n",
      "[59]\tvalidation_0-mae:148978.55989\tvalidation_1-mae:140139.28563\n",
      "Elapsed 0.14 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 204279.1784815\ttest: 204279.1784815\ttest1: 222628.8561412\tbest: 222628.8561412 (0)\ttotal: 167ms\tremaining: 27m 54s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 149012.7729\n",
      "bestIteration = 236\n",
      "\n",
      "Shrink model to first 237 iterations.\n",
      "Elapsed 0.08 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 208753.8908655\ttest: 208753.8908655\ttest1: 213684.1316464\tbest: 213684.1316464 (0)\ttotal: 608ms\tremaining: 1h 41m 16s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 138911.046\n",
      "bestIteration = 360\n",
      "\n",
      "Shrink model to first 361 iterations.\n",
      "Elapsed 0.23 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 217918.7726265\ttest: 217918.7726265\ttest1: 194567.7798680\tbest: 194567.7798680 (0)\ttotal: 143ms\tremaining: 23m 47s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 131322.6436\n",
      "bestIteration = 157\n",
      "\n",
      "Shrink model to first 158 iterations.\n",
      "Elapsed 0.29 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:207506.34485\tvalidation_1-mae:225603.89469\n",
      "[73]\tvalidation_0-mae:141000.05747\tvalidation_1-mae:159785.84856\n",
      "Elapsed 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:211706.84455\tvalidation_1-mae:217485.62981\n",
      "[79]\tvalidation_0-mae:145473.21613\tvalidation_1-mae:149227.18738\n",
      "Elapsed 0.09 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:221444.23665\tvalidation_1-mae:197398.61950\n",
      "[67]\tvalidation_0-mae:150121.81098\tvalidation_1-mae:140248.41541\n",
      "Elapsed 0.13 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[346]\ttraining's l1: 144592\tvalid_1's l1: 175710\n",
      "Elapsed 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[410]\ttraining's l1: 147021\tvalid_1's l1: 165109\n",
      "Elapsed 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[425]\ttraining's l1: 151381\tvalid_1's l1: 154519\n",
      "Elapsed 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 204188.8276837\ttest: 204188.8276837\ttest1: 222521.8696605\tbest: 222521.8696605 (0)\ttotal: 630ms\tremaining: 1h 44m 57s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 149275.4246\n",
      "bestIteration = 244\n",
      "\n",
      "Shrink model to first 245 iterations.\n",
      "Elapsed 0.13 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 208753.0177646\ttest: 208753.0177646\ttest1: 213637.7301235\tbest: 213637.7301235 (0)\ttotal: 420ms\tremaining: 1h 9m 56s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 139756.0376\n",
      "bestIteration = 316\n",
      "\n",
      "Shrink model to first 317 iterations.\n",
      "Elapsed 0.22 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 218045.7795434\ttest: 218045.7795434\ttest1: 194797.1325681\tbest: 194797.1325681 (0)\ttotal: 580ms\tremaining: 1h 36m 40s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 129991.7149\n",
      "bestIteration = 239\n",
      "\n",
      "Shrink model to first 240 iterations.\n",
      "Elapsed 0.31 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 204207.1704024\ttest: 204207.1704024\ttest1: 222578.2210302\tbest: 222578.2210302 (0)\ttotal: 602ms\tremaining: 1h 40m 21s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 145976.5421\n",
      "bestIteration = 436\n",
      "\n",
      "Shrink model to first 437 iterations.\n",
      "Elapsed 0.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 208893.1970879\ttest: 208893.1970879\ttest1: 213811.8967292\tbest: 213811.8967292 (0)\ttotal: 1.69s\tremaining: 4h 42m 2s\n",
      "500:\tlearn: 132428.2502211\ttest: 132428.2502211\ttest1: 138205.4488904\tbest: 138205.4488904 (500)\ttotal: 6.91s\tremaining: 2m 10s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 137869.862\n",
      "bestIteration = 554\n",
      "\n",
      "Shrink model to first 555 iterations.\n",
      "Elapsed 0.26 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 218155.5911436\ttest: 218155.5911436\ttest1: 194851.2293006\tbest: 194851.2293006 (0)\ttotal: 2.1s\tremaining: 5h 49m 44s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 130040.1212\n",
      "bestIteration = 246\n",
      "\n",
      "Shrink model to first 247 iterations.\n",
      "Elapsed 0.41 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[325]\ttraining's l1: 145443\tvalid_1's l1: 177793\n",
      "Elapsed 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[461]\ttraining's l1: 145795\tvalid_1's l1: 159805\n",
      "Elapsed 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[397]\ttraining's l1: 152103\tvalid_1's l1: 157088\n",
      "Elapsed 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:207510.34285\tvalidation_1-mae:225676.74546\n",
      "[69]\tvalidation_0-mae:138854.85778\tvalidation_1-mae:158991.87064\n",
      "Elapsed 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:211715.85988\tvalidation_1-mae:217391.16729\n",
      "[73]\tvalidation_0-mae:143058.14052\tvalidation_1-mae:147775.64029\n",
      "Elapsed 0.09 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:221442.80765\tvalidation_1-mae:197455.57947\n",
      "[64]\tvalidation_0-mae:147461.70932\tvalidation_1-mae:140311.72176\n",
      "Elapsed 0.13 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fossil.models.gbdt import FossilGBDT\n",
    "gbdt_models = FossilGBDT()\n",
    "\n",
    "oof_preds = {}\n",
    "cv_models = {}\n",
    "models = ['lgb','xgb','cat', 'reg']\n",
    "\n",
    "for i,base_model in enumerate(models):\n",
    "    cols = principal_features+['sku_name','sku_coded', 'month', 'year']\n",
    "    feature_cols = [c for c in base_data.columns if c in cols]\n",
    "    target_cols = [c for c in base_data.columns if 'target' in c]\n",
    "\n",
    "    primary_cv_models = gbdt_models.train_model(base_data, feature_cols, target_cols, True, \n",
    "                                             True, False, model_type=base_model)\n",
    "    \n",
    "    primary_val_mae, primary_oof = gbdt_models.test_model(base_data, feature_cols, target_cols, primary_cv_models)\n",
    "    \n",
    "    target_cols = [f'target_{i}' for i in range(ModelsConfig.N_STEPS)]\n",
    "    pred_cols = [f'preds_{i}' for i in range(ModelsConfig.N_STEPS)]\n",
    "\n",
    "    secondary_data = fossil_preproc.prepare_secondary_data(base_data, primary_oof, target_cols, pred_cols)\n",
    "    oof_preds[f'primary_{i}'] = secondary_data['preds']\n",
    "    cv_models[f'primary_{i}'] = primary_cv_models\n",
    "    \n",
    "    for j,meta_learner in enumerate([m for m in models if m!=base_model]):\n",
    "        cols = ['sku_name','sku_coded','preds', 'time_step', 'month', 'year']\n",
    "        secondary_features = [c for c in secondary_data.columns if 'lag' in c or c in cols]\n",
    "        secondary_targets = 'target'\n",
    "\n",
    "        secondary_cv_models = gbdt_models.train_model(secondary_data, secondary_features, secondary_targets, False, \n",
    "                                                      False, False, model_type=meta_learner,  model_level='meta')\n",
    "        secondary_val_mae, secondary_oof = gbdt_models.test_model(secondary_data, secondary_features, secondary_targets,\n",
    "                                      secondary_cv_models, False, False)\n",
    "        oof_preds[f'secondary_{i}_{j}'] = secondary_oof        \n",
    "        cv_models[f'secondary_{i}_{j}'] = secondary_cv_models\n",
    "        \n",
    "        for k,meta_learner_ in enumerate([m for m in models if m!=base_model and m!=meta_learner]):\n",
    "            secondary_data['preds_'] = secondary_oof\n",
    "            \n",
    "            cols = ['sku_name','sku_coded','preds', 'preds_', 'time_step', 'month', 'year']\n",
    "            tertiary_features = [c for c in secondary_data.columns if 'lag' in c or c in cols]\n",
    "            \n",
    "            tertiary_cv_models = gbdt_models.train_model(secondary_data, tertiary_features, secondary_targets, False, \n",
    "                                                          False, False, model_type=meta_learner_,  model_level='meta')\n",
    "            tertiary_val_mae, tertiary_oof = gbdt_models.test_model(secondary_data, tertiary_features, secondary_targets,\n",
    "                                          tertiary_cv_models, False, False)\n",
    "            oof_preds[f'tertiary_{i}_{j}'] = tertiary_oof        \n",
    "            cv_models[f'tertiary_{i}_{j}'] = tertiary_cv_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33b3f7f9-8bf0-4ad5-8354-852706537ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.transpose(np.concatenate([[v] for v in oof_preds.values()]), (1,0)).mean(1)\n",
    "y_true = secondary_data['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0aefbc6f-cdff-497a-9ac7-942711575893",
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_mae = np.absolute(np.subtract(y_true, y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "594ff140-751f-499c-8abf-f926323f70bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122673.51479071683"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blended_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a7c09f4-0ec4-46b6-a3e9-b2b70d488d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119702.23796283815"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blended_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd0917d-9aa1-47f7-b107-7dce57800164",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61402e21-a61b-4e7d-84c6-9904dfd44f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fossil.models.gbdt import FossilGBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f24593a-9a12-4841-8788-ca4670ceef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_models = FossilGBDT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945baea6-488c-4a94-be11-1fd898392dc8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for timestep 1 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[239]\ttraining's l1: 113999\tvalid_1's l1: 155863\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 88194.1\tvalid_1's l1: 118728\n",
      "Early stopping, best iteration is:\n",
      "[675]\ttraining's l1: 79765\tvalid_1's l1: 117185\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 86117.6\tvalid_1's l1: 129252\n",
      "Early stopping, best iteration is:\n",
      "[673]\ttraining's l1: 78092.4\tvalid_1's l1: 127827\n",
      "Elapsed 0.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 83360.2\tvalid_1's l1: 150188\n",
      "Early stopping, best iteration is:\n",
      "[629]\ttraining's l1: 77315.8\tvalid_1's l1: 148814\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 85279.4\tvalid_1's l1: 134225\n",
      "Early stopping, best iteration is:\n",
      "[650]\ttraining's l1: 78336.7\tvalid_1's l1: 133285\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[439]\ttraining's l1: 95652.6\tvalid_1's l1: 127419\n",
      "Elapsed 0.20 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[423]\ttraining's l1: 87024\tvalid_1's l1: 143375\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 84882.8\tvalid_1's l1: 116310\n",
      "Early stopping, best iteration is:\n",
      "[625]\ttraining's l1: 78730.9\tvalid_1's l1: 115586\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 81774.8\tvalid_1's l1: 131857\n",
      "Early stopping, best iteration is:\n",
      "[625]\ttraining's l1: 76026.1\tvalid_1's l1: 131154\n",
      "Elapsed 0.30 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "\n",
      "\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[482]\ttraining's l1: 84214.7\tvalid_1's l1: 130017\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 78268.7\tvalid_1's l1: 125213\n",
      "Early stopping, best iteration is:\n",
      "[678]\ttraining's l1: 70685.3\tvalid_1's l1: 123767\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[500]\ttraining's l1: 77678.2\tvalid_1's l1: 126836\n",
      "Early stopping, best iteration is:\n",
      "[660]\ttraining's l1: 70499.4\tvalid_1's l1: 125723\n",
      "Elapsed 0.41 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = principal_features+['sku_name','sku_coded', 'month', 'year']\n",
    "feature_cols = [c for c in base_data.columns if c in cols]\n",
    "target_cols = [c for c in base_data.columns if 'target' in c]\n",
    "\n",
    "primary_cv_models = gbdt_models.train_model(base_data, feature_cols, target_cols, True, \n",
    "                                         True, False, model_type=ModelsConfig.BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54634b66-5142-43a4-bfa2-b3b201c8ce3a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making timestep 1 predictions\n",
      "Val MAE: 133795.3755917538\n",
      "\n",
      "\n",
      "Making timestep 2 predictions\n",
      "Val MAE: 136597.2491148936\n",
      "\n",
      "\n",
      "Making timestep 3 predictions\n",
      "Val MAE: 130142.7268334461\n",
      "\n",
      "\n",
      "Making timestep 4 predictions\n",
      "Val MAE: 126529.39100255941\n",
      "\n",
      "\n",
      "Average Val MAE: 131766.1856356564\n"
     ]
    }
   ],
   "source": [
    "primary_val_mae, primary_oof = gbdt_models.test_model(base_data, feature_cols, target_cols, primary_cv_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59f893b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [f'target_{i}' for i in range(ModelsConfig.N_STEPS)]\n",
    "pred_cols = [f'preds_{i}' for i in range(ModelsConfig.N_STEPS)]\n",
    "\n",
    "secondary_data = fossil_preproc.prepare_secondary_data(base_data, primary_oof, target_cols, pred_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26f1f962-c201-4183-b89e-f2e0de51e3c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training fold 1\n",
      "0:\tlearn: 208656.1697324\ttest: 208656.1697324\ttest1: 212991.4316369\tbest: 212991.4316369 (0)\ttotal: 8.3ms\tremaining: 1m 22s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 129525.8649\n",
      "bestIteration = 273\n",
      "\n",
      "Shrink model to first 274 iterations.\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "0:\tlearn: 208590.5636982\ttest: 208590.5636982\ttest1: 212977.4893775\tbest: 212977.4893775 (0)\ttotal: 9.97ms\tremaining: 1m 39s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 114765.9882\n",
      "bestIteration = 269\n",
      "\n",
      "Shrink model to first 270 iterations.\n",
      "Elapsed 0.09 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "0:\tlearn: 212940.2084086\ttest: 212940.2084086\ttest1: 204280.4992301\tbest: 204280.4992301 (0)\ttotal: 8.29ms\tremaining: 1m 22s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 120431.0379\n",
      "bestIteration = 166\n",
      "\n",
      "Shrink model to first 167 iterations.\n",
      "Elapsed 0.12 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['sku_name','sku_coded','preds', 'time_step', 'month', 'year']\n",
    "secondary_features = [c for c in secondary_data.columns if 'lag' in c or c in cols]\n",
    "secondary_targets = 'target'\n",
    "\n",
    "secondary_cv_models = gbdt_models.train_model(secondary_data, secondary_features, secondary_targets, False, \n",
    "                                              False, False, model_type=ModelsConfig.META_LEARNER,  model_level='meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1dab6b62-d260-43c2-bb6c-9ee21569736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "Val MAE: 129525.86493584719\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "Val MAE: 114765.98818620258\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "Val MAE: 120431.03793220311\n",
      "\n",
      "\n",
      "Average Val MAE: 121635.72251532094\n"
     ]
    }
   ],
   "source": [
    "secondary_val_mae, secondary_oof = gbdt_models.test_model(secondary_data, secondary_features, secondary_targets,\n",
    "                                      secondary_cv_models, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "faa7438d-c251-4186-868f-72846883ab7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sku_name', 'month', 'year', 'sku_coded', 'preds', 'time_step']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3e68dfc-4df0-4633-97f1-4fca48614923",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = secondary_data.copy()\n",
    "meta_data['preds_1'] = secondary_oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "147a0293-0801-40d8-b1f0-c5edaba85ff2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training fold 1\n",
      "[0]\tvalidation_0-mae:211965.73308\tvalidation_1-mae:216699.09509\n",
      "[92]\tvalidation_0-mae:115410.46751\tvalidation_1-mae:148006.43434\n",
      "Elapsed 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "[0]\tvalidation_0-mae:212240.47463\tvalidation_1-mae:216190.29722\n",
      "[108]\tvalidation_0-mae:120618.00100\tvalidation_1-mae:117407.61671\n",
      "Elapsed 0.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "[0]\tvalidation_0-mae:216301.64219\tvalidation_1-mae:207236.03203\n",
      "[68]\tvalidation_0-mae:121640.43029\tvalidation_1-mae:119616.78564\n",
      "Elapsed 0.14 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['sku_name','sku_coded','preds', 'preds_1', 'time_step', 'month', 'year']\n",
    "secondary_features = [c for c in secondary_data.columns if 'lag' in c or c in cols]\n",
    "secondary_targets = 'target'\n",
    "\n",
    "meta_cv_models = gbdt_models.train_model(meta_data, secondary_features, secondary_targets, False, \n",
    "                                              False, False, 'xgb',  model_level='meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f60eb523-8682-4a93-8c31-166e7f2e7eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "Val MAE: 147892.09896966783\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "Val MAE: 117262.19216229246\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "Val MAE: 119093.72902127054\n",
      "\n",
      "\n",
      "Average Val MAE: 128232.71753000416\n"
     ]
    }
   ],
   "source": [
    "meta_val_mae, meta_oof = gbdt_models.test_model(meta_data, secondary_features, secondary_targets,\n",
    "                                      meta_cv_models, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ec443af-da54-4f2e-bf03-341b16a1ead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Making fold 1 predictions\n",
      "Val MAE: 147892.09896966783\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "Val MAE: 117262.19216229246\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "Val MAE: 119093.72902127054\n",
      "\n",
      "\n",
      "Average Val MAE: 128232.71753000416\n"
     ]
    }
   ],
   "source": [
    "meta_val_mae, meta_oof = gbdt_models.test_model(meta_data, secondary_features, secondary_targets,\n",
    "                                      meta_cv_models, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "306d5959-f5cc-4935-9cf6-e4051a4b0e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = secondary_data[secondary_features].drop(columns=['sku_name','sku_coded','pred_month','pred_year']).columns\n",
    "\n",
    "for i,model in enumerate(secondary_cv_models):\n",
    "    \n",
    "    feature_importances[f'fold_{i}'] = model.feature_importances_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e0701-aafb-42f6-86eb-ab201a101971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "feature_importances_flatten = pd.DataFrame()\n",
    "for i in range(1, len(feature_importances.columns)-1):\n",
    "    col = ['feature', feature_importances.columns.values[i]]\n",
    "    feature_importances_flatten = pd.concat([feature_importances_flatten, feature_importances[col].rename(columns={f'fold_{i-1}': 'importance'})], axis=0)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances_flatten.sort_values(by='importance', ascending=False), x='importance', y='feature')\n",
    "plt.title('Feature Importances over {} folds'.format(ModelsConfig.FOLDS))  \n",
    "# plt.savefig(\"feature_importances.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f88d53-1113-4b90-ae4c-4939c38a3977",
   "metadata": {},
   "source": [
    "## Meta Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf93777-924a-4491-8b69-584e3a84659f",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a219fd-ba60-4eac-841a-52460619f80b",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63beb5a2-a4f7-4e67-800a-5626b842c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv(f'{TEST_DIR}/Test.csv')\n",
    "train = pd.read_csv(f'{TRAIN_DIR}/Train.csv')\n",
    "desc = pd.read_csv(f'{DATA_DIR}/DataDictionary.csv')\n",
    "\n",
    "CAT = desc[36:]['Column Name'].tolist()\n",
    "train_df = train.drop(columns=CAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff72446c-7fdf-4e4b-9ae2-ea3b34c65e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fossil.inference import prepare_test_context, prepare_submission_data, prepare_test_dates\n",
    "from fossil.config import ModelsConfig\n",
    "from fossil.preprocessing import FossilData, LabelEncoder\n",
    "\n",
    "np.random.seed(ModelsConfig.SEED)\n",
    "sku_encoder = LabelEncoder(train.sku_name.sample(frac=0.95).unique())\n",
    "\n",
    "\n",
    "fossil_preproc = FossilPreprocessor(sku_encoder)\n",
    "test_context, test_dates = prepare_test_context(train_df, test, fossil_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1af44a5e-ba5a-4691-9766-3fafdf63ee38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d21531749074ea5aa762b1003051582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a169ea74a65642dcbb9bb0dd36dc9113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8313c63f5543fcb5c8193908231fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332959200ff94811962c38a3f3a30897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baffcfcdbcb54b60bcd5bc0c3e6abc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fa8e5f23754e8ca925164446da15d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80817a4c1984a0ab8c41f886fa4713b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf0d85e7c9e445898f57fd6a41123b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_cols = [c for c in test_context.columns if c not in ['sku_name', 'month', 'year']\n",
    "             and all(l not in c for l in ['target', 'channel','rel'])]\n",
    "\n",
    "test_data = fossil_preproc.prepare_primary_data(test_context, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c50a2f32-e2a0-4912-ae3a-f10ac35f6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[test_data[['month', 'year']].apply(tuple, axis=1).isin(test_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f8cf15e7-77a0-4263-97ff-1088f5e76323",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [f'target_{i}' for i in range(ModelsConfig.N_STEPS)]\n",
    "pred_cols = [f'preds_{i}' for i in range(ModelsConfig.N_STEPS)]\n",
    "\n",
    "cols = principal_features+['month','year']\n",
    "non_features = ['sku_name','sku_coded']+target_cols\n",
    "feature_cols = [c for c in test_data.columns if c not in non_features and c in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0d97a68-49ac-4867-9149-0a804a0cffdf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from fossil.models.gbdt import FossilGBDT\n",
    "gbdt_models = FossilGBDT()\n",
    "\n",
    "\n",
    "test_preds = {}\n",
    "for i in range(len(models)):\n",
    "    primary_preds = gbdt_models.forecast(test_data, feature_cols, target_cols, cv_models[f'primary_{i}'], True, True, True)\n",
    "    \n",
    "    sub_df = prepare_submission_data(test, primary_preds, target_cols, pred_cols)\n",
    "    # sub_df = fossil_preproc.adjust_expanded_dates(sub_df)\n",
    "    _, pred_dates = prepare_test_dates(test) \n",
    "\n",
    "    sub_df['month'] = pd.DataFrame(pred_dates*int(len(sub_df)/ModelsConfig.N_STEPS)).loc[:, 0].values\n",
    "    sub_df['year'] = pd.DataFrame(pred_dates*int(len(sub_df)/ModelsConfig.N_STEPS)).loc[:, 1].values\n",
    "    test_preds[f'primary_{i}'] = sub_df['preds'].values\n",
    "    \n",
    "    cols = ['preds', 'time_step', 'month', 'year', 'pred_month', 'pred_year']\n",
    "    meta_features = [c for c in sub_df.columns if 'lag' in c or c in cols]\n",
    "    meta_targets = 'Target'\n",
    "    \n",
    "    for j in range(len(models)-1):\n",
    "        secondary_preds = gbdt_models.forecast(sub_df, meta_features, meta_targets, \n",
    "                                                cv_models[f'secondary_{i}_{j}'], True, False, False)\n",
    "        \n",
    "        test_preds[f'secondary_{i}_{j}'] = secondary_preds['Target'].values        \n",
    "        sub_df['preds_'] = secondary_preds['Target']\n",
    "        \n",
    "        cols = ['preds_', 'preds', 'time_step', 'month', 'year', 'pred_month', 'pred_year']        \n",
    "        meta_features_ = [c for c in sub_df.columns if 'lag' in c or c in cols]\n",
    "        \n",
    "        for k in range(len(models)-2):\n",
    "            tertiary_preds = gbdt_models.forecast(sub_df, meta_features_, meta_targets, \n",
    "                                                    cv_models[f'tertiary_{i}_{k}'], True, False, False)\n",
    "\n",
    "            test_preds[f'tertiary_{i}_{k}'] = tertiary_preds['Target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a538fe4-179f-41a6-b8cc-6def48202c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.transpose(np.concatenate([[v] for v in test_preds.values()]), (1,0)).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24b52b3f-2f44-4560-aa99-ce31a2c057a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['Target'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c2162f4-d300-48b1-b569-63b62a6cb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fossil.models.gbdt import FossilGBDT\n",
    "gbdt_models = FossilGBDT()\n",
    "\n",
    "primary_preds = gbdt_models.forecast(test_data, feature_cols, target_cols, primary_cv_models, True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27286400-835e-479f-b3b3-02504653d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = prepare_submission_data(test, primary_preds, target_cols, pred_cols)\n",
    "sub_df = fossil_preproc.adjust_expanded_dates(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46657102-b5ec-491f-9ba4-944257ce6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# secondary_test = fossil_preproc.prepare_secondary_data(test_data, primary_preds[target_cols], target_cols, pred_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74bb2f6e-efc2-4ad7-ad7b-993605c483e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['preds', 'time_step', 'month', 'year', 'pred_month', 'pred_year']\n",
    "meta_features = [c for c in sub_df.columns if 'lag' in c or c in cols]\n",
    "meta_targets = 'Target'\n",
    "\n",
    "sub_df = gbdt_models.forecast(sub_df, meta_features, meta_targets, secondary_cv_models, True, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "411f9c5e-54b4-4275-8591-1b8f34b378db",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred_dates = prepare_test_dates(test) \n",
    "\n",
    "sub_df['month'] = pd.DataFrame(pred_dates*int(len(sub_df)/ModelsConfig.N_STEPS)).loc[:, 0].values\n",
    "sub_df['year'] = pd.DataFrame(pred_dates*int(len(sub_df)/ModelsConfig.N_STEPS)).loc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec5c1175-cb30-4361-b1ab-68a91275add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_df['Item_ID'] = sub_df['sku_name'].astype(str)+'_'+sub_df['month'].astype(int).astype(str)+'_'+sub_df['year'].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f960042-3177-478c-a7d7-9ccbd4b17be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_name = f'{OUTPUT_DIR}/fossil_{ModelsConfig.BASE_MODEL}_{ModelsConfig.META_LEARNER}_{secondary_val_mae}.csv'\n",
    "# sub_df[['Item_ID','Target']].to_csv(save_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "65dd572f-a40b-46f5-9c14-d66c7bf3048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f'{OUTPUT_DIR}/fossil_blended_{blended_mae}_cnnpred.csv'\n",
    "sub_df[['Item_ID','Target']].to_csv(save_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6eb7465-6dfa-43a9-9cab-d078008739c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>starting_inventory</th>\n",
       "      <th>sellin</th>\n",
       "      <th>sellin_channel_1</th>\n",
       "      <th>sellin_channel_2</th>\n",
       "      <th>sellin_channel_3</th>\n",
       "      <th>sellin_channel_4</th>\n",
       "      <th>sellin_channel_5</th>\n",
       "      <th>...</th>\n",
       "      <th>leftover_inventory_6month_MM</th>\n",
       "      <th>leftover_inventory_9month_MM</th>\n",
       "      <th>price_6month_MM</th>\n",
       "      <th>price_9month_MM</th>\n",
       "      <th>sku_coded</th>\n",
       "      <th>preds</th>\n",
       "      <th>time_step</th>\n",
       "      <th>Target</th>\n",
       "      <th>preds_</th>\n",
       "      <th>Item_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>35072.725079</td>\n",
       "      <td>0</td>\n",
       "      <td>43363.963964</td>\n",
       "      <td>66900.240234</td>\n",
       "      <td>ABEAHAMASHL_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>44672.654183</td>\n",
       "      <td>1</td>\n",
       "      <td>69069.227243</td>\n",
       "      <td>72972.720703</td>\n",
       "      <td>ABEAHAMASHL_12_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>43210.416403</td>\n",
       "      <td>2</td>\n",
       "      <td>77896.419235</td>\n",
       "      <td>77067.552734</td>\n",
       "      <td>ABEAHAMASHL_1_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>56187.085045</td>\n",
       "      <td>3</td>\n",
       "      <td>72459.635888</td>\n",
       "      <td>90953.414062</td>\n",
       "      <td>ABEAHAMASHL_2_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABEENNEARMAZZ</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>152963.0</td>\n",
       "      <td>58754.0</td>\n",
       "      <td>35455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68884.0</td>\n",
       "      <td>106365.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>159290.384286</td>\n",
       "      <td>0</td>\n",
       "      <td>187192.382883</td>\n",
       "      <td>236596.312500</td>\n",
       "      <td>ABEENNEARMAZZ_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>YOSHRENECARL</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>1527604.0</td>\n",
       "      <td>288705.0</td>\n",
       "      <td>199561.0</td>\n",
       "      <td>79014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>211717.0</td>\n",
       "      <td>214756.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>207297.427046</td>\n",
       "      <td>3</td>\n",
       "      <td>207270.552980</td>\n",
       "      <td>226664.242188</td>\n",
       "      <td>YOSHRENECARL_2_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>205218.138990</td>\n",
       "      <td>0</td>\n",
       "      <td>201965.760370</td>\n",
       "      <td>269986.476562</td>\n",
       "      <td>YOSHTLYNYOSHZZ_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>196515.846253</td>\n",
       "      <td>1</td>\n",
       "      <td>207205.092680</td>\n",
       "      <td>265641.031250</td>\n",
       "      <td>YOSHTLYNYOSHZZ_12_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>140360.727007</td>\n",
       "      <td>2</td>\n",
       "      <td>171106.706433</td>\n",
       "      <td>212035.796875</td>\n",
       "      <td>YOSHTLYNYOSHZZ_1_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>154050.194269</td>\n",
       "      <td>3</td>\n",
       "      <td>144737.369804</td>\n",
       "      <td>206124.699219</td>\n",
       "      <td>YOSHTLYNYOSHZZ_2_2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528 rows  84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sku_name  month  year  starting_inventory    sellin  \\\n",
       "0        ABEAHAMASHL     11  2021            410265.0   18234.0   \n",
       "1        ABEAHAMASHL     12  2021            410265.0   18234.0   \n",
       "2        ABEAHAMASHL      1  2022            410265.0   18234.0   \n",
       "3        ABEAHAMASHL      2  2022            410265.0   18234.0   \n",
       "4      ABEENNEARMAZZ     11  2021             42546.0  152963.0   \n",
       "...              ...    ...   ...                 ...       ...   \n",
       "1523    YOSHRENECARL      2  2022           1527604.0  288705.0   \n",
       "1524  YOSHTLYNYOSHZZ     11  2021            156002.0  163093.0   \n",
       "1525  YOSHTLYNYOSHZZ     12  2021            156002.0  163093.0   \n",
       "1526  YOSHTLYNYOSHZZ      1  2022            156002.0  163093.0   \n",
       "1527  YOSHTLYNYOSHZZ      2  2022            156002.0  163093.0   \n",
       "\n",
       "      sellin_channel_1  sellin_channel_2  sellin_channel_3  sellin_channel_4  \\\n",
       "0                  0.0               0.0               0.0            1013.0   \n",
       "1                  0.0               0.0               0.0            1013.0   \n",
       "2                  0.0               0.0               0.0            1013.0   \n",
       "3                  0.0               0.0               0.0            1013.0   \n",
       "4              58754.0           35455.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "1523          199561.0           79014.0               0.0               0.0   \n",
       "1524          122573.0           30390.0               0.0               0.0   \n",
       "1525          122573.0           30390.0               0.0               0.0   \n",
       "1526          122573.0           30390.0               0.0               0.0   \n",
       "1527          122573.0           30390.0               0.0               0.0   \n",
       "\n",
       "      sellin_channel_5  ...  leftover_inventory_6month_MM  \\\n",
       "0                  0.0  ...                      -13675.5   \n",
       "1                  0.0  ...                      -13675.5   \n",
       "2                  0.0  ...                      -13675.5   \n",
       "3                  0.0  ...                      -13675.5   \n",
       "4                  0.0  ...                       68884.0   \n",
       "...                ...  ...                           ...   \n",
       "1523               0.0  ...                      211717.0   \n",
       "1524               0.0  ...                      180820.5   \n",
       "1525               0.0  ...                      180820.5   \n",
       "1526               0.0  ...                      180820.5   \n",
       "1527               0.0  ...                      180820.5   \n",
       "\n",
       "      leftover_inventory_9month_MM  price_6month_MM  price_9month_MM  \\\n",
       "0                         -16208.0            149.0            149.0   \n",
       "1                         -16208.0            149.0            149.0   \n",
       "2                         -16208.0            149.0            149.0   \n",
       "3                         -16208.0            149.0            149.0   \n",
       "4                         106365.0            129.0            129.0   \n",
       "...                            ...              ...              ...   \n",
       "1523                      214756.0            129.0            129.0   \n",
       "1524                      200574.0            149.0            149.0   \n",
       "1525                      200574.0            149.0            149.0   \n",
       "1526                      200574.0            149.0            149.0   \n",
       "1527                      200574.0            149.0            149.0   \n",
       "\n",
       "      sku_coded          preds  time_step         Target         preds_  \\\n",
       "0          74.0   35072.725079          0   43363.963964   66900.240234   \n",
       "1          74.0   44672.654183          1   69069.227243   72972.720703   \n",
       "2          74.0   43210.416403          2   77896.419235   77067.552734   \n",
       "3          74.0   56187.085045          3   72459.635888   90953.414062   \n",
       "4         633.0  159290.384286          0  187192.382883  236596.312500   \n",
       "...         ...            ...        ...            ...            ...   \n",
       "1523      248.0  207297.427046          3  207270.552980  226664.242188   \n",
       "1524     1852.0  205218.138990          0  201965.760370  269986.476562   \n",
       "1525     1852.0  196515.846253          1  207205.092680  265641.031250   \n",
       "1526     1852.0  140360.727007          2  171106.706433  212035.796875   \n",
       "1527     1852.0  154050.194269          3  144737.369804  206124.699219   \n",
       "\n",
       "                     Item_ID  \n",
       "0        ABEAHAMASHL_11_2021  \n",
       "1        ABEAHAMASHL_12_2021  \n",
       "2         ABEAHAMASHL_1_2022  \n",
       "3         ABEAHAMASHL_2_2022  \n",
       "4      ABEENNEARMAZZ_11_2021  \n",
       "...                      ...  \n",
       "1523     YOSHRENECARL_2_2022  \n",
       "1524  YOSHTLYNYOSHZZ_11_2021  \n",
       "1525  YOSHTLYNYOSHZZ_12_2021  \n",
       "1526   YOSHTLYNYOSHZZ_1_2022  \n",
       "1527   YOSHTLYNYOSHZZ_2_2022  \n",
       "\n",
       "[1528 rows x 84 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df#[['month','year','pred_month','pred_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63a640bc-05bb-4b44-8b46-920a0196a354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>starting_inventory</th>\n",
       "      <th>sellin</th>\n",
       "      <th>sellin_channel_1</th>\n",
       "      <th>sellin_channel_2</th>\n",
       "      <th>sellin_channel_3</th>\n",
       "      <th>sellin_channel_4</th>\n",
       "      <th>sellin_channel_5</th>\n",
       "      <th>...</th>\n",
       "      <th>leftover_inventory_6month_MM</th>\n",
       "      <th>leftover_inventory_9month_MM</th>\n",
       "      <th>price_6month_MM</th>\n",
       "      <th>price_9month_MM</th>\n",
       "      <th>sku_coded</th>\n",
       "      <th>preds</th>\n",
       "      <th>time_step</th>\n",
       "      <th>Target</th>\n",
       "      <th>preds_</th>\n",
       "      <th>Item_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>85509.473288</td>\n",
       "      <td>0</td>\n",
       "      <td>49498.703989</td>\n",
       "      <td>8305.492067</td>\n",
       "      <td>ABEAHAMASHL_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>73653.580835</td>\n",
       "      <td>1</td>\n",
       "      <td>80219.634262</td>\n",
       "      <td>8772.766280</td>\n",
       "      <td>ABEAHAMASHL_12_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-23172.118497</td>\n",
       "      <td>2</td>\n",
       "      <td>73448.434937</td>\n",
       "      <td>6079.710530</td>\n",
       "      <td>ABEAHAMASHL_1_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-12692.608886</td>\n",
       "      <td>3</td>\n",
       "      <td>72998.606684</td>\n",
       "      <td>13084.023870</td>\n",
       "      <td>ABEAHAMASHL_2_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABEENNEARMAZZ</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>152963.0</td>\n",
       "      <td>58754.0</td>\n",
       "      <td>35455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68884.0</td>\n",
       "      <td>106365.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>190494.777842</td>\n",
       "      <td>0</td>\n",
       "      <td>189409.584717</td>\n",
       "      <td>54210.055156</td>\n",
       "      <td>ABEENNEARMAZZ_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>YOSHRENECARL</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>1527604.0</td>\n",
       "      <td>288705.0</td>\n",
       "      <td>199561.0</td>\n",
       "      <td>79014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>211717.0</td>\n",
       "      <td>214756.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>339418.875335</td>\n",
       "      <td>3</td>\n",
       "      <td>237634.229696</td>\n",
       "      <td>129088.896690</td>\n",
       "      <td>YOSHRENECARL_2_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>212768.907268</td>\n",
       "      <td>0</td>\n",
       "      <td>212067.121829</td>\n",
       "      <td>78222.107386</td>\n",
       "      <td>YOSHTLYNYOSHZZ_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>182937.246466</td>\n",
       "      <td>1</td>\n",
       "      <td>205343.266206</td>\n",
       "      <td>43850.017086</td>\n",
       "      <td>YOSHTLYNYOSHZZ_12_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>76368.337287</td>\n",
       "      <td>2</td>\n",
       "      <td>160262.601554</td>\n",
       "      <td>23255.564303</td>\n",
       "      <td>YOSHTLYNYOSHZZ_1_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>100341.706262</td>\n",
       "      <td>3</td>\n",
       "      <td>148248.558862</td>\n",
       "      <td>13812.885390</td>\n",
       "      <td>YOSHTLYNYOSHZZ_2_2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528 rows  84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sku_name  month  year  starting_inventory    sellin  \\\n",
       "0        ABEAHAMASHL     11  2021            410265.0   18234.0   \n",
       "1        ABEAHAMASHL     12  2021            410265.0   18234.0   \n",
       "2        ABEAHAMASHL      1  2022            410265.0   18234.0   \n",
       "3        ABEAHAMASHL      2  2022            410265.0   18234.0   \n",
       "4      ABEENNEARMAZZ     11  2021             42546.0  152963.0   \n",
       "...              ...    ...   ...                 ...       ...   \n",
       "1523    YOSHRENECARL      2  2022           1527604.0  288705.0   \n",
       "1524  YOSHTLYNYOSHZZ     11  2021            156002.0  163093.0   \n",
       "1525  YOSHTLYNYOSHZZ     12  2021            156002.0  163093.0   \n",
       "1526  YOSHTLYNYOSHZZ      1  2022            156002.0  163093.0   \n",
       "1527  YOSHTLYNYOSHZZ      2  2022            156002.0  163093.0   \n",
       "\n",
       "      sellin_channel_1  sellin_channel_2  sellin_channel_3  sellin_channel_4  \\\n",
       "0                  0.0               0.0               0.0            1013.0   \n",
       "1                  0.0               0.0               0.0            1013.0   \n",
       "2                  0.0               0.0               0.0            1013.0   \n",
       "3                  0.0               0.0               0.0            1013.0   \n",
       "4              58754.0           35455.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "1523          199561.0           79014.0               0.0               0.0   \n",
       "1524          122573.0           30390.0               0.0               0.0   \n",
       "1525          122573.0           30390.0               0.0               0.0   \n",
       "1526          122573.0           30390.0               0.0               0.0   \n",
       "1527          122573.0           30390.0               0.0               0.0   \n",
       "\n",
       "      sellin_channel_5  ...  leftover_inventory_6month_MM  \\\n",
       "0                  0.0  ...                      -13675.5   \n",
       "1                  0.0  ...                      -13675.5   \n",
       "2                  0.0  ...                      -13675.5   \n",
       "3                  0.0  ...                      -13675.5   \n",
       "4                  0.0  ...                       68884.0   \n",
       "...                ...  ...                           ...   \n",
       "1523               0.0  ...                      211717.0   \n",
       "1524               0.0  ...                      180820.5   \n",
       "1525               0.0  ...                      180820.5   \n",
       "1526               0.0  ...                      180820.5   \n",
       "1527               0.0  ...                      180820.5   \n",
       "\n",
       "      leftover_inventory_9month_MM  price_6month_MM  price_9month_MM  \\\n",
       "0                         -16208.0            149.0            149.0   \n",
       "1                         -16208.0            149.0            149.0   \n",
       "2                         -16208.0            149.0            149.0   \n",
       "3                         -16208.0            149.0            149.0   \n",
       "4                         106365.0            129.0            129.0   \n",
       "...                            ...              ...              ...   \n",
       "1523                      214756.0            129.0            129.0   \n",
       "1524                      200574.0            149.0            149.0   \n",
       "1525                      200574.0            149.0            149.0   \n",
       "1526                      200574.0            149.0            149.0   \n",
       "1527                      200574.0            149.0            149.0   \n",
       "\n",
       "      sku_coded          preds  time_step         Target         preds_  \\\n",
       "0          74.0   85509.473288          0   49498.703989    8305.492067   \n",
       "1          74.0   73653.580835          1   80219.634262    8772.766280   \n",
       "2          74.0  -23172.118497          2   73448.434937    6079.710530   \n",
       "3          74.0  -12692.608886          3   72998.606684   13084.023870   \n",
       "4         633.0  190494.777842          0  189409.584717   54210.055156   \n",
       "...         ...            ...        ...            ...            ...   \n",
       "1523      248.0  339418.875335          3  237634.229696  129088.896690   \n",
       "1524     1852.0  212768.907268          0  212067.121829   78222.107386   \n",
       "1525     1852.0  182937.246466          1  205343.266206   43850.017086   \n",
       "1526     1852.0   76368.337287          2  160262.601554   23255.564303   \n",
       "1527     1852.0  100341.706262          3  148248.558862   13812.885390   \n",
       "\n",
       "                     Item_ID  \n",
       "0        ABEAHAMASHL_11_2021  \n",
       "1        ABEAHAMASHL_12_2021  \n",
       "2         ABEAHAMASHL_1_2022  \n",
       "3         ABEAHAMASHL_2_2022  \n",
       "4      ABEENNEARMAZZ_11_2021  \n",
       "...                      ...  \n",
       "1523     YOSHRENECARL_2_2022  \n",
       "1524  YOSHTLYNYOSHZZ_11_2021  \n",
       "1525  YOSHTLYNYOSHZZ_12_2021  \n",
       "1526   YOSHTLYNYOSHZZ_1_2022  \n",
       "1527   YOSHTLYNYOSHZZ_2_2022  \n",
       "\n",
       "[1528 rows x 84 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df#[['month','year','pred_month','pred_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f211af45-99c3-4a7e-b3fe-85301ebd6da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>starting_inventory</th>\n",
       "      <th>sellin</th>\n",
       "      <th>sellin_channel_1</th>\n",
       "      <th>sellin_channel_2</th>\n",
       "      <th>sellin_channel_3</th>\n",
       "      <th>sellin_channel_4</th>\n",
       "      <th>sellin_channel_5</th>\n",
       "      <th>...</th>\n",
       "      <th>onhand_inventory_9month_MM</th>\n",
       "      <th>leftover_inventory_6month_MM</th>\n",
       "      <th>leftover_inventory_9month_MM</th>\n",
       "      <th>price_6month_MM</th>\n",
       "      <th>price_9month_MM</th>\n",
       "      <th>sku_coded</th>\n",
       "      <th>preds</th>\n",
       "      <th>time_step</th>\n",
       "      <th>Target</th>\n",
       "      <th>Item_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>291744.0</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>35072.725079</td>\n",
       "      <td>0</td>\n",
       "      <td>47640.930020</td>\n",
       "      <td>ABEAHAMASHL_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>291744.0</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>44672.654183</td>\n",
       "      <td>1</td>\n",
       "      <td>77943.376682</td>\n",
       "      <td>ABEAHAMASHL_12_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>291744.0</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>43210.416403</td>\n",
       "      <td>2</td>\n",
       "      <td>87058.224649</td>\n",
       "      <td>ABEAHAMASHL_1_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>291744.0</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>56187.085045</td>\n",
       "      <td>3</td>\n",
       "      <td>79939.259651</td>\n",
       "      <td>ABEAHAMASHL_2_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABEENNEARMAZZ</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>152963.0</td>\n",
       "      <td>58754.0</td>\n",
       "      <td>35455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>600709.0</td>\n",
       "      <td>68884.0</td>\n",
       "      <td>106365.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>159290.384286</td>\n",
       "      <td>0</td>\n",
       "      <td>203265.296056</td>\n",
       "      <td>ABEENNEARMAZZ_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>YOSHRENECARL</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>1527604.0</td>\n",
       "      <td>288705.0</td>\n",
       "      <td>199561.0</td>\n",
       "      <td>79014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>793179.0</td>\n",
       "      <td>211717.0</td>\n",
       "      <td>214756.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>207297.427046</td>\n",
       "      <td>3</td>\n",
       "      <td>223475.074890</td>\n",
       "      <td>YOSHRENECARL_2_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>193483.0</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>205218.138990</td>\n",
       "      <td>0</td>\n",
       "      <td>220793.851680</td>\n",
       "      <td>YOSHTLYNYOSHZZ_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>193483.0</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>196515.846253</td>\n",
       "      <td>1</td>\n",
       "      <td>223099.976004</td>\n",
       "      <td>YOSHTLYNYOSHZZ_12_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>193483.0</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>140360.727007</td>\n",
       "      <td>2</td>\n",
       "      <td>189837.577327</td>\n",
       "      <td>YOSHTLYNYOSHZZ_1_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>193483.0</td>\n",
       "      <td>180820.5</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>154050.194269</td>\n",
       "      <td>3</td>\n",
       "      <td>160543.516043</td>\n",
       "      <td>YOSHTLYNYOSHZZ_2_2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528 rows  83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sku_name  month  year  starting_inventory    sellin  \\\n",
       "0        ABEAHAMASHL     11  2021            410265.0   18234.0   \n",
       "1        ABEAHAMASHL     12  2021            410265.0   18234.0   \n",
       "2        ABEAHAMASHL      1  2022            410265.0   18234.0   \n",
       "3        ABEAHAMASHL      2  2022            410265.0   18234.0   \n",
       "4      ABEENNEARMAZZ     11  2021             42546.0  152963.0   \n",
       "...              ...    ...   ...                 ...       ...   \n",
       "1523    YOSHRENECARL      2  2022           1527604.0  288705.0   \n",
       "1524  YOSHTLYNYOSHZZ     11  2021            156002.0  163093.0   \n",
       "1525  YOSHTLYNYOSHZZ     12  2021            156002.0  163093.0   \n",
       "1526  YOSHTLYNYOSHZZ      1  2022            156002.0  163093.0   \n",
       "1527  YOSHTLYNYOSHZZ      2  2022            156002.0  163093.0   \n",
       "\n",
       "      sellin_channel_1  sellin_channel_2  sellin_channel_3  sellin_channel_4  \\\n",
       "0                  0.0               0.0               0.0            1013.0   \n",
       "1                  0.0               0.0               0.0            1013.0   \n",
       "2                  0.0               0.0               0.0            1013.0   \n",
       "3                  0.0               0.0               0.0            1013.0   \n",
       "4              58754.0           35455.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "1523          199561.0           79014.0               0.0               0.0   \n",
       "1524          122573.0           30390.0               0.0               0.0   \n",
       "1525          122573.0           30390.0               0.0               0.0   \n",
       "1526          122573.0           30390.0               0.0               0.0   \n",
       "1527          122573.0           30390.0               0.0               0.0   \n",
       "\n",
       "      sellin_channel_5  ...  onhand_inventory_9month_MM  \\\n",
       "0                  0.0  ...                    291744.0   \n",
       "1                  0.0  ...                    291744.0   \n",
       "2                  0.0  ...                    291744.0   \n",
       "3                  0.0  ...                    291744.0   \n",
       "4                  0.0  ...                    600709.0   \n",
       "...                ...  ...                         ...   \n",
       "1523               0.0  ...                    793179.0   \n",
       "1524               0.0  ...                    193483.0   \n",
       "1525               0.0  ...                    193483.0   \n",
       "1526               0.0  ...                    193483.0   \n",
       "1527               0.0  ...                    193483.0   \n",
       "\n",
       "      leftover_inventory_6month_MM  leftover_inventory_9month_MM  \\\n",
       "0                         -13675.5                      -16208.0   \n",
       "1                         -13675.5                      -16208.0   \n",
       "2                         -13675.5                      -16208.0   \n",
       "3                         -13675.5                      -16208.0   \n",
       "4                          68884.0                      106365.0   \n",
       "...                            ...                           ...   \n",
       "1523                      211717.0                      214756.0   \n",
       "1524                      180820.5                      200574.0   \n",
       "1525                      180820.5                      200574.0   \n",
       "1526                      180820.5                      200574.0   \n",
       "1527                      180820.5                      200574.0   \n",
       "\n",
       "      price_6month_MM  price_9month_MM  sku_coded          preds  time_step  \\\n",
       "0               149.0            149.0       74.0   35072.725079          0   \n",
       "1               149.0            149.0       74.0   44672.654183          1   \n",
       "2               149.0            149.0       74.0   43210.416403          2   \n",
       "3               149.0            149.0       74.0   56187.085045          3   \n",
       "4               129.0            129.0      633.0  159290.384286          0   \n",
       "...               ...              ...        ...            ...        ...   \n",
       "1523            129.0            129.0      248.0  207297.427046          3   \n",
       "1524            149.0            149.0     1852.0  205218.138990          0   \n",
       "1525            149.0            149.0     1852.0  196515.846253          1   \n",
       "1526            149.0            149.0     1852.0  140360.727007          2   \n",
       "1527            149.0            149.0     1852.0  154050.194269          3   \n",
       "\n",
       "             Target                 Item_ID  \n",
       "0      47640.930020     ABEAHAMASHL_11_2021  \n",
       "1      77943.376682     ABEAHAMASHL_12_2021  \n",
       "2      87058.224649      ABEAHAMASHL_1_2022  \n",
       "3      79939.259651      ABEAHAMASHL_2_2022  \n",
       "4     203265.296056   ABEENNEARMAZZ_11_2021  \n",
       "...             ...                     ...  \n",
       "1523  223475.074890     YOSHRENECARL_2_2022  \n",
       "1524  220793.851680  YOSHTLYNYOSHZZ_11_2021  \n",
       "1525  223099.976004  YOSHTLYNYOSHZZ_12_2021  \n",
       "1526  189837.577327   YOSHTLYNYOSHZZ_1_2022  \n",
       "1527  160543.516043   YOSHTLYNYOSHZZ_2_2022  \n",
       "\n",
       "[1528 rows x 83 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df#[['month','year','pred_month','pred_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f13d1e9a-a3f4-4822-97f3-cb43cddd3ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>starting_inventory</th>\n",
       "      <th>sellin</th>\n",
       "      <th>sellin_channel_1</th>\n",
       "      <th>sellin_channel_2</th>\n",
       "      <th>sellin_channel_3</th>\n",
       "      <th>sellin_channel_4</th>\n",
       "      <th>sellin_channel_5</th>\n",
       "      <th>...</th>\n",
       "      <th>onhand_inventory_6month_MM</th>\n",
       "      <th>onhand_inventory_9month_MM</th>\n",
       "      <th>leftover_inventory_6month_MM</th>\n",
       "      <th>leftover_inventory_9month_MM</th>\n",
       "      <th>price_6month_MM</th>\n",
       "      <th>price_9month_MM</th>\n",
       "      <th>sku_coded</th>\n",
       "      <th>target</th>\n",
       "      <th>preds</th>\n",
       "      <th>time_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1529123.50</td>\n",
       "      <td>1584332.0</td>\n",
       "      <td>50143.50</td>\n",
       "      <td>56728.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>358602.0</td>\n",
       "      <td>993988.457097</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1529123.50</td>\n",
       "      <td>1584332.0</td>\n",
       "      <td>50143.50</td>\n",
       "      <td>56728.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>963319.364140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1529123.50</td>\n",
       "      <td>1584332.0</td>\n",
       "      <td>50143.50</td>\n",
       "      <td>56728.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>638536.972105</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1529123.50</td>\n",
       "      <td>1584332.0</td>\n",
       "      <td>50143.50</td>\n",
       "      <td>56728.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>613783.671361</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABEANHARLE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>388992.0</td>\n",
       "      <td>321121.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38494.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1201924.50</td>\n",
       "      <td>1192301.0</td>\n",
       "      <td>-20260.00</td>\n",
       "      <td>10130.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1223.0</td>\n",
       "      <td>154989.0</td>\n",
       "      <td>420339.695346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172931</th>\n",
       "      <td>YOSHLEENBART</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>135742.0</td>\n",
       "      <td>152963.0</td>\n",
       "      <td>25325.0</td>\n",
       "      <td>10130.0</td>\n",
       "      <td>66858.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>252490.25</td>\n",
       "      <td>251224.0</td>\n",
       "      <td>191963.50</td>\n",
       "      <td>141820.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>237042.0</td>\n",
       "      <td>132547.547953</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172932</th>\n",
       "      <td>YOSHRENECARL</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>76988.0</td>\n",
       "      <td>84079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40520.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>...</td>\n",
       "      <td>823822.25</td>\n",
       "      <td>793179.0</td>\n",
       "      <td>219567.75</td>\n",
       "      <td>214756.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>456863.0</td>\n",
       "      <td>187077.629589</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172933</th>\n",
       "      <td>YOSHRENECARL</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>76988.0</td>\n",
       "      <td>84079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40520.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>...</td>\n",
       "      <td>823822.25</td>\n",
       "      <td>793179.0</td>\n",
       "      <td>219567.75</td>\n",
       "      <td>214756.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>362654.0</td>\n",
       "      <td>255827.652601</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172934</th>\n",
       "      <td>YOSHRENECARL</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>76988.0</td>\n",
       "      <td>84079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40520.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>...</td>\n",
       "      <td>823822.25</td>\n",
       "      <td>793179.0</td>\n",
       "      <td>219567.75</td>\n",
       "      <td>214756.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>241094.0</td>\n",
       "      <td>291710.736521</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172935</th>\n",
       "      <td>YOSHRENECARL</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>76988.0</td>\n",
       "      <td>84079.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40520.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>...</td>\n",
       "      <td>823822.25</td>\n",
       "      <td>793179.0</td>\n",
       "      <td>219567.75</td>\n",
       "      <td>214756.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>338342.0</td>\n",
       "      <td>254321.610931</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172936 rows  76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sku_name  month    year  starting_inventory    sellin  \\\n",
       "0        ABEAHAMASHL    5.0  2016.0           6844841.0  902583.0   \n",
       "1        ABEAHAMASHL    5.0  2016.0           6844841.0  902583.0   \n",
       "2        ABEAHAMASHL    5.0  2016.0           6844841.0  902583.0   \n",
       "3        ABEAHAMASHL    5.0  2016.0           6844841.0  902583.0   \n",
       "4         ABEANHARLE    5.0  2016.0                 0.0  388992.0   \n",
       "...              ...    ...     ...                 ...       ...   \n",
       "172931  YOSHLEENBART    4.0  2021.0            135742.0  152963.0   \n",
       "172932  YOSHRENECARL    4.0  2021.0             76988.0   84079.0   \n",
       "172933  YOSHRENECARL    4.0  2021.0             76988.0   84079.0   \n",
       "172934  YOSHRENECARL    4.0  2021.0             76988.0   84079.0   \n",
       "172935  YOSHRENECARL    4.0  2021.0             76988.0   84079.0   \n",
       "\n",
       "        sellin_channel_1  sellin_channel_2  sellin_channel_3  \\\n",
       "0               542968.0          236029.0           34442.0   \n",
       "1               542968.0          236029.0           34442.0   \n",
       "2               542968.0          236029.0           34442.0   \n",
       "3               542968.0          236029.0           34442.0   \n",
       "4               321121.0               0.0           38494.0   \n",
       "...                  ...               ...               ...   \n",
       "172931           25325.0           10130.0           66858.0   \n",
       "172932               0.0               0.0           40520.0   \n",
       "172933               0.0               0.0           40520.0   \n",
       "172934               0.0               0.0           40520.0   \n",
       "172935               0.0               0.0           40520.0   \n",
       "\n",
       "        sellin_channel_4  sellin_channel_5  ...  onhand_inventory_6month_MM  \\\n",
       "0                    0.0           14182.0  ...                  1529123.50   \n",
       "1                    0.0           14182.0  ...                  1529123.50   \n",
       "2                    0.0           14182.0  ...                  1529123.50   \n",
       "3                    0.0           14182.0  ...                  1529123.50   \n",
       "4                    0.0               0.0  ...                  1201924.50   \n",
       "...                  ...               ...  ...                         ...   \n",
       "172931            1013.0               0.0  ...                   252490.25   \n",
       "172932               0.0            1013.0  ...                   823822.25   \n",
       "172933               0.0            1013.0  ...                   823822.25   \n",
       "172934               0.0            1013.0  ...                   823822.25   \n",
       "172935               0.0            1013.0  ...                   823822.25   \n",
       "\n",
       "        onhand_inventory_9month_MM  leftover_inventory_6month_MM  \\\n",
       "0                        1584332.0                      50143.50   \n",
       "1                        1584332.0                      50143.50   \n",
       "2                        1584332.0                      50143.50   \n",
       "3                        1584332.0                      50143.50   \n",
       "4                        1192301.0                     -20260.00   \n",
       "...                            ...                           ...   \n",
       "172931                    251224.0                     191963.50   \n",
       "172932                    793179.0                     219567.75   \n",
       "172933                    793179.0                     219567.75   \n",
       "172934                    793179.0                     219567.75   \n",
       "172935                    793179.0                     219567.75   \n",
       "\n",
       "        leftover_inventory_9month_MM  price_6month_MM  price_9month_MM  \\\n",
       "0                            56728.0            145.0            145.0   \n",
       "1                            56728.0            145.0            145.0   \n",
       "2                            56728.0            145.0            145.0   \n",
       "3                            56728.0            145.0            145.0   \n",
       "4                            10130.0            125.0            125.0   \n",
       "...                              ...              ...              ...   \n",
       "172931                      141820.0            129.0            129.0   \n",
       "172932                      214756.0            129.0            129.0   \n",
       "172933                      214756.0            129.0            129.0   \n",
       "172934                      214756.0            129.0            129.0   \n",
       "172935                      214756.0            129.0            129.0   \n",
       "\n",
       "        sku_coded     target          preds  time_step  \n",
       "0            74.0   358602.0  993988.457097          0  \n",
       "1            74.0  1177106.0  963319.364140          1  \n",
       "2            74.0   968428.0  638536.972105          2  \n",
       "3            74.0   470032.0  613783.671361          3  \n",
       "4          1223.0   154989.0  420339.695346          0  \n",
       "...           ...        ...            ...        ...  \n",
       "172931      725.0   237042.0  132547.547953          3  \n",
       "172932      248.0   456863.0  187077.629589          0  \n",
       "172933      248.0   362654.0  255827.652601          1  \n",
       "172934      248.0   241094.0  291710.736521          2  \n",
       "172935      248.0   338342.0  254321.610931          3  \n",
       "\n",
       "[172936 rows x 76 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondary_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b222630-b626-4140-afa5-c9d7abeb76b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988d8ad-e7c0-4428-8b26-5edf803407a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_cols = [f'target_{i}' for i in range(ModelsConfig.N_STEPS)]\n",
    "pred_cols = [f'preds_{i}' for i in range(ModelsConfig.N_STEPS)]\n",
    "\n",
    "for i,oof in enumerate([primary_oof, secondary_oof]):\n",
    "    meta_base = base_data.copy()\n",
    "    meta_base[pred_cols] = oof.reshape(-1, ModelsConfig.N_STEPS)\n",
    "    meta_padded = meta_base.groupby(['month','year']).apply(fossil_preproc.pad_sku_sequence, pad_value=np.nan)\n",
    "    meta_padded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    meta_primary = meta_padded.drop(columns=pred_cols)\n",
    "    meta_oof = meta_padded[pred_cols].values\n",
    "    meta_expanded = fossil_preproc.expand_primary_data(meta_primary, meta_oof, target_cols, pred_cols)\n",
    "    \n",
    "    if i==0:\n",
    "        meta_data = meta_expanded.drop(columns=['preds']).copy()\n",
    "        \n",
    "    meta_data[f'preds_{i}'] = meta_expanded['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22dfa0-c8f2-4739-8289-3e2e10a58d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "meta_data['target'] = meta_data.groupby('sku_name')['target'].progress_transform(lambda x: x.fillna(x.median()))\n",
    "meta_data['target'] = meta_data.groupby(['month','year'])['target'].progress_transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a39f805-e5a6-46d2-baed-b1bac23f211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data.fillna(0, inplace=True)\n",
    "\n",
    "meta_dates = sorted([(m, y) for y,m in meta_data.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8759edf-404f-4ceb-84b7-fe414b05f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "meta_features = [c for c in meta_data.columns if 'preds' in c]\n",
    "meta_targets = 'target'\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "x_scaler.fit(meta_data[meta_features])\n",
    "y_scaler.fit(meta_data['target'].values.reshape(-1,1))\n",
    "\n",
    "meta_data[meta_features] = x_scaler.transform(meta_data[meta_features])\n",
    "meta_data[meta_targets] = y_scaler.transform(meta_data[meta_targets].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8d6d927-d51c-4028-b976-b42d056b6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data['time_step'] = meta_data.groupby(['sku_name','sku_coded','month','year']).cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12f4f1b0-f5d0-4a8a-8372-d22cc601400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def get_data(\n",
    "        data:pd.DataFrame, \n",
    "        dates:list, \n",
    "        feature_cols:list, \n",
    "        target_cols:list, \n",
    "        validation:bool=False\n",
    "        ):\n",
    "    \n",
    "    data_list = []\n",
    "    y_test = None\n",
    "    \n",
    "    for date in tqdm(dates):\n",
    "        time_step_data = data[data[['month','year']].apply(tuple, axis=1).isin([date])]\n",
    "        time_step_avg = time_step_data.groupby(['sku_coded','time_step']).mean().reset_index()\n",
    "        \n",
    "        grouped_data = time_step_avg.sort_values(['sku_coded','year','month']).groupby(['sku_coded'])\n",
    "                \n",
    "        sequence = np.array([v[feature_cols] for k,v in grouped_data])\n",
    "        targets = time_step_avg[target_cols].values\n",
    "        \n",
    "        if validation:\n",
    "            y_test = time_step_data[time_step_data['sku_name']!=0]\n",
    "        data_list.append(((sequence, targets), y_test))\n",
    "        \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36e73a85-7aed-4eda-86e3-7dc90d66b680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_features = [c for c in meta_data.columns if 'preds' in c]\n",
    "meta_targets = 'target'\n",
    "\n",
    "meta_train = get_data(meta_data, meta_dates[:-1], meta_features, meta_targets)\n",
    "meta_val = get_data(meta_data, meta_dates[1:], meta_features, meta_targets, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f31e01d7-3607-440e-9cea-8fd69f1486e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 1 from 1.0/2016.0 to 1.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.1191\tVal Loss: 0.3690\tVal MAE: 4264309.90\telapsed: 0.02 mins\n",
      "Epoch 2: Train Loss: 0.3600\tVal Loss: 0.1409\tVal MAE: 1685420.66\telapsed: 0.02 mins\n",
      "Epoch 3: Train Loss: 0.1579\tVal Loss: 0.1086\tVal MAE: 1295376.10\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.1084\tVal Loss: 0.1070\tVal MAE: 1278222.50\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.1069\tVal Loss: 0.1053\tVal MAE: 1259055.51\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.1051\tVal Loss: 0.1035\tVal MAE: 1238796.28\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.1032\tVal Loss: 0.1016\tVal MAE: 1217961.49\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.1013\tVal Loss: 0.0997\tVal MAE: 1196713.11\telapsed: 0.02 mins\n",
      "Epoch 9: Train Loss: 0.0994\tVal Loss: 0.0977\tVal MAE: 1175399.43\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0974\tVal Loss: 0.0957\tVal MAE: 1154044.64\telapsed: 0.02 mins\n",
      "Epoch 11: Train Loss: 0.0954\tVal Loss: 0.0938\tVal MAE: 1132702.41\telapsed: 0.02 mins\n",
      "Epoch 12: Train Loss: 0.0934\tVal Loss: 0.0919\tVal MAE: 1111464.94\telapsed: 0.02 mins\n",
      "Epoch 13: Train Loss: 0.0915\tVal Loss: 0.0899\tVal MAE: 1090238.23\telapsed: 0.02 mins\n",
      "Epoch 14: Train Loss: 0.0895\tVal Loss: 0.0880\tVal MAE: 1069123.82\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0876\tVal Loss: 0.0861\tVal MAE: 1048053.50\telapsed: 0.02 mins\n",
      "Epoch 16: Train Loss: 0.0856\tVal Loss: 0.0842\tVal MAE: 1027257.04\telapsed: 0.02 mins\n",
      "Epoch 17: Train Loss: 0.0837\tVal Loss: 0.0824\tVal MAE: 1006764.00\telapsed: 0.02 mins\n",
      "Epoch 18: Train Loss: 0.0818\tVal Loss: 0.0805\tVal MAE: 986589.94\telapsed: 0.02 mins\n",
      "Epoch 19: Train Loss: 0.0800\tVal Loss: 0.0787\tVal MAE: 966645.32\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0781\tVal Loss: 0.0769\tVal MAE: 946935.83\telapsed: 0.02 mins\n",
      "Epoch 21: Train Loss: 0.0763\tVal Loss: 0.0751\tVal MAE: 927480.92\telapsed: 0.02 mins\n",
      "Epoch 22: Train Loss: 0.0745\tVal Loss: 0.0733\tVal MAE: 908318.10\telapsed: 0.02 mins\n",
      "Epoch 23: Train Loss: 0.0727\tVal Loss: 0.0716\tVal MAE: 889285.98\telapsed: 0.02 mins\n",
      "Epoch 24: Train Loss: 0.0709\tVal Loss: 0.0699\tVal MAE: 870364.13\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0691\tVal Loss: 0.0681\tVal MAE: 851504.22\telapsed: 0.02 mins\n",
      "Epoch 26: Train Loss: 0.0674\tVal Loss: 0.0664\tVal MAE: 832868.41\telapsed: 0.02 mins\n",
      "Epoch 27: Train Loss: 0.0657\tVal Loss: 0.0648\tVal MAE: 814646.76\telapsed: 0.02 mins\n",
      "Epoch 28: Train Loss: 0.0640\tVal Loss: 0.0631\tVal MAE: 796794.87\telapsed: 0.02 mins\n",
      "Epoch 29: Train Loss: 0.0623\tVal Loss: 0.0615\tVal MAE: 779217.52\telapsed: 0.02 mins\n",
      "Epoch 30: Train Loss: 0.0607\tVal Loss: 0.0599\tVal MAE: 761945.44\telapsed: 0.02 mins\n",
      "Epoch 31: Train Loss: 0.0591\tVal Loss: 0.0583\tVal MAE: 744871.82\telapsed: 0.02 mins\n",
      "Epoch 32: Train Loss: 0.0575\tVal Loss: 0.0568\tVal MAE: 728115.18\telapsed: 0.02 mins\n",
      "Epoch 33: Train Loss: 0.0559\tVal Loss: 0.0553\tVal MAE: 711569.06\telapsed: 0.02 mins\n",
      "Epoch 34: Train Loss: 0.0543\tVal Loss: 0.0537\tVal MAE: 695525.30\telapsed: 0.02 mins\n",
      "Epoch 35: Train Loss: 0.0528\tVal Loss: 0.0523\tVal MAE: 679835.60\telapsed: 0.02 mins\n",
      "Epoch 36: Train Loss: 0.0512\tVal Loss: 0.0508\tVal MAE: 664322.13\telapsed: 0.02 mins\n",
      "Epoch 37: Train Loss: 0.0497\tVal Loss: 0.0493\tVal MAE: 648726.51\telapsed: 0.02 mins\n",
      "Epoch 38: Train Loss: 0.0482\tVal Loss: 0.0479\tVal MAE: 633334.28\telapsed: 0.02 mins\n",
      "Epoch 39: Train Loss: 0.0468\tVal Loss: 0.0465\tVal MAE: 618357.13\telapsed: 0.02 mins\n",
      "Epoch 40: Train Loss: 0.0453\tVal Loss: 0.0451\tVal MAE: 603595.19\telapsed: 0.02 mins\n",
      "Epoch 41: Train Loss: 0.0439\tVal Loss: 0.0437\tVal MAE: 589027.88\telapsed: 0.02 mins\n",
      "Epoch 42: Train Loss: 0.0425\tVal Loss: 0.0424\tVal MAE: 574860.28\telapsed: 0.02 mins\n",
      "Epoch 43: Train Loss: 0.0411\tVal Loss: 0.0411\tVal MAE: 561051.60\telapsed: 0.02 mins\n",
      "Epoch 44: Train Loss: 0.0398\tVal Loss: 0.0398\tVal MAE: 547456.80\telapsed: 0.02 mins\n",
      "Epoch 45: Train Loss: 0.0385\tVal Loss: 0.0385\tVal MAE: 534168.84\telapsed: 0.02 mins\n",
      "Epoch 46: Train Loss: 0.0372\tVal Loss: 0.0373\tVal MAE: 521080.63\telapsed: 0.02 mins\n",
      "Epoch 47: Train Loss: 0.0359\tVal Loss: 0.0361\tVal MAE: 508213.70\telapsed: 0.02 mins\n",
      "Epoch 48: Train Loss: 0.0347\tVal Loss: 0.0349\tVal MAE: 495651.03\telapsed: 0.02 mins\n",
      "Epoch 49: Train Loss: 0.0334\tVal Loss: 0.0337\tVal MAE: 483422.83\telapsed: 0.02 mins\n",
      "Epoch 50: Train Loss: 0.0322\tVal Loss: 0.0325\tVal MAE: 471327.14\telapsed: 0.02 mins\n",
      "Epoch 51: Train Loss: 0.0310\tVal Loss: 0.0314\tVal MAE: 459393.26\telapsed: 0.02 mins\n",
      "Epoch 52: Train Loss: 0.0299\tVal Loss: 0.0303\tVal MAE: 447796.95\telapsed: 0.03 mins\n",
      "Epoch 53: Train Loss: 0.0287\tVal Loss: 0.0292\tVal MAE: 436467.09\telapsed: 0.03 mins\n",
      "Epoch 54: Train Loss: 0.0276\tVal Loss: 0.0282\tVal MAE: 425515.83\telapsed: 0.03 mins\n",
      "Epoch 55: Train Loss: 0.0265\tVal Loss: 0.0272\tVal MAE: 414821.70\telapsed: 0.03 mins\n",
      "Epoch 56: Train Loss: 0.0255\tVal Loss: 0.0262\tVal MAE: 404546.36\telapsed: 0.03 mins\n",
      "Epoch 57: Train Loss: 0.0244\tVal Loss: 0.0253\tVal MAE: 394488.25\telapsed: 0.03 mins\n",
      "Epoch 58: Train Loss: 0.0235\tVal Loss: 0.0244\tVal MAE: 384886.93\telapsed: 0.03 mins\n",
      "Epoch 59: Train Loss: 0.0225\tVal Loss: 0.0235\tVal MAE: 375684.15\telapsed: 0.03 mins\n",
      "Epoch 60: Train Loss: 0.0216\tVal Loss: 0.0226\tVal MAE: 366829.68\telapsed: 0.03 mins\n",
      "Epoch 61: Train Loss: 0.0207\tVal Loss: 0.0218\tVal MAE: 358136.37\telapsed: 0.03 mins\n",
      "Epoch 62: Train Loss: 0.0198\tVal Loss: 0.0210\tVal MAE: 349612.01\telapsed: 0.03 mins\n",
      "Epoch 63: Train Loss: 0.0189\tVal Loss: 0.0202\tVal MAE: 341320.27\telapsed: 0.03 mins\n",
      "Epoch 64: Train Loss: 0.0181\tVal Loss: 0.0194\tVal MAE: 333443.21\telapsed: 0.03 mins\n",
      "Epoch 65: Train Loss: 0.0173\tVal Loss: 0.0187\tVal MAE: 325792.61\telapsed: 0.03 mins\n",
      "Epoch 66: Train Loss: 0.0165\tVal Loss: 0.0180\tVal MAE: 318292.60\telapsed: 0.03 mins\n",
      "Epoch 67: Train Loss: 0.0158\tVal Loss: 0.0173\tVal MAE: 310815.00\telapsed: 0.03 mins\n",
      "Epoch 68: Train Loss: 0.0150\tVal Loss: 0.0166\tVal MAE: 303667.16\telapsed: 0.03 mins\n",
      "Epoch 69: Train Loss: 0.0143\tVal Loss: 0.0160\tVal MAE: 296851.86\telapsed: 0.03 mins\n",
      "Epoch 70: Train Loss: 0.0136\tVal Loss: 0.0154\tVal MAE: 290294.51\telapsed: 0.03 mins\n",
      "Epoch 71: Train Loss: 0.0130\tVal Loss: 0.0148\tVal MAE: 283899.00\telapsed: 0.03 mins\n",
      "Epoch 72: Train Loss: 0.0124\tVal Loss: 0.0142\tVal MAE: 277652.28\telapsed: 0.03 mins\n",
      "Epoch 73: Train Loss: 0.0118\tVal Loss: 0.0137\tVal MAE: 271609.24\telapsed: 0.03 mins\n",
      "Epoch 74: Train Loss: 0.0112\tVal Loss: 0.0131\tVal MAE: 265806.97\telapsed: 0.03 mins\n",
      "Epoch 75: Train Loss: 0.0107\tVal Loss: 0.0126\tVal MAE: 260001.85\telapsed: 0.03 mins\n",
      "Epoch 76: Train Loss: 0.0101\tVal Loss: 0.0122\tVal MAE: 254510.84\telapsed: 0.03 mins\n",
      "Epoch 77: Train Loss: 0.0096\tVal Loss: 0.0117\tVal MAE: 249303.80\telapsed: 0.03 mins\n",
      "Epoch 78: Train Loss: 0.0091\tVal Loss: 0.0113\tVal MAE: 244285.30\telapsed: 0.03 mins\n",
      "Epoch 79: Train Loss: 0.0087\tVal Loss: 0.0108\tVal MAE: 239382.47\telapsed: 0.03 mins\n",
      "Epoch 80: Train Loss: 0.0082\tVal Loss: 0.0105\tVal MAE: 234773.49\telapsed: 0.03 mins\n",
      "Epoch 81: Train Loss: 0.0078\tVal Loss: 0.0101\tVal MAE: 230492.02\telapsed: 0.03 mins\n",
      "Epoch 82: Train Loss: 0.0074\tVal Loss: 0.0097\tVal MAE: 226419.28\telapsed: 0.03 mins\n",
      "Epoch 83: Train Loss: 0.0070\tVal Loss: 0.0094\tVal MAE: 222576.27\telapsed: 0.03 mins\n",
      "Epoch 84: Train Loss: 0.0066\tVal Loss: 0.0091\tVal MAE: 218973.90\telapsed: 0.03 mins\n",
      "Epoch 85: Train Loss: 0.0063\tVal Loss: 0.0088\tVal MAE: 215553.52\telapsed: 0.03 mins\n",
      "Epoch 86: Train Loss: 0.0059\tVal Loss: 0.0085\tVal MAE: 212303.15\telapsed: 0.03 mins\n",
      "Epoch 87: Train Loss: 0.0056\tVal Loss: 0.0083\tVal MAE: 209261.94\telapsed: 0.03 mins\n",
      "Epoch 88: Train Loss: 0.0053\tVal Loss: 0.0080\tVal MAE: 206548.34\telapsed: 0.03 mins\n",
      "Epoch 89: Train Loss: 0.0050\tVal Loss: 0.0078\tVal MAE: 204152.51\telapsed: 0.03 mins\n",
      "Epoch 90: Train Loss: 0.0048\tVal Loss: 0.0076\tVal MAE: 201766.50\telapsed: 0.03 mins\n",
      "Epoch 91: Train Loss: 0.0045\tVal Loss: 0.0073\tVal MAE: 199218.24\telapsed: 0.03 mins\n",
      "Epoch 92: Train Loss: 0.0042\tVal Loss: 0.0071\tVal MAE: 196643.08\telapsed: 0.03 mins\n",
      "Epoch 93: Train Loss: 0.0040\tVal Loss: 0.0069\tVal MAE: 194088.35\telapsed: 0.03 mins\n",
      "Epoch 94: Train Loss: 0.0038\tVal Loss: 0.0068\tVal MAE: 191834.85\telapsed: 0.03 mins\n",
      "Epoch 95: Train Loss: 0.0036\tVal Loss: 0.0066\tVal MAE: 189892.26\telapsed: 0.03 mins\n",
      "Epoch 96: Train Loss: 0.0034\tVal Loss: 0.0065\tVal MAE: 188092.66\telapsed: 0.03 mins\n",
      "Epoch 97: Train Loss: 0.0032\tVal Loss: 0.0063\tVal MAE: 186263.55\telapsed: 0.03 mins\n",
      "Epoch 98: Train Loss: 0.0030\tVal Loss: 0.0062\tVal MAE: 184516.95\telapsed: 0.03 mins\n",
      "Epoch 99: Train Loss: 0.0028\tVal Loss: 0.0060\tVal MAE: 182835.51\telapsed: 0.03 mins\n",
      "Epoch 100: Train Loss: 0.0027\tVal Loss: 0.0059\tVal MAE: 181376.78\telapsed: 0.03 mins\n",
      "Epoch 101: Train Loss: 0.0025\tVal Loss: 0.0058\tVal MAE: 180031.80\telapsed: 0.03 mins\n",
      "Epoch 102: Train Loss: 0.0024\tVal Loss: 0.0057\tVal MAE: 178656.50\telapsed: 0.03 mins\n",
      "Epoch 103: Train Loss: 0.0023\tVal Loss: 0.0056\tVal MAE: 177327.48\telapsed: 0.03 mins\n",
      "Epoch 104: Train Loss: 0.0021\tVal Loss: 0.0055\tVal MAE: 176083.70\telapsed: 0.03 mins\n",
      "Epoch 105: Train Loss: 0.0020\tVal Loss: 0.0054\tVal MAE: 174888.79\telapsed: 0.03 mins\n",
      "Epoch 106: Train Loss: 0.0019\tVal Loss: 0.0054\tVal MAE: 173693.99\telapsed: 0.03 mins\n",
      "Epoch 107: Train Loss: 0.0018\tVal Loss: 0.0053\tVal MAE: 172566.36\telapsed: 0.03 mins\n",
      "Epoch 108: Train Loss: 0.0017\tVal Loss: 0.0052\tVal MAE: 171535.70\telapsed: 0.03 mins\n",
      "Epoch 109: Train Loss: 0.0016\tVal Loss: 0.0052\tVal MAE: 170546.64\telapsed: 0.03 mins\n",
      "Epoch 110: Train Loss: 0.0015\tVal Loss: 0.0051\tVal MAE: 169511.28\telapsed: 0.03 mins\n",
      "Epoch 111: Train Loss: 0.0015\tVal Loss: 0.0050\tVal MAE: 168626.94\telapsed: 0.03 mins\n",
      "Epoch 112: Train Loss: 0.0014\tVal Loss: 0.0050\tVal MAE: 167914.70\telapsed: 0.03 mins\n",
      "Epoch 113: Train Loss: 0.0013\tVal Loss: 0.0049\tVal MAE: 167219.85\telapsed: 0.03 mins\n",
      "Epoch 114: Train Loss: 0.0012\tVal Loss: 0.0049\tVal MAE: 166566.73\telapsed: 0.03 mins\n",
      "Epoch 115: Train Loss: 0.0012\tVal Loss: 0.0049\tVal MAE: 165916.57\telapsed: 0.03 mins\n",
      "Epoch 116: Train Loss: 0.0011\tVal Loss: 0.0048\tVal MAE: 165345.96\telapsed: 0.03 mins\n",
      "Epoch 117: Train Loss: 0.0011\tVal Loss: 0.0048\tVal MAE: 164818.76\telapsed: 0.04 mins\n",
      "Epoch 118: Train Loss: 0.0010\tVal Loss: 0.0048\tVal MAE: 164402.64\telapsed: 0.04 mins\n",
      "Epoch 119: Train Loss: 0.0010\tVal Loss: 0.0047\tVal MAE: 164017.81\telapsed: 0.04 mins\n",
      "Epoch 120: Train Loss: 0.0009\tVal Loss: 0.0047\tVal MAE: 163602.92\telapsed: 0.04 mins\n",
      "Epoch 121: Train Loss: 0.0009\tVal Loss: 0.0047\tVal MAE: 163078.09\telapsed: 0.04 mins\n",
      "Epoch 122: Train Loss: 0.0009\tVal Loss: 0.0047\tVal MAE: 162521.29\telapsed: 0.04 mins\n",
      "Epoch 123: Train Loss: 0.0008\tVal Loss: 0.0047\tVal MAE: 162046.30\telapsed: 0.04 mins\n",
      "Epoch 124: Train Loss: 0.0008\tVal Loss: 0.0047\tVal MAE: 161691.93\telapsed: 0.04 mins\n",
      "Epoch 125: Train Loss: 0.0008\tVal Loss: 0.0046\tVal MAE: 161353.36\telapsed: 0.04 mins\n",
      "Epoch 126: Train Loss: 0.0007\tVal Loss: 0.0046\tVal MAE: 160948.37\telapsed: 0.04 mins\n",
      "Epoch 127: Train Loss: 0.0007\tVal Loss: 0.0046\tVal MAE: 160635.18\telapsed: 0.04 mins\n",
      "Epoch 128: Train Loss: 0.0007\tVal Loss: 0.0046\tVal MAE: 160441.40\telapsed: 0.04 mins\n",
      "Epoch 129: Train Loss: 0.0007\tVal Loss: 0.0046\tVal MAE: 160285.10\telapsed: 0.04 mins\n",
      "Epoch 130: Train Loss: 0.0006\tVal Loss: 0.0046\tVal MAE: 160139.74\telapsed: 0.04 mins\n",
      "Epoch 131: Train Loss: 0.0006\tVal Loss: 0.0046\tVal MAE: 159967.92\telapsed: 0.04 mins\n",
      "Epoch 132: Train Loss: 0.0006\tVal Loss: 0.0046\tVal MAE: 159713.89\telapsed: 0.04 mins\n",
      "Epoch 133: Train Loss: 0.0006\tVal Loss: 0.0045\tVal MAE: 159468.11\telapsed: 0.04 mins\n",
      "Epoch 134: Train Loss: 0.0006\tVal Loss: 0.0045\tVal MAE: 159221.34\telapsed: 0.04 mins\n",
      "Epoch 135: Train Loss: 0.0005\tVal Loss: 0.0045\tVal MAE: 159131.73\telapsed: 0.04 mins\n",
      "Epoch 136: Train Loss: 0.0005\tVal Loss: 0.0045\tVal MAE: 159093.76\telapsed: 0.04 mins\n",
      "Epoch 137: Train Loss: 0.0005\tVal Loss: 0.0045\tVal MAE: 158982.41\telapsed: 0.04 mins\n",
      "Epoch 138: Train Loss: 0.0005\tVal Loss: 0.0045\tVal MAE: 158816.58\telapsed: 0.04 mins\n",
      "Epoch 139: Train Loss: 0.0005\tVal Loss: 0.0045\tVal MAE: 158628.32\telapsed: 0.04 mins\n",
      "Epoch 140: Train Loss: 0.0005\tVal Loss: 0.0045\tVal MAE: 158522.43\telapsed: 0.04 mins\n",
      "Epoch 141: Train Loss: 0.0005\tVal Loss: 0.0045\tVal MAE: 158541.04\telapsed: 0.04 mins\n",
      "Epoch 142: Train Loss: 0.0005\tVal Loss: 0.0045\tVal MAE: 158508.88\telapsed: 0.04 mins\n",
      "Epoch 143: Train Loss: 0.0005\tVal Loss: 0.0045\tVal MAE: 158486.76\telapsed: 0.04 mins\n",
      "Epoch 144: Train Loss: 0.0004\tVal Loss: 0.0045\tVal MAE: 158472.50\telapsed: 0.04 mins\n",
      "Epoch 145: Train Loss: 0.0004\tVal Loss: 0.0045\tVal MAE: 158387.25\telapsed: 0.04 mins\n",
      "Epoch 146: Train Loss: 0.0004\tVal Loss: 0.0045\tVal MAE: 158334.90\telapsed: 0.04 mins\n",
      "Epoch 147: Train Loss: 0.0004\tVal Loss: 0.0045\tVal MAE: 158394.54\telapsed: 0.04 mins\n",
      "Epoch 148: Train Loss: 0.0004\tVal Loss: 0.0045\tVal MAE: 158476.04\telapsed: 0.04 mins\n",
      "Epoch 149: Train Loss: 0.0004\tVal Loss: 0.0045\tVal MAE: 158528.51\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 146\tVal loss: 0.0045\tVal MAE: 158334.90\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 2 from 1.0/2016.0 to 2.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.5644\tVal Loss: 0.2063\tVal MAE: 2374873.82\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.1077\tVal Loss: 0.1064\tVal MAE: 1264955.56\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.1050\tVal Loss: 0.1035\tVal MAE: 1232647.32\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.1018\tVal Loss: 0.1002\tVal MAE: 1196991.79\telapsed: 0.00 mins\n",
      "Epoch 5: Train Loss: 0.0985\tVal Loss: 0.0968\tVal MAE: 1159774.31\telapsed: 0.00 mins\n",
      "Epoch 6: Train Loss: 0.0950\tVal Loss: 0.0934\tVal MAE: 1122084.79\telapsed: 0.00 mins\n",
      "Epoch 7: Train Loss: 0.0915\tVal Loss: 0.0899\tVal MAE: 1084383.39\telapsed: 0.00 mins\n",
      "Epoch 8: Train Loss: 0.0881\tVal Loss: 0.0865\tVal MAE: 1046917.51\telapsed: 0.00 mins\n",
      "Epoch 9: Train Loss: 0.0847\tVal Loss: 0.0832\tVal MAE: 1009817.56\telapsed: 0.00 mins\n",
      "Epoch 10: Train Loss: 0.0813\tVal Loss: 0.0798\tVal MAE: 973051.65\telapsed: 0.00 mins\n",
      "Epoch 11: Train Loss: 0.0780\tVal Loss: 0.0765\tVal MAE: 936993.94\telapsed: 0.00 mins\n",
      "Epoch 12: Train Loss: 0.0748\tVal Loss: 0.0733\tVal MAE: 901993.15\telapsed: 0.00 mins\n",
      "Epoch 13: Train Loss: 0.0715\tVal Loss: 0.0701\tVal MAE: 867859.05\telapsed: 0.00 mins\n",
      "Epoch 14: Train Loss: 0.0684\tVal Loss: 0.0670\tVal MAE: 834192.79\telapsed: 0.00 mins\n",
      "Epoch 15: Train Loss: 0.0652\tVal Loss: 0.0639\tVal MAE: 801331.13\telapsed: 0.00 mins\n",
      "Epoch 16: Train Loss: 0.0622\tVal Loss: 0.0609\tVal MAE: 769037.67\telapsed: 0.00 mins\n",
      "Epoch 17: Train Loss: 0.0592\tVal Loss: 0.0580\tVal MAE: 737421.03\telapsed: 0.00 mins\n",
      "Epoch 18: Train Loss: 0.0562\tVal Loss: 0.0551\tVal MAE: 706674.20\telapsed: 0.01 mins\n",
      "Epoch 19: Train Loss: 0.0533\tVal Loss: 0.0522\tVal MAE: 676744.01\telapsed: 0.01 mins\n",
      "Epoch 20: Train Loss: 0.0505\tVal Loss: 0.0495\tVal MAE: 646910.56\telapsed: 0.01 mins\n",
      "Epoch 21: Train Loss: 0.0478\tVal Loss: 0.0468\tVal MAE: 617494.22\telapsed: 0.01 mins\n",
      "Epoch 22: Train Loss: 0.0451\tVal Loss: 0.0442\tVal MAE: 589126.44\telapsed: 0.01 mins\n",
      "Epoch 23: Train Loss: 0.0424\tVal Loss: 0.0416\tVal MAE: 561614.16\telapsed: 0.01 mins\n",
      "Epoch 24: Train Loss: 0.0399\tVal Loss: 0.0391\tVal MAE: 535264.23\telapsed: 0.01 mins\n",
      "Epoch 25: Train Loss: 0.0374\tVal Loss: 0.0367\tVal MAE: 510099.56\telapsed: 0.01 mins\n",
      "Epoch 26: Train Loss: 0.0350\tVal Loss: 0.0344\tVal MAE: 485669.96\telapsed: 0.01 mins\n",
      "Epoch 27: Train Loss: 0.0327\tVal Loss: 0.0322\tVal MAE: 462403.50\telapsed: 0.01 mins\n",
      "Epoch 28: Train Loss: 0.0305\tVal Loss: 0.0301\tVal MAE: 439919.87\telapsed: 0.01 mins\n",
      "Epoch 29: Train Loss: 0.0284\tVal Loss: 0.0280\tVal MAE: 418312.66\telapsed: 0.01 mins\n",
      "Epoch 30: Train Loss: 0.0264\tVal Loss: 0.0261\tVal MAE: 397576.88\telapsed: 0.01 mins\n",
      "Epoch 31: Train Loss: 0.0245\tVal Loss: 0.0243\tVal MAE: 378030.67\telapsed: 0.01 mins\n",
      "Epoch 32: Train Loss: 0.0227\tVal Loss: 0.0225\tVal MAE: 359509.66\telapsed: 0.01 mins\n",
      "Epoch 33: Train Loss: 0.0209\tVal Loss: 0.0209\tVal MAE: 342020.08\telapsed: 0.01 mins\n",
      "Epoch 34: Train Loss: 0.0193\tVal Loss: 0.0194\tVal MAE: 325139.42\telapsed: 0.01 mins\n",
      "Epoch 35: Train Loss: 0.0178\tVal Loss: 0.0179\tVal MAE: 309026.85\telapsed: 0.01 mins\n",
      "Epoch 36: Train Loss: 0.0164\tVal Loss: 0.0165\tVal MAE: 293889.60\telapsed: 0.01 mins\n",
      "Epoch 37: Train Loss: 0.0151\tVal Loss: 0.0153\tVal MAE: 279599.35\telapsed: 0.01 mins\n",
      "Epoch 38: Train Loss: 0.0139\tVal Loss: 0.0141\tVal MAE: 266679.61\telapsed: 0.01 mins\n",
      "Epoch 39: Train Loss: 0.0127\tVal Loss: 0.0130\tVal MAE: 254928.04\telapsed: 0.01 mins\n",
      "Epoch 40: Train Loss: 0.0117\tVal Loss: 0.0120\tVal MAE: 244002.06\telapsed: 0.01 mins\n",
      "Epoch 41: Train Loss: 0.0107\tVal Loss: 0.0111\tVal MAE: 234231.49\telapsed: 0.01 mins\n",
      "Epoch 42: Train Loss: 0.0098\tVal Loss: 0.0103\tVal MAE: 225132.54\telapsed: 0.01 mins\n",
      "Epoch 43: Train Loss: 0.0089\tVal Loss: 0.0095\tVal MAE: 216814.92\telapsed: 0.01 mins\n",
      "Epoch 44: Train Loss: 0.0082\tVal Loss: 0.0088\tVal MAE: 209280.30\telapsed: 0.01 mins\n",
      "Epoch 45: Train Loss: 0.0074\tVal Loss: 0.0081\tVal MAE: 202251.46\telapsed: 0.01 mins\n",
      "Epoch 46: Train Loss: 0.0068\tVal Loss: 0.0075\tVal MAE: 195737.75\telapsed: 0.01 mins\n",
      "Epoch 47: Train Loss: 0.0063\tVal Loss: 0.0071\tVal MAE: 189773.15\telapsed: 0.01 mins\n",
      "Epoch 48: Train Loss: 0.0058\tVal Loss: 0.0066\tVal MAE: 184335.66\telapsed: 0.01 mins\n",
      "Epoch 49: Train Loss: 0.0054\tVal Loss: 0.0062\tVal MAE: 179628.24\telapsed: 0.01 mins\n",
      "Epoch 50: Train Loss: 0.0050\tVal Loss: 0.0058\tVal MAE: 175281.65\telapsed: 0.01 mins\n",
      "Epoch 51: Train Loss: 0.0047\tVal Loss: 0.0055\tVal MAE: 171358.78\telapsed: 0.01 mins\n",
      "Epoch 52: Train Loss: 0.0044\tVal Loss: 0.0052\tVal MAE: 168092.15\telapsed: 0.01 mins\n",
      "Epoch 53: Train Loss: 0.0041\tVal Loss: 0.0050\tVal MAE: 165338.13\telapsed: 0.01 mins\n",
      "Epoch 54: Train Loss: 0.0038\tVal Loss: 0.0048\tVal MAE: 162833.39\telapsed: 0.01 mins\n",
      "Epoch 55: Train Loss: 0.0036\tVal Loss: 0.0046\tVal MAE: 160398.91\telapsed: 0.02 mins\n",
      "Epoch 56: Train Loss: 0.0034\tVal Loss: 0.0045\tVal MAE: 158341.12\telapsed: 0.02 mins\n",
      "Epoch 57: Train Loss: 0.0033\tVal Loss: 0.0043\tVal MAE: 156560.96\telapsed: 0.02 mins\n",
      "Epoch 58: Train Loss: 0.0032\tVal Loss: 0.0042\tVal MAE: 154789.14\telapsed: 0.02 mins\n",
      "Epoch 59: Train Loss: 0.0030\tVal Loss: 0.0041\tVal MAE: 153192.22\telapsed: 0.02 mins\n",
      "Epoch 60: Train Loss: 0.0029\tVal Loss: 0.0040\tVal MAE: 151859.02\telapsed: 0.02 mins\n",
      "Epoch 61: Train Loss: 0.0029\tVal Loss: 0.0039\tVal MAE: 150744.18\telapsed: 0.02 mins\n",
      "Epoch 62: Train Loss: 0.0028\tVal Loss: 0.0038\tVal MAE: 149758.42\telapsed: 0.02 mins\n",
      "Epoch 63: Train Loss: 0.0027\tVal Loss: 0.0038\tVal MAE: 148948.00\telapsed: 0.02 mins\n",
      "Epoch 64: Train Loss: 0.0027\tVal Loss: 0.0037\tVal MAE: 148275.66\telapsed: 0.02 mins\n",
      "Epoch 65: Train Loss: 0.0026\tVal Loss: 0.0037\tVal MAE: 147676.19\telapsed: 0.02 mins\n",
      "Epoch 66: Train Loss: 0.0026\tVal Loss: 0.0036\tVal MAE: 147165.73\telapsed: 0.02 mins\n",
      "Epoch 67: Train Loss: 0.0026\tVal Loss: 0.0036\tVal MAE: 146884.32\telapsed: 0.02 mins\n",
      "Epoch 68: Train Loss: 0.0025\tVal Loss: 0.0036\tVal MAE: 146466.89\telapsed: 0.02 mins\n",
      "Epoch 69: Train Loss: 0.0025\tVal Loss: 0.0036\tVal MAE: 146133.99\telapsed: 0.02 mins\n",
      "Epoch 70: Train Loss: 0.0025\tVal Loss: 0.0036\tVal MAE: 145884.37\telapsed: 0.02 mins\n",
      "Epoch 71: Train Loss: 0.0025\tVal Loss: 0.0036\tVal MAE: 145616.74\telapsed: 0.02 mins\n",
      "Epoch 72: Train Loss: 0.0025\tVal Loss: 0.0035\tVal MAE: 145503.82\telapsed: 0.02 mins\n",
      "Epoch 73: Train Loss: 0.0024\tVal Loss: 0.0035\tVal MAE: 145330.46\telapsed: 0.02 mins\n",
      "Epoch 74: Train Loss: 0.0024\tVal Loss: 0.0035\tVal MAE: 145187.50\telapsed: 0.02 mins\n",
      "Epoch 75: Train Loss: 0.0024\tVal Loss: 0.0035\tVal MAE: 145108.85\telapsed: 0.02 mins\n",
      "Epoch 76: Train Loss: 0.0024\tVal Loss: 0.0035\tVal MAE: 145096.61\telapsed: 0.02 mins\n",
      "Epoch 77: Train Loss: 0.0024\tVal Loss: 0.0035\tVal MAE: 145117.54\telapsed: 0.02 mins\n",
      "Epoch 78: Train Loss: 0.0024\tVal Loss: 0.0035\tVal MAE: 145180.91\telapsed: 0.02 mins\n",
      "Epoch 79: Train Loss: 0.0024\tVal Loss: 0.0035\tVal MAE: 145143.09\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 76\tVal loss: 0.0035\tVal MAE: 145096.61\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 3 from 1.0/2016.0 to 3.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.6308\tVal Loss: 0.3807\tVal MAE: 4296192.92\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.1191\tVal Loss: 0.1067\tVal MAE: 1250821.99\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.1014\tVal Loss: 0.1001\tVal MAE: 1169144.82\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.0980\tVal Loss: 0.0965\tVal MAE: 1128343.31\telapsed: 0.00 mins\n",
      "Epoch 5: Train Loss: 0.0940\tVal Loss: 0.0924\tVal MAE: 1082568.45\telapsed: 0.00 mins\n",
      "Epoch 6: Train Loss: 0.0898\tVal Loss: 0.0881\tVal MAE: 1034792.07\telapsed: 0.00 mins\n",
      "Epoch 7: Train Loss: 0.0854\tVal Loss: 0.0838\tVal MAE: 986631.11\telapsed: 0.00 mins\n",
      "Epoch 8: Train Loss: 0.0811\tVal Loss: 0.0796\tVal MAE: 938802.00\telapsed: 0.00 mins\n",
      "Epoch 9: Train Loss: 0.0768\tVal Loss: 0.0753\tVal MAE: 891598.47\telapsed: 0.00 mins\n",
      "Epoch 10: Train Loss: 0.0726\tVal Loss: 0.0711\tVal MAE: 844730.55\telapsed: 0.00 mins\n",
      "Epoch 11: Train Loss: 0.0684\tVal Loss: 0.0670\tVal MAE: 798545.67\telapsed: 0.00 mins\n",
      "Epoch 12: Train Loss: 0.0643\tVal Loss: 0.0630\tVal MAE: 753409.02\telapsed: 0.00 mins\n",
      "Epoch 13: Train Loss: 0.0603\tVal Loss: 0.0591\tVal MAE: 709212.68\telapsed: 0.01 mins\n",
      "Epoch 14: Train Loss: 0.0564\tVal Loss: 0.0552\tVal MAE: 665685.06\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0525\tVal Loss: 0.0514\tVal MAE: 622863.73\telapsed: 0.01 mins\n",
      "Epoch 16: Train Loss: 0.0488\tVal Loss: 0.0478\tVal MAE: 581404.95\telapsed: 0.01 mins\n",
      "Epoch 17: Train Loss: 0.0451\tVal Loss: 0.0442\tVal MAE: 541377.19\telapsed: 0.01 mins\n",
      "Epoch 18: Train Loss: 0.0416\tVal Loss: 0.0408\tVal MAE: 503171.92\telapsed: 0.01 mins\n",
      "Epoch 19: Train Loss: 0.0382\tVal Loss: 0.0375\tVal MAE: 466599.42\telapsed: 0.01 mins\n",
      "Epoch 20: Train Loss: 0.0350\tVal Loss: 0.0344\tVal MAE: 431545.12\telapsed: 0.01 mins\n",
      "Epoch 21: Train Loss: 0.0320\tVal Loss: 0.0314\tVal MAE: 398094.57\telapsed: 0.01 mins\n",
      "Epoch 22: Train Loss: 0.0290\tVal Loss: 0.0286\tVal MAE: 366706.12\telapsed: 0.01 mins\n",
      "Epoch 23: Train Loss: 0.0263\tVal Loss: 0.0260\tVal MAE: 336985.50\telapsed: 0.01 mins\n",
      "Epoch 24: Train Loss: 0.0237\tVal Loss: 0.0235\tVal MAE: 309309.79\telapsed: 0.01 mins\n",
      "Epoch 25: Train Loss: 0.0213\tVal Loss: 0.0213\tVal MAE: 283701.12\telapsed: 0.01 mins\n",
      "Epoch 26: Train Loss: 0.0191\tVal Loss: 0.0191\tVal MAE: 259788.66\telapsed: 0.01 mins\n",
      "Epoch 27: Train Loss: 0.0171\tVal Loss: 0.0172\tVal MAE: 237498.35\telapsed: 0.01 mins\n",
      "Epoch 28: Train Loss: 0.0152\tVal Loss: 0.0155\tVal MAE: 217286.61\telapsed: 0.01 mins\n",
      "Epoch 29: Train Loss: 0.0135\tVal Loss: 0.0139\tVal MAE: 199328.30\telapsed: 0.01 mins\n",
      "Epoch 30: Train Loss: 0.0120\tVal Loss: 0.0125\tVal MAE: 183459.81\telapsed: 0.01 mins\n",
      "Epoch 31: Train Loss: 0.0106\tVal Loss: 0.0112\tVal MAE: 169373.36\telapsed: 0.01 mins\n",
      "Epoch 32: Train Loss: 0.0094\tVal Loss: 0.0102\tVal MAE: 156925.51\telapsed: 0.01 mins\n",
      "Epoch 33: Train Loss: 0.0084\tVal Loss: 0.0092\tVal MAE: 145962.49\telapsed: 0.01 mins\n",
      "Epoch 34: Train Loss: 0.0074\tVal Loss: 0.0084\tVal MAE: 135841.57\telapsed: 0.01 mins\n",
      "Epoch 35: Train Loss: 0.0066\tVal Loss: 0.0076\tVal MAE: 126857.06\telapsed: 0.01 mins\n",
      "Epoch 36: Train Loss: 0.0058\tVal Loss: 0.0069\tVal MAE: 119435.94\telapsed: 0.01 mins\n",
      "Epoch 37: Train Loss: 0.0052\tVal Loss: 0.0064\tVal MAE: 113237.11\telapsed: 0.01 mins\n",
      "Epoch 38: Train Loss: 0.0047\tVal Loss: 0.0059\tVal MAE: 107757.34\telapsed: 0.02 mins\n",
      "Epoch 39: Train Loss: 0.0043\tVal Loss: 0.0055\tVal MAE: 103072.76\telapsed: 0.02 mins\n",
      "Epoch 40: Train Loss: 0.0039\tVal Loss: 0.0052\tVal MAE: 99376.25\telapsed: 0.02 mins\n",
      "Epoch 41: Train Loss: 0.0035\tVal Loss: 0.0050\tVal MAE: 96252.70\telapsed: 0.02 mins\n",
      "Epoch 42: Train Loss: 0.0033\tVal Loss: 0.0047\tVal MAE: 93638.15\telapsed: 0.02 mins\n",
      "Epoch 43: Train Loss: 0.0031\tVal Loss: 0.0046\tVal MAE: 91550.05\telapsed: 0.02 mins\n",
      "Epoch 44: Train Loss: 0.0029\tVal Loss: 0.0044\tVal MAE: 89836.38\telapsed: 0.02 mins\n",
      "Epoch 45: Train Loss: 0.0027\tVal Loss: 0.0043\tVal MAE: 88590.58\telapsed: 0.02 mins\n",
      "Epoch 46: Train Loss: 0.0026\tVal Loss: 0.0042\tVal MAE: 87739.55\telapsed: 0.02 mins\n",
      "Epoch 47: Train Loss: 0.0025\tVal Loss: 0.0041\tVal MAE: 87105.13\telapsed: 0.02 mins\n",
      "Epoch 48: Train Loss: 0.0024\tVal Loss: 0.0041\tVal MAE: 86351.27\telapsed: 0.02 mins\n",
      "Epoch 49: Train Loss: 0.0023\tVal Loss: 0.0040\tVal MAE: 85515.88\telapsed: 0.02 mins\n",
      "Epoch 50: Train Loss: 0.0023\tVal Loss: 0.0040\tVal MAE: 85012.50\telapsed: 0.02 mins\n",
      "Epoch 51: Train Loss: 0.0022\tVal Loss: 0.0040\tVal MAE: 84690.00\telapsed: 0.02 mins\n",
      "Epoch 52: Train Loss: 0.0022\tVal Loss: 0.0039\tVal MAE: 84462.57\telapsed: 0.02 mins\n",
      "Epoch 53: Train Loss: 0.0021\tVal Loss: 0.0039\tVal MAE: 84334.59\telapsed: 0.02 mins\n",
      "Epoch 54: Train Loss: 0.0021\tVal Loss: 0.0039\tVal MAE: 84245.53\telapsed: 0.02 mins\n",
      "Epoch 55: Train Loss: 0.0021\tVal Loss: 0.0039\tVal MAE: 84105.02\telapsed: 0.02 mins\n",
      "Epoch 56: Train Loss: 0.0021\tVal Loss: 0.0039\tVal MAE: 83823.51\telapsed: 0.02 mins\n",
      "Epoch 57: Train Loss: 0.0021\tVal Loss: 0.0039\tVal MAE: 83548.95\telapsed: 0.02 mins\n",
      "Epoch 58: Train Loss: 0.0021\tVal Loss: 0.0039\tVal MAE: 83508.75\telapsed: 0.02 mins\n",
      "Epoch 59: Train Loss: 0.0021\tVal Loss: 0.0039\tVal MAE: 83293.73\telapsed: 0.02 mins\n",
      "Epoch 60: Train Loss: 0.0020\tVal Loss: 0.0039\tVal MAE: 83245.14\telapsed: 0.02 mins\n",
      "Epoch 61: Train Loss: 0.0020\tVal Loss: 0.0039\tVal MAE: 83187.19\telapsed: 0.02 mins\n",
      "Epoch 62: Train Loss: 0.0020\tVal Loss: 0.0039\tVal MAE: 83269.09\telapsed: 0.02 mins\n",
      "Epoch 63: Train Loss: 0.0020\tVal Loss: 0.0039\tVal MAE: 83260.62\telapsed: 0.03 mins\n",
      "Epoch 64: Train Loss: 0.0020\tVal Loss: 0.0039\tVal MAE: 83219.50\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 61\tVal loss: 0.0039\tVal MAE: 83187.19\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 4 from 1.0/2016.0 to 4.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.1084\tVal Loss: 0.1074\tVal MAE: 1292730.49\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.1013\tVal Loss: 0.1000\tVal MAE: 1209995.85\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0936\tVal Loss: 0.0924\tVal MAE: 1124387.33\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.0861\tVal Loss: 0.0849\tVal MAE: 1040412.59\telapsed: 0.00 mins\n",
      "Epoch 5: Train Loss: 0.0787\tVal Loss: 0.0776\tVal MAE: 958872.57\telapsed: 0.00 mins\n",
      "Epoch 6: Train Loss: 0.0716\tVal Loss: 0.0705\tVal MAE: 880016.78\telapsed: 0.00 mins\n",
      "Epoch 7: Train Loss: 0.0647\tVal Loss: 0.0637\tVal MAE: 803661.18\telapsed: 0.00 mins\n",
      "Epoch 8: Train Loss: 0.0581\tVal Loss: 0.0572\tVal MAE: 730012.45\telapsed: 0.00 mins\n",
      "Epoch 9: Train Loss: 0.0518\tVal Loss: 0.0510\tVal MAE: 659968.00\telapsed: 0.00 mins\n",
      "Epoch 10: Train Loss: 0.0458\tVal Loss: 0.0452\tVal MAE: 593878.44\telapsed: 0.01 mins\n",
      "Epoch 11: Train Loss: 0.0402\tVal Loss: 0.0397\tVal MAE: 530936.39\telapsed: 0.01 mins\n",
      "Epoch 12: Train Loss: 0.0350\tVal Loss: 0.0346\tVal MAE: 473451.47\telapsed: 0.01 mins\n",
      "Epoch 13: Train Loss: 0.0303\tVal Loss: 0.0300\tVal MAE: 421377.73\telapsed: 0.01 mins\n",
      "Epoch 14: Train Loss: 0.0260\tVal Loss: 0.0259\tVal MAE: 375413.25\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0221\tVal Loss: 0.0221\tVal MAE: 333175.08\telapsed: 0.01 mins\n",
      "Epoch 16: Train Loss: 0.0187\tVal Loss: 0.0188\tVal MAE: 294873.10\telapsed: 0.01 mins\n",
      "Epoch 17: Train Loss: 0.0157\tVal Loss: 0.0160\tVal MAE: 261554.12\telapsed: 0.01 mins\n",
      "Epoch 18: Train Loss: 0.0132\tVal Loss: 0.0135\tVal MAE: 232478.61\telapsed: 0.01 mins\n",
      "Epoch 19: Train Loss: 0.0110\tVal Loss: 0.0115\tVal MAE: 208407.22\telapsed: 0.01 mins\n",
      "Epoch 20: Train Loss: 0.0093\tVal Loss: 0.0098\tVal MAE: 189146.61\telapsed: 0.01 mins\n",
      "Epoch 21: Train Loss: 0.0079\tVal Loss: 0.0084\tVal MAE: 173087.43\telapsed: 0.01 mins\n",
      "Epoch 22: Train Loss: 0.0067\tVal Loss: 0.0074\tVal MAE: 160631.53\telapsed: 0.01 mins\n",
      "Epoch 23: Train Loss: 0.0058\tVal Loss: 0.0065\tVal MAE: 149807.79\telapsed: 0.01 mins\n",
      "Epoch 24: Train Loss: 0.0051\tVal Loss: 0.0059\tVal MAE: 141512.98\telapsed: 0.01 mins\n",
      "Epoch 25: Train Loss: 0.0046\tVal Loss: 0.0054\tVal MAE: 135125.98\telapsed: 0.01 mins\n",
      "Epoch 26: Train Loss: 0.0042\tVal Loss: 0.0050\tVal MAE: 129868.47\telapsed: 0.01 mins\n",
      "Epoch 27: Train Loss: 0.0038\tVal Loss: 0.0047\tVal MAE: 125692.52\telapsed: 0.01 mins\n",
      "Epoch 28: Train Loss: 0.0036\tVal Loss: 0.0045\tVal MAE: 122667.39\telapsed: 0.01 mins\n",
      "Epoch 29: Train Loss: 0.0034\tVal Loss: 0.0043\tVal MAE: 120543.47\telapsed: 0.02 mins\n",
      "Epoch 30: Train Loss: 0.0033\tVal Loss: 0.0042\tVal MAE: 118973.44\telapsed: 0.02 mins\n",
      "Epoch 31: Train Loss: 0.0032\tVal Loss: 0.0041\tVal MAE: 117641.12\telapsed: 0.02 mins\n",
      "Epoch 32: Train Loss: 0.0032\tVal Loss: 0.0040\tVal MAE: 116530.85\telapsed: 0.02 mins\n",
      "Epoch 33: Train Loss: 0.0031\tVal Loss: 0.0040\tVal MAE: 115907.67\telapsed: 0.02 mins\n",
      "Epoch 34: Train Loss: 0.0031\tVal Loss: 0.0040\tVal MAE: 115381.28\telapsed: 0.02 mins\n",
      "Epoch 35: Train Loss: 0.0031\tVal Loss: 0.0039\tVal MAE: 114918.81\telapsed: 0.02 mins\n",
      "Epoch 36: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 114556.74\telapsed: 0.02 mins\n",
      "Epoch 37: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 114395.61\telapsed: 0.02 mins\n",
      "Epoch 38: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 114163.11\telapsed: 0.02 mins\n",
      "Epoch 39: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 113662.66\telapsed: 0.02 mins\n",
      "Epoch 40: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 113446.97\telapsed: 0.02 mins\n",
      "Epoch 41: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 113391.18\telapsed: 0.02 mins\n",
      "Epoch 42: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 113331.71\telapsed: 0.02 mins\n",
      "Epoch 43: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 113263.07\telapsed: 0.02 mins\n",
      "Epoch 44: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 112913.70\telapsed: 0.02 mins\n",
      "Epoch 45: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 112970.44\telapsed: 0.02 mins\n",
      "Epoch 46: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 112925.36\telapsed: 0.02 mins\n",
      "Epoch 47: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 112699.83\telapsed: 0.02 mins\n",
      "Epoch 48: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 112699.13\telapsed: 0.03 mins\n",
      "Epoch 49: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 112738.34\telapsed: 0.03 mins\n",
      "Epoch 50: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 112727.67\telapsed: 0.03 mins\n",
      "Epoch 51: Train Loss: 0.0030\tVal Loss: 0.0039\tVal MAE: 112834.18\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 48\tVal loss: 0.0039\tVal MAE: 112699.13\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 5 from 1.0/2016.0 to 5.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.1144\tVal Loss: 0.1123\tVal MAE: 1336546.77\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.1029\tVal Loss: 0.1004\tVal MAE: 1204289.08\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0909\tVal Loss: 0.0886\tVal MAE: 1071646.40\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.0797\tVal Loss: 0.0776\tVal MAE: 948213.55\telapsed: 0.00 mins\n",
      "Epoch 5: Train Loss: 0.0693\tVal Loss: 0.0674\tVal MAE: 834642.64\telapsed: 0.00 mins\n",
      "Epoch 6: Train Loss: 0.0598\tVal Loss: 0.0582\tVal MAE: 731139.54\telapsed: 0.00 mins\n",
      "Epoch 7: Train Loss: 0.0512\tVal Loss: 0.0497\tVal MAE: 636034.84\telapsed: 0.00 mins\n",
      "Epoch 8: Train Loss: 0.0433\tVal Loss: 0.0421\tVal MAE: 549558.38\telapsed: 0.01 mins\n",
      "Epoch 9: Train Loss: 0.0363\tVal Loss: 0.0353\tVal MAE: 472625.92\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0301\tVal Loss: 0.0293\tVal MAE: 404633.75\telapsed: 0.01 mins\n",
      "Epoch 11: Train Loss: 0.0248\tVal Loss: 0.0242\tVal MAE: 346152.21\telapsed: 0.01 mins\n",
      "Epoch 12: Train Loss: 0.0202\tVal Loss: 0.0198\tVal MAE: 295989.54\telapsed: 0.01 mins\n",
      "Epoch 13: Train Loss: 0.0164\tVal Loss: 0.0162\tVal MAE: 254945.91\telapsed: 0.01 mins\n",
      "Epoch 14: Train Loss: 0.0133\tVal Loss: 0.0132\tVal MAE: 220812.79\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0108\tVal Loss: 0.0109\tVal MAE: 194015.28\telapsed: 0.01 mins\n",
      "Epoch 16: Train Loss: 0.0088\tVal Loss: 0.0091\tVal MAE: 172938.36\telapsed: 0.01 mins\n",
      "Epoch 17: Train Loss: 0.0073\tVal Loss: 0.0077\tVal MAE: 156786.52\telapsed: 0.01 mins\n",
      "Epoch 18: Train Loss: 0.0062\tVal Loss: 0.0066\tVal MAE: 144004.72\telapsed: 0.01 mins\n",
      "Epoch 19: Train Loss: 0.0053\tVal Loss: 0.0059\tVal MAE: 134508.30\telapsed: 0.01 mins\n",
      "Epoch 20: Train Loss: 0.0047\tVal Loss: 0.0053\tVal MAE: 128097.15\telapsed: 0.01 mins\n",
      "Epoch 21: Train Loss: 0.0043\tVal Loss: 0.0049\tVal MAE: 123388.62\telapsed: 0.01 mins\n",
      "Epoch 22: Train Loss: 0.0040\tVal Loss: 0.0046\tVal MAE: 119473.82\telapsed: 0.01 mins\n",
      "Epoch 23: Train Loss: 0.0038\tVal Loss: 0.0044\tVal MAE: 116719.12\telapsed: 0.01 mins\n",
      "Epoch 24: Train Loss: 0.0036\tVal Loss: 0.0043\tVal MAE: 114715.89\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0035\tVal Loss: 0.0042\tVal MAE: 113288.49\telapsed: 0.02 mins\n",
      "Epoch 26: Train Loss: 0.0034\tVal Loss: 0.0041\tVal MAE: 112135.33\telapsed: 0.02 mins\n",
      "Epoch 27: Train Loss: 0.0034\tVal Loss: 0.0041\tVal MAE: 111272.64\telapsed: 0.02 mins\n",
      "Epoch 28: Train Loss: 0.0034\tVal Loss: 0.0041\tVal MAE: 110785.33\telapsed: 0.02 mins\n",
      "Epoch 29: Train Loss: 0.0033\tVal Loss: 0.0040\tVal MAE: 110452.69\telapsed: 0.02 mins\n",
      "Epoch 30: Train Loss: 0.0033\tVal Loss: 0.0040\tVal MAE: 110240.04\telapsed: 0.02 mins\n",
      "Epoch 31: Train Loss: 0.0033\tVal Loss: 0.0040\tVal MAE: 110070.85\telapsed: 0.02 mins\n",
      "Epoch 32: Train Loss: 0.0033\tVal Loss: 0.0040\tVal MAE: 110022.28\telapsed: 0.02 mins\n",
      "Epoch 33: Train Loss: 0.0033\tVal Loss: 0.0040\tVal MAE: 109998.24\telapsed: 0.02 mins\n",
      "Epoch 34: Train Loss: 0.0033\tVal Loss: 0.0040\tVal MAE: 110220.63\telapsed: 0.02 mins\n",
      "Epoch 35: Train Loss: 0.0033\tVal Loss: 0.0040\tVal MAE: 110180.87\telapsed: 0.02 mins\n",
      "Epoch 36: Train Loss: 0.0033\tVal Loss: 0.0040\tVal MAE: 110310.25\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 33\tVal loss: 0.0040\tVal MAE: 109998.24\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 6 from 1.0/2016.0 to 6.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.1094\tVal Loss: 0.1083\tVal MAE: 1318667.50\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0994\tVal Loss: 0.0981\tVal MAE: 1208084.80\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0890\tVal Loss: 0.0878\tVal MAE: 1095452.44\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.0789\tVal Loss: 0.0778\tVal MAE: 987201.29\telapsed: 0.00 mins\n",
      "Epoch 5: Train Loss: 0.0693\tVal Loss: 0.0683\tVal MAE: 884029.61\telapsed: 0.00 mins\n",
      "Epoch 6: Train Loss: 0.0601\tVal Loss: 0.0592\tVal MAE: 787211.15\telapsed: 0.00 mins\n",
      "Epoch 7: Train Loss: 0.0515\tVal Loss: 0.0507\tVal MAE: 695727.33\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0435\tVal Loss: 0.0429\tVal MAE: 610950.28\telapsed: 0.01 mins\n",
      "Epoch 9: Train Loss: 0.0362\tVal Loss: 0.0357\tVal MAE: 533662.11\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0298\tVal Loss: 0.0295\tVal MAE: 466568.79\telapsed: 0.01 mins\n",
      "Epoch 11: Train Loss: 0.0242\tVal Loss: 0.0240\tVal MAE: 408916.73\telapsed: 0.01 mins\n",
      "Epoch 12: Train Loss: 0.0194\tVal Loss: 0.0194\tVal MAE: 359135.33\telapsed: 0.01 mins\n",
      "Epoch 13: Train Loss: 0.0156\tVal Loss: 0.0157\tVal MAE: 318625.15\telapsed: 0.01 mins\n",
      "Epoch 14: Train Loss: 0.0124\tVal Loss: 0.0126\tVal MAE: 284349.31\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0099\tVal Loss: 0.0103\tVal MAE: 257566.90\telapsed: 0.01 mins\n",
      "Epoch 16: Train Loss: 0.0080\tVal Loss: 0.0084\tVal MAE: 236849.65\telapsed: 0.01 mins\n",
      "Epoch 17: Train Loss: 0.0066\tVal Loss: 0.0071\tVal MAE: 220954.17\telapsed: 0.01 mins\n",
      "Epoch 18: Train Loss: 0.0056\tVal Loss: 0.0061\tVal MAE: 209664.78\telapsed: 0.01 mins\n",
      "Epoch 19: Train Loss: 0.0048\tVal Loss: 0.0054\tVal MAE: 200839.92\telapsed: 0.01 mins\n",
      "Epoch 20: Train Loss: 0.0043\tVal Loss: 0.0049\tVal MAE: 195008.62\telapsed: 0.02 mins\n",
      "Epoch 21: Train Loss: 0.0040\tVal Loss: 0.0046\tVal MAE: 191063.58\telapsed: 0.02 mins\n",
      "Epoch 22: Train Loss: 0.0038\tVal Loss: 0.0044\tVal MAE: 187705.68\telapsed: 0.02 mins\n",
      "Epoch 23: Train Loss: 0.0036\tVal Loss: 0.0043\tVal MAE: 185929.22\telapsed: 0.02 mins\n",
      "Epoch 24: Train Loss: 0.0036\tVal Loss: 0.0042\tVal MAE: 184385.76\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0035\tVal Loss: 0.0041\tVal MAE: 182779.11\telapsed: 0.02 mins\n",
      "Epoch 26: Train Loss: 0.0035\tVal Loss: 0.0041\tVal MAE: 182138.74\telapsed: 0.02 mins\n",
      "Epoch 27: Train Loss: 0.0034\tVal Loss: 0.0041\tVal MAE: 181644.13\telapsed: 0.02 mins\n",
      "Epoch 28: Train Loss: 0.0034\tVal Loss: 0.0041\tVal MAE: 181130.89\telapsed: 0.02 mins\n",
      "Epoch 29: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 180960.26\telapsed: 0.02 mins\n",
      "Epoch 30: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 180855.39\telapsed: 0.02 mins\n",
      "Epoch 31: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 180274.91\telapsed: 0.02 mins\n",
      "Epoch 32: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 179739.09\telapsed: 0.02 mins\n",
      "Epoch 33: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 179795.70\telapsed: 0.03 mins\n",
      "Epoch 34: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 179655.62\telapsed: 0.03 mins\n",
      "Epoch 35: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 179533.13\telapsed: 0.03 mins\n",
      "Epoch 36: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 179356.93\telapsed: 0.03 mins\n",
      "Epoch 37: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 179238.31\telapsed: 0.03 mins\n",
      "Epoch 38: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 179337.91\telapsed: 0.03 mins\n",
      "Epoch 39: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 179528.19\telapsed: 0.03 mins\n",
      "Epoch 40: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 179343.68\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 37\tVal loss: 0.0040\tVal MAE: 179238.31\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 7 from 1.0/2016.0 to 7.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.1068\tVal Loss: 0.1048\tVal MAE: 1321408.23\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0897\tVal Loss: 0.0878\tVal MAE: 1139038.78\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0739\tVal Loss: 0.0722\tVal MAE: 968818.13\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.0596\tVal Loss: 0.0583\tVal MAE: 817568.21\telapsed: 0.00 mins\n",
      "Epoch 5: Train Loss: 0.0472\tVal Loss: 0.0461\tVal MAE: 685946.92\telapsed: 0.00 mins\n",
      "Epoch 6: Train Loss: 0.0364\tVal Loss: 0.0357\tVal MAE: 572039.85\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0275\tVal Loss: 0.0271\tVal MAE: 476121.06\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0205\tVal Loss: 0.0204\tVal MAE: 399766.11\telapsed: 0.01 mins\n",
      "Epoch 9: Train Loss: 0.0152\tVal Loss: 0.0153\tVal MAE: 341575.71\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0114\tVal Loss: 0.0117\tVal MAE: 299800.65\telapsed: 0.01 mins\n",
      "Epoch 11: Train Loss: 0.0087\tVal Loss: 0.0092\tVal MAE: 269035.17\telapsed: 0.01 mins\n",
      "Epoch 12: Train Loss: 0.0070\tVal Loss: 0.0075\tVal MAE: 249209.67\telapsed: 0.01 mins\n",
      "Epoch 13: Train Loss: 0.0058\tVal Loss: 0.0064\tVal MAE: 236077.62\telapsed: 0.01 mins\n",
      "Epoch 14: Train Loss: 0.0051\tVal Loss: 0.0057\tVal MAE: 228693.53\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0046\tVal Loss: 0.0053\tVal MAE: 223067.52\telapsed: 0.01 mins\n",
      "Epoch 16: Train Loss: 0.0043\tVal Loss: 0.0051\tVal MAE: 219140.89\telapsed: 0.01 mins\n",
      "Epoch 17: Train Loss: 0.0042\tVal Loss: 0.0049\tVal MAE: 216818.47\telapsed: 0.02 mins\n",
      "Epoch 18: Train Loss: 0.0041\tVal Loss: 0.0048\tVal MAE: 215361.91\telapsed: 0.02 mins\n",
      "Epoch 19: Train Loss: 0.0040\tVal Loss: 0.0047\tVal MAE: 214109.16\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0039\tVal Loss: 0.0047\tVal MAE: 213479.56\telapsed: 0.02 mins\n",
      "Epoch 21: Train Loss: 0.0039\tVal Loss: 0.0046\tVal MAE: 212930.53\telapsed: 0.02 mins\n",
      "Epoch 22: Train Loss: 0.0039\tVal Loss: 0.0046\tVal MAE: 212710.56\telapsed: 0.02 mins\n",
      "Epoch 23: Train Loss: 0.0038\tVal Loss: 0.0046\tVal MAE: 212472.20\telapsed: 0.02 mins\n",
      "Epoch 24: Train Loss: 0.0038\tVal Loss: 0.0045\tVal MAE: 211728.02\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0038\tVal Loss: 0.0045\tVal MAE: 211149.10\telapsed: 0.02 mins\n",
      "Epoch 26: Train Loss: 0.0038\tVal Loss: 0.0045\tVal MAE: 211073.26\telapsed: 0.02 mins\n",
      "Epoch 27: Train Loss: 0.0037\tVal Loss: 0.0045\tVal MAE: 210426.41\telapsed: 0.03 mins\n",
      "Epoch 28: Train Loss: 0.0037\tVal Loss: 0.0045\tVal MAE: 210373.35\telapsed: 0.03 mins\n",
      "Epoch 29: Train Loss: 0.0037\tVal Loss: 0.0045\tVal MAE: 210427.07\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0037\tVal Loss: 0.0045\tVal MAE: 210289.84\telapsed: 0.03 mins\n",
      "Epoch 31: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 210105.99\telapsed: 0.03 mins\n",
      "Epoch 32: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209801.14\telapsed: 0.03 mins\n",
      "Epoch 33: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209622.60\telapsed: 0.03 mins\n",
      "Epoch 34: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209739.34\telapsed: 0.03 mins\n",
      "Epoch 35: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209830.19\telapsed: 0.03 mins\n",
      "Epoch 36: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209220.52\telapsed: 0.04 mins\n",
      "Epoch 37: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209629.02\telapsed: 0.04 mins\n",
      "Epoch 38: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209529.93\telapsed: 0.04 mins\n",
      "Epoch 39: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209129.63\telapsed: 0.04 mins\n",
      "Epoch 40: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209403.19\telapsed: 0.04 mins\n",
      "Epoch 41: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209431.32\telapsed: 0.04 mins\n",
      "Epoch 42: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 208911.70\telapsed: 0.04 mins\n",
      "Epoch 43: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209235.68\telapsed: 0.04 mins\n",
      "Epoch 44: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 208882.41\telapsed: 0.04 mins\n",
      "Epoch 45: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209448.06\telapsed: 0.04 mins\n",
      "Epoch 46: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209335.26\telapsed: 0.04 mins\n",
      "Epoch 47: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 209184.60\telapsed: 0.05 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 44\tVal loss: 0.0044\tVal MAE: 208882.41\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 8 from 1.0/2016.0 to 8.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.1048\tVal Loss: 0.1024\tVal MAE: 1207884.34\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0881\tVal Loss: 0.0858\tVal MAE: 1023911.80\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0726\tVal Loss: 0.0706\tVal MAE: 855138.04\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.0587\tVal Loss: 0.0569\tVal MAE: 703460.35\telapsed: 0.00 mins\n",
      "Epoch 5: Train Loss: 0.0464\tVal Loss: 0.0449\tVal MAE: 569311.95\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0357\tVal Loss: 0.0344\tVal MAE: 452923.26\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0268\tVal Loss: 0.0257\tVal MAE: 354875.66\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0197\tVal Loss: 0.0189\tVal MAE: 278562.29\telapsed: 0.01 mins\n",
      "Epoch 9: Train Loss: 0.0145\tVal Loss: 0.0139\tVal MAE: 224154.75\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0108\tVal Loss: 0.0104\tVal MAE: 185786.54\telapsed: 0.01 mins\n",
      "Epoch 11: Train Loss: 0.0081\tVal Loss: 0.0080\tVal MAE: 159335.77\telapsed: 0.01 mins\n",
      "Epoch 12: Train Loss: 0.0065\tVal Loss: 0.0065\tVal MAE: 143024.82\telapsed: 0.01 mins\n",
      "Epoch 13: Train Loss: 0.0055\tVal Loss: 0.0056\tVal MAE: 133592.92\telapsed: 0.01 mins\n",
      "Epoch 14: Train Loss: 0.0049\tVal Loss: 0.0051\tVal MAE: 128308.49\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0046\tVal Loss: 0.0048\tVal MAE: 125317.53\telapsed: 0.02 mins\n",
      "Epoch 16: Train Loss: 0.0044\tVal Loss: 0.0047\tVal MAE: 124153.73\telapsed: 0.02 mins\n",
      "Epoch 17: Train Loss: 0.0043\tVal Loss: 0.0046\tVal MAE: 123256.02\telapsed: 0.02 mins\n",
      "Epoch 18: Train Loss: 0.0043\tVal Loss: 0.0046\tVal MAE: 122607.59\telapsed: 0.02 mins\n",
      "Epoch 19: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122165.53\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122121.26\telapsed: 0.02 mins\n",
      "Epoch 21: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122106.70\telapsed: 0.02 mins\n",
      "Epoch 22: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122082.24\telapsed: 0.02 mins\n",
      "Epoch 23: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122188.37\telapsed: 0.02 mins\n",
      "Epoch 24: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122126.54\telapsed: 0.03 mins\n",
      "Epoch 25: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122053.79\telapsed: 0.03 mins\n",
      "Epoch 26: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122041.87\telapsed: 0.03 mins\n",
      "Epoch 27: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122071.76\telapsed: 0.03 mins\n",
      "Epoch 28: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122027.42\telapsed: 0.03 mins\n",
      "Epoch 29: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122153.98\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122254.45\telapsed: 0.03 mins\n",
      "Epoch 31: Train Loss: 0.0042\tVal Loss: 0.0045\tVal MAE: 122249.58\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 28\tVal loss: 0.0045\tVal MAE: 122027.42\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 9 from 1.0/2016.0 to 9.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.1047\tVal Loss: 0.1019\tVal MAE: 1205336.74\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0810\tVal Loss: 0.0784\tVal MAE: 948537.27\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0604\tVal Loss: 0.0583\tVal MAE: 727847.91\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.0435\tVal Loss: 0.0419\tVal MAE: 550075.91\telapsed: 0.00 mins\n",
      "Epoch 5: Train Loss: 0.0302\tVal Loss: 0.0291\tVal MAE: 411232.42\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0204\tVal Loss: 0.0196\tVal MAE: 308155.59\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0137\tVal Loss: 0.0133\tVal MAE: 242261.70\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0094\tVal Loss: 0.0094\tVal MAE: 200298.55\telapsed: 0.01 mins\n",
      "Epoch 9: Train Loss: 0.0070\tVal Loss: 0.0071\tVal MAE: 175831.69\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0056\tVal Loss: 0.0058\tVal MAE: 161672.93\telapsed: 0.01 mins\n",
      "Epoch 11: Train Loss: 0.0049\tVal Loss: 0.0052\tVal MAE: 154445.81\telapsed: 0.01 mins\n",
      "Epoch 12: Train Loss: 0.0046\tVal Loss: 0.0049\tVal MAE: 151447.60\telapsed: 0.01 mins\n",
      "Epoch 13: Train Loss: 0.0044\tVal Loss: 0.0048\tVal MAE: 150391.09\telapsed: 0.02 mins\n",
      "Epoch 14: Train Loss: 0.0043\tVal Loss: 0.0047\tVal MAE: 149727.82\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0043\tVal Loss: 0.0047\tVal MAE: 149950.28\telapsed: 0.02 mins\n",
      "Epoch 16: Train Loss: 0.0043\tVal Loss: 0.0047\tVal MAE: 149844.23\telapsed: 0.02 mins\n",
      "Epoch 17: Train Loss: 0.0043\tVal Loss: 0.0047\tVal MAE: 149995.43\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 14\tVal loss: 0.0047\tVal MAE: 149727.82\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 10 from 1.0/2016.0 to 10.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0979\tVal Loss: 0.0956\tVal MAE: 1145216.18\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0806\tVal Loss: 0.0783\tVal MAE: 956786.51\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0646\tVal Loss: 0.0625\tVal MAE: 783877.31\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.0501\tVal Loss: 0.0481\tVal MAE: 625435.90\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0373\tVal Loss: 0.0356\tVal MAE: 487513.27\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0267\tVal Loss: 0.0253\tVal MAE: 376998.19\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0186\tVal Loss: 0.0174\tVal MAE: 292997.18\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0128\tVal Loss: 0.0119\tVal MAE: 234546.08\telapsed: 0.01 mins\n",
      "Epoch 9: Train Loss: 0.0090\tVal Loss: 0.0083\tVal MAE: 197407.54\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0068\tVal Loss: 0.0063\tVal MAE: 176861.26\telapsed: 0.01 mins\n",
      "Epoch 11: Train Loss: 0.0055\tVal Loss: 0.0051\tVal MAE: 164687.06\telapsed: 0.01 mins\n",
      "Epoch 12: Train Loss: 0.0049\tVal Loss: 0.0045\tVal MAE: 158419.11\telapsed: 0.02 mins\n",
      "Epoch 13: Train Loss: 0.0045\tVal Loss: 0.0042\tVal MAE: 155709.51\telapsed: 0.02 mins\n",
      "Epoch 14: Train Loss: 0.0044\tVal Loss: 0.0041\tVal MAE: 154653.36\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0043\tVal Loss: 0.0041\tVal MAE: 154547.69\telapsed: 0.02 mins\n",
      "Epoch 16: Train Loss: 0.0043\tVal Loss: 0.0040\tVal MAE: 155186.25\telapsed: 0.02 mins\n",
      "Epoch 17: Train Loss: 0.0042\tVal Loss: 0.0040\tVal MAE: 155133.41\telapsed: 0.02 mins\n",
      "Epoch 18: Train Loss: 0.0042\tVal Loss: 0.0040\tVal MAE: 155257.79\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 15\tVal loss: 0.0041\tVal MAE: 154547.69\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 11 from 1.0/2016.0 to 11.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0971\tVal Loss: 0.0953\tVal MAE: 1150442.85\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0737\tVal Loss: 0.0716\tVal MAE: 897138.58\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0535\tVal Loss: 0.0518\tVal MAE: 686116.86\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.0370\tVal Loss: 0.0357\tVal MAE: 515007.88\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0245\tVal Loss: 0.0237\tVal MAE: 387579.87\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0158\tVal Loss: 0.0153\tVal MAE: 298625.95\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0102\tVal Loss: 0.0100\tVal MAE: 243939.99\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0070\tVal Loss: 0.0070\tVal MAE: 212418.64\telapsed: 0.01 mins\n",
      "Epoch 9: Train Loss: 0.0053\tVal Loss: 0.0055\tVal MAE: 195608.99\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0044\tVal Loss: 0.0047\tVal MAE: 188650.43\telapsed: 0.01 mins\n",
      "Epoch 11: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 186465.90\telapsed: 0.02 mins\n",
      "Epoch 12: Train Loss: 0.0039\tVal Loss: 0.0043\tVal MAE: 185604.45\telapsed: 0.02 mins\n",
      "Epoch 13: Train Loss: 0.0039\tVal Loss: 0.0043\tVal MAE: 185934.84\telapsed: 0.02 mins\n",
      "Epoch 14: Train Loss: 0.0038\tVal Loss: 0.0043\tVal MAE: 185971.55\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0039\tVal Loss: 0.0043\tVal MAE: 186305.22\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0043\tVal MAE: 185604.45\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 12 from 1.0/2016.0 to 12.0/2016.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0890\tVal Loss: 0.0872\tVal MAE: 1092336.21\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0678\tVal Loss: 0.0663\tVal MAE: 870475.34\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0496\tVal Loss: 0.0485\tVal MAE: 676560.50\telapsed: 0.00 mins\n",
      "Epoch 4: Train Loss: 0.0343\tVal Loss: 0.0334\tVal MAE: 514323.54\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0224\tVal Loss: 0.0219\tVal MAE: 387956.94\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0141\tVal Loss: 0.0140\tVal MAE: 303908.01\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0090\tVal Loss: 0.0092\tVal MAE: 255041.41\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0063\tVal Loss: 0.0066\tVal MAE: 227690.37\telapsed: 0.01 mins\n",
      "Epoch 9: Train Loss: 0.0049\tVal Loss: 0.0054\tVal MAE: 214201.59\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0043\tVal Loss: 0.0049\tVal MAE: 207232.01\telapsed: 0.02 mins\n",
      "Epoch 11: Train Loss: 0.0041\tVal Loss: 0.0047\tVal MAE: 204223.06\telapsed: 0.02 mins\n",
      "Epoch 12: Train Loss: 0.0040\tVal Loss: 0.0047\tVal MAE: 203558.24\telapsed: 0.02 mins\n",
      "Epoch 13: Train Loss: 0.0040\tVal Loss: 0.0046\tVal MAE: 203021.49\telapsed: 0.02 mins\n",
      "Epoch 14: Train Loss: 0.0039\tVal Loss: 0.0046\tVal MAE: 202365.20\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0039\tVal Loss: 0.0046\tVal MAE: 202378.99\telapsed: 0.02 mins\n",
      "Epoch 16: Train Loss: 0.0039\tVal Loss: 0.0046\tVal MAE: 202514.31\telapsed: 0.02 mins\n",
      "Epoch 17: Train Loss: 0.0040\tVal Loss: 0.0046\tVal MAE: 202664.31\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 14\tVal loss: 0.0046\tVal MAE: 202365.20\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 13 from 1.0/2016.0 to 1.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0862\tVal Loss: 0.0842\tVal MAE: 997954.75\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0615\tVal Loss: 0.0598\tVal MAE: 726791.33\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0412\tVal Loss: 0.0396\tVal MAE: 501582.15\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0255\tVal Loss: 0.0243\tVal MAE: 329519.86\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0149\tVal Loss: 0.0140\tVal MAE: 217341.72\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0090\tVal Loss: 0.0082\tVal MAE: 154407.62\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0063\tVal Loss: 0.0055\tVal MAE: 123935.60\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0052\tVal Loss: 0.0043\tVal MAE: 111489.82\telapsed: 0.01 mins\n",
      "Epoch 9: Train Loss: 0.0049\tVal Loss: 0.0039\tVal MAE: 106652.01\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0048\tVal Loss: 0.0037\tVal MAE: 103628.86\telapsed: 0.02 mins\n",
      "Epoch 11: Train Loss: 0.0048\tVal Loss: 0.0037\tVal MAE: 102286.62\telapsed: 0.02 mins\n",
      "Epoch 12: Train Loss: 0.0047\tVal Loss: 0.0036\tVal MAE: 101161.68\telapsed: 0.02 mins\n",
      "Epoch 13: Train Loss: 0.0047\tVal Loss: 0.0036\tVal MAE: 100723.55\telapsed: 0.02 mins\n",
      "Epoch 14: Train Loss: 0.0047\tVal Loss: 0.0036\tVal MAE: 100215.26\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0047\tVal Loss: 0.0036\tVal MAE: 100248.05\telapsed: 0.02 mins\n",
      "Epoch 16: Train Loss: 0.0047\tVal Loss: 0.0036\tVal MAE: 99864.80\telapsed: 0.03 mins\n",
      "Epoch 17: Train Loss: 0.0047\tVal Loss: 0.0036\tVal MAE: 99980.37\telapsed: 0.03 mins\n",
      "Epoch 18: Train Loss: 0.0047\tVal Loss: 0.0036\tVal MAE: 99897.78\telapsed: 0.03 mins\n",
      "Epoch 19: Train Loss: 0.0047\tVal Loss: 0.0036\tVal MAE: 100003.87\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 16\tVal loss: 0.0036\tVal MAE: 99864.80\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 14 from 1.0/2016.0 to 2.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0850\tVal Loss: 0.0828\tVal MAE: 1007642.63\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0585\tVal Loss: 0.0567\tVal MAE: 726936.19\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0371\tVal Loss: 0.0357\tVal MAE: 496368.37\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0214\tVal Loss: 0.0206\tVal MAE: 329599.13\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0116\tVal Loss: 0.0113\tVal MAE: 230382.26\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0067\tVal Loss: 0.0068\tVal MAE: 181955.29\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0046\tVal Loss: 0.0049\tVal MAE: 161642.70\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0039\tVal Loss: 0.0043\tVal MAE: 153288.12\telapsed: 0.01 mins\n",
      "Epoch 9: Train Loss: 0.0036\tVal Loss: 0.0041\tVal MAE: 150197.41\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0036\tVal Loss: 0.0041\tVal MAE: 149162.68\telapsed: 0.02 mins\n",
      "Epoch 11: Train Loss: 0.0035\tVal Loss: 0.0040\tVal MAE: 148825.94\telapsed: 0.02 mins\n",
      "Epoch 12: Train Loss: 0.0035\tVal Loss: 0.0041\tVal MAE: 148728.89\telapsed: 0.02 mins\n",
      "Epoch 13: Train Loss: 0.0035\tVal Loss: 0.0040\tVal MAE: 148625.80\telapsed: 0.02 mins\n",
      "Epoch 14: Train Loss: 0.0035\tVal Loss: 0.0040\tVal MAE: 148085.20\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0035\tVal Loss: 0.0040\tVal MAE: 148191.09\telapsed: 0.03 mins\n",
      "Epoch 16: Train Loss: 0.0035\tVal Loss: 0.0040\tVal MAE: 148221.84\telapsed: 0.03 mins\n",
      "Epoch 17: Train Loss: 0.0035\tVal Loss: 0.0040\tVal MAE: 147762.68\telapsed: 0.03 mins\n",
      "Epoch 18: Train Loss: 0.0035\tVal Loss: 0.0040\tVal MAE: 148226.48\telapsed: 0.03 mins\n",
      "Epoch 19: Train Loss: 0.0035\tVal Loss: 0.0040\tVal MAE: 147915.36\telapsed: 0.03 mins\n",
      "Epoch 20: Train Loss: 0.0035\tVal Loss: 0.0040\tVal MAE: 147640.14\telapsed: 0.04 mins\n",
      "Epoch 21: Train Loss: 0.0035\tVal Loss: 0.0040\tVal MAE: 147717.59\telapsed: 0.04 mins\n",
      "Epoch 22: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147489.95\telapsed: 0.04 mins\n",
      "Epoch 23: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147497.32\telapsed: 0.04 mins\n",
      "Epoch 24: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147517.25\telapsed: 0.04 mins\n",
      "Epoch 25: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147218.38\telapsed: 0.04 mins\n",
      "Epoch 26: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147750.29\telapsed: 0.05 mins\n",
      "Epoch 27: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147084.90\telapsed: 0.05 mins\n",
      "Epoch 28: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147228.89\telapsed: 0.05 mins\n",
      "Epoch 29: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147419.74\telapsed: 0.05 mins\n",
      "Epoch 30: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147078.76\telapsed: 0.05 mins\n",
      "Epoch 31: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147370.59\telapsed: 0.06 mins\n",
      "Epoch 32: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147180.26\telapsed: 0.06 mins\n",
      "Epoch 33: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147043.89\telapsed: 0.06 mins\n",
      "Epoch 34: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147340.50\telapsed: 0.06 mins\n",
      "Epoch 35: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147105.80\telapsed: 0.06 mins\n",
      "Epoch 36: Train Loss: 0.0034\tVal Loss: 0.0040\tVal MAE: 147164.82\telapsed: 0.06 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 33\tVal loss: 0.0040\tVal MAE: 147043.89\tTotal time elapsed: 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 15 from 1.0/2016.0 to 3.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0881\tVal Loss: 0.0858\tVal MAE: 1031315.82\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0594\tVal Loss: 0.0575\tVal MAE: 726993.34\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0364\tVal Loss: 0.0351\tVal MAE: 488734.55\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0205\tVal Loss: 0.0197\tVal MAE: 325465.19\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0111\tVal Loss: 0.0107\tVal MAE: 231831.49\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0065\tVal Loss: 0.0065\tVal MAE: 187958.37\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0047\tVal Loss: 0.0049\tVal MAE: 171073.78\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0042\tVal Loss: 0.0044\tVal MAE: 166783.34\telapsed: 0.02 mins\n",
      "Epoch 9: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 165670.57\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 165432.53\telapsed: 0.02 mins\n",
      "Epoch 11: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 165834.68\telapsed: 0.02 mins\n",
      "Epoch 12: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 165453.63\telapsed: 0.02 mins\n",
      "Epoch 13: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 165329.46\telapsed: 0.02 mins\n",
      "Epoch 14: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 165654.00\telapsed: 0.03 mins\n",
      "Epoch 15: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 165228.50\telapsed: 0.03 mins\n",
      "Epoch 16: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 164801.69\telapsed: 0.03 mins\n",
      "Epoch 17: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 165416.78\telapsed: 0.03 mins\n",
      "Epoch 18: Train Loss: 0.0040\tVal Loss: 0.0044\tVal MAE: 165460.42\telapsed: 0.03 mins\n",
      "Epoch 19: Train Loss: 0.0040\tVal Loss: 0.0044\tVal MAE: 165236.12\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 16\tVal loss: 0.0044\tVal MAE: 164801.69\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 16 from 1.0/2016.0 to 4.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0852\tVal Loss: 0.0835\tVal MAE: 1018169.04\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0540\tVal Loss: 0.0529\tVal MAE: 703495.42\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0304\tVal Loss: 0.0298\tVal MAE: 465456.49\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0155\tVal Loss: 0.0154\tVal MAE: 322348.56\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0082\tVal Loss: 0.0085\tVal MAE: 254113.57\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0054\tVal Loss: 0.0059\tVal MAE: 228348.59\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0046\tVal Loss: 0.0052\tVal MAE: 220421.99\telapsed: 0.01 mins\n",
      "Epoch 8: Train Loss: 0.0045\tVal Loss: 0.0050\tVal MAE: 217521.35\telapsed: 0.02 mins\n",
      "Epoch 9: Train Loss: 0.0045\tVal Loss: 0.0051\tVal MAE: 216139.03\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0045\tVal Loss: 0.0051\tVal MAE: 215258.68\telapsed: 0.02 mins\n",
      "Epoch 11: Train Loss: 0.0045\tVal Loss: 0.0051\tVal MAE: 215415.91\telapsed: 0.02 mins\n",
      "Epoch 12: Train Loss: 0.0045\tVal Loss: 0.0051\tVal MAE: 215557.11\telapsed: 0.02 mins\n",
      "Epoch 13: Train Loss: 0.0045\tVal Loss: 0.0051\tVal MAE: 215585.51\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0051\tVal MAE: 215258.68\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 17 from 1.0/2016.0 to 5.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0748\tVal Loss: 0.0726\tVal MAE: 901799.03\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0414\tVal Loss: 0.0400\tVal MAE: 554670.54\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0205\tVal Loss: 0.0198\tVal MAE: 339945.91\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0100\tVal Loss: 0.0097\tVal MAE: 236513.17\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0062\tVal Loss: 0.0060\tVal MAE: 199673.41\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0052\tVal Loss: 0.0050\tVal MAE: 188528.10\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0050\tVal Loss: 0.0047\tVal MAE: 185023.04\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0051\tVal Loss: 0.0047\tVal MAE: 184993.62\telapsed: 0.02 mins\n",
      "Epoch 9: Train Loss: 0.0051\tVal Loss: 0.0047\tVal MAE: 185635.14\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0051\tVal Loss: 0.0047\tVal MAE: 185487.89\telapsed: 0.02 mins\n",
      "Epoch 11: Train Loss: 0.0052\tVal Loss: 0.0047\tVal MAE: 185665.04\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0047\tVal MAE: 184993.62\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 18 from 1.0/2016.0 to 6.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0899\tVal Loss: 0.0889\tVal MAE: 1128098.12\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0633\tVal Loss: 0.0624\tVal MAE: 850073.16\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0401\tVal Loss: 0.0396\tVal MAE: 607564.93\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0227\tVal Loss: 0.0226\tVal MAE: 428514.88\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0121\tVal Loss: 0.0124\tVal MAE: 322898.55\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0069\tVal Loss: 0.0075\tVal MAE: 271567.82\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0050\tVal Loss: 0.0057\tVal MAE: 252696.82\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0045\tVal Loss: 0.0052\tVal MAE: 245501.46\telapsed: 0.02 mins\n",
      "Epoch 9: Train Loss: 0.0044\tVal Loss: 0.0052\tVal MAE: 244440.29\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0044\tVal Loss: 0.0052\tVal MAE: 244141.67\telapsed: 0.02 mins\n",
      "Epoch 11: Train Loss: 0.0044\tVal Loss: 0.0052\tVal MAE: 243727.76\telapsed: 0.03 mins\n",
      "Epoch 12: Train Loss: 0.0044\tVal Loss: 0.0052\tVal MAE: 243246.30\telapsed: 0.03 mins\n",
      "Epoch 13: Train Loss: 0.0044\tVal Loss: 0.0052\tVal MAE: 243614.58\telapsed: 0.03 mins\n",
      "Epoch 14: Train Loss: 0.0043\tVal Loss: 0.0052\tVal MAE: 242915.31\telapsed: 0.03 mins\n",
      "Epoch 15: Train Loss: 0.0043\tVal Loss: 0.0052\tVal MAE: 243113.05\telapsed: 0.03 mins\n",
      "Epoch 16: Train Loss: 0.0044\tVal Loss: 0.0052\tVal MAE: 243235.87\telapsed: 0.04 mins\n",
      "Epoch 17: Train Loss: 0.0043\tVal Loss: 0.0052\tVal MAE: 242628.43\telapsed: 0.04 mins\n",
      "Epoch 18: Train Loss: 0.0043\tVal Loss: 0.0052\tVal MAE: 242853.55\telapsed: 0.04 mins\n",
      "Epoch 19: Train Loss: 0.0043\tVal Loss: 0.0052\tVal MAE: 242879.46\telapsed: 0.04 mins\n",
      "Epoch 20: Train Loss: 0.0043\tVal Loss: 0.0052\tVal MAE: 242358.26\telapsed: 0.05 mins\n",
      "Epoch 21: Train Loss: 0.0043\tVal Loss: 0.0052\tVal MAE: 242561.09\telapsed: 0.05 mins\n",
      "Epoch 22: Train Loss: 0.0043\tVal Loss: 0.0052\tVal MAE: 242609.96\telapsed: 0.05 mins\n",
      "Epoch 23: Train Loss: 0.0043\tVal Loss: 0.0052\tVal MAE: 242703.89\telapsed: 0.05 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 20\tVal loss: 0.0052\tVal MAE: 242358.26\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 19 from 1.0/2016.0 to 7.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0732\tVal Loss: 0.0713\tVal MAE: 915595.03\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0392\tVal Loss: 0.0381\tVal MAE: 558648.96\telapsed: 0.00 mins\n",
      "Epoch 3: Train Loss: 0.0183\tVal Loss: 0.0180\tVal MAE: 343741.80\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0089\tVal Loss: 0.0090\tVal MAE: 249470.32\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0058\tVal Loss: 0.0063\tVal MAE: 218748.20\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0051\tVal Loss: 0.0057\tVal MAE: 210410.39\telapsed: 0.01 mins\n",
      "Epoch 7: Train Loss: 0.0049\tVal Loss: 0.0055\tVal MAE: 207273.70\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0049\tVal Loss: 0.0055\tVal MAE: 206275.61\telapsed: 0.02 mins\n",
      "Epoch 9: Train Loss: 0.0049\tVal Loss: 0.0055\tVal MAE: 205566.93\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0049\tVal Loss: 0.0054\tVal MAE: 204436.52\telapsed: 0.02 mins\n",
      "Epoch 11: Train Loss: 0.0049\tVal Loss: 0.0054\tVal MAE: 204541.97\telapsed: 0.03 mins\n",
      "Epoch 12: Train Loss: 0.0049\tVal Loss: 0.0054\tVal MAE: 204261.31\telapsed: 0.03 mins\n",
      "Epoch 13: Train Loss: 0.0049\tVal Loss: 0.0054\tVal MAE: 203850.54\telapsed: 0.03 mins\n",
      "Epoch 14: Train Loss: 0.0049\tVal Loss: 0.0054\tVal MAE: 203613.61\telapsed: 0.03 mins\n",
      "Epoch 15: Train Loss: 0.0049\tVal Loss: 0.0054\tVal MAE: 204033.88\telapsed: 0.04 mins\n",
      "Epoch 16: Train Loss: 0.0049\tVal Loss: 0.0054\tVal MAE: 203998.88\telapsed: 0.04 mins\n",
      "Epoch 17: Train Loss: 0.0049\tVal Loss: 0.0054\tVal MAE: 203791.66\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 14\tVal loss: 0.0054\tVal MAE: 203613.61\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 20 from 1.0/2016.0 to 8.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0719\tVal Loss: 0.0713\tVal MAE: 970746.19\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0380\tVal Loss: 0.0402\tVal MAE: 640532.53\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0174\tVal Loss: 0.0225\tVal MAE: 446135.56\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0086\tVal Loss: 0.0159\tVal MAE: 368781.43\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0061\tVal Loss: 0.0143\tVal MAE: 348776.05\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0056\tVal Loss: 0.0139\tVal MAE: 343690.67\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0055\tVal Loss: 0.0139\tVal MAE: 341181.24\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0055\tVal Loss: 0.0139\tVal MAE: 341087.25\telapsed: 0.02 mins\n",
      "Epoch 9: Train Loss: 0.0055\tVal Loss: 0.0139\tVal MAE: 340748.79\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0055\tVal Loss: 0.0139\tVal MAE: 340893.13\telapsed: 0.03 mins\n",
      "Epoch 11: Train Loss: 0.0055\tVal Loss: 0.0139\tVal MAE: 340981.99\telapsed: 0.03 mins\n",
      "Epoch 12: Train Loss: 0.0055\tVal Loss: 0.0139\tVal MAE: 341108.91\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0139\tVal MAE: 340748.79\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 21 from 1.0/2016.0 to 9.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0773\tVal Loss: 0.0741\tVal MAE: 968943.57\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0475\tVal Loss: 0.0429\tVal MAE: 639813.60\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0270\tVal Loss: 0.0204\tVal MAE: 411995.98\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0174\tVal Loss: 0.0095\tVal MAE: 303838.24\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0146\tVal Loss: 0.0060\tVal MAE: 269112.91\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0141\tVal Loss: 0.0052\tVal MAE: 259751.39\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0139\tVal Loss: 0.0051\tVal MAE: 257978.60\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0139\tVal Loss: 0.0051\tVal MAE: 257951.38\telapsed: 0.02 mins\n",
      "Epoch 9: Train Loss: 0.0140\tVal Loss: 0.0051\tVal MAE: 258633.64\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0140\tVal Loss: 0.0051\tVal MAE: 258156.49\telapsed: 0.03 mins\n",
      "Epoch 11: Train Loss: 0.0140\tVal Loss: 0.0052\tVal MAE: 258980.12\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0051\tVal MAE: 257951.38\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 22 from 1.0/2016.0 to 10.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0771\tVal Loss: 0.0748\tVal MAE: 924352.39\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0426\tVal Loss: 0.0408\tVal MAE: 557776.37\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0198\tVal Loss: 0.0186\tVal MAE: 318890.85\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0092\tVal Loss: 0.0086\tVal MAE: 207973.10\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0058\tVal Loss: 0.0058\tVal MAE: 176664.50\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0051\tVal Loss: 0.0054\tVal MAE: 170115.00\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0050\tVal Loss: 0.0054\tVal MAE: 171045.39\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0050\tVal Loss: 0.0054\tVal MAE: 171931.35\telapsed: 0.02 mins\n",
      "Epoch 9: Train Loss: 0.0050\tVal Loss: 0.0054\tVal MAE: 173022.39\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 6\tVal loss: 0.0054\tVal MAE: 170115.00\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 23 from 1.0/2016.0 to 11.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0664\tVal Loss: 0.0643\tVal MAE: 826559.74\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0318\tVal Loss: 0.0302\tVal MAE: 475045.53\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0129\tVal Loss: 0.0119\tVal MAE: 290410.69\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0065\tVal Loss: 0.0057\tVal MAE: 231525.58\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0053\tVal Loss: 0.0045\tVal MAE: 223313.43\telapsed: 0.01 mins\n",
      "Epoch 6: Train Loss: 0.0051\tVal Loss: 0.0043\tVal MAE: 224811.27\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0051\tVal Loss: 0.0043\tVal MAE: 225762.85\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0051\tVal Loss: 0.0043\tVal MAE: 226500.61\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 5\tVal loss: 0.0045\tVal MAE: 223313.43\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 24 from 1.0/2016.0 to 12.0/2017.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0723\tVal Loss: 0.0706\tVal MAE: 881391.89\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0373\tVal Loss: 0.0361\tVal MAE: 514345.45\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0162\tVal Loss: 0.0157\tVal MAE: 298463.22\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0071\tVal Loss: 0.0072\tVal MAE: 211472.15\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0046\tVal Loss: 0.0049\tVal MAE: 186511.47\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 183502.29\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0040\tVal Loss: 0.0044\tVal MAE: 185505.91\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 186730.83\telapsed: 0.02 mins\n",
      "Epoch 9: Train Loss: 0.0041\tVal Loss: 0.0044\tVal MAE: 188430.85\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 6\tVal loss: 0.0044\tVal MAE: 183502.29\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 25 from 1.0/2016.0 to 1.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0670\tVal Loss: 0.0650\tVal MAE: 817311.67\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0293\tVal Loss: 0.0279\tVal MAE: 426299.59\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0110\tVal Loss: 0.0102\tVal MAE: 247502.32\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0053\tVal Loss: 0.0047\tVal MAE: 194251.11\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0043\tVal Loss: 0.0037\tVal MAE: 189726.59\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0042\tVal Loss: 0.0035\tVal MAE: 189072.17\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0042\tVal Loss: 0.0036\tVal MAE: 190549.31\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0042\tVal Loss: 0.0036\tVal MAE: 190464.50\telapsed: 0.03 mins\n",
      "Epoch 9: Train Loss: 0.0042\tVal Loss: 0.0035\tVal MAE: 191516.16\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 6\tVal loss: 0.0035\tVal MAE: 189072.17\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 26 from 1.0/2016.0 to 2.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0559\tVal Loss: 0.0540\tVal MAE: 684413.46\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0195\tVal Loss: 0.0187\tVal MAE: 313524.63\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0064\tVal Loss: 0.0064\tVal MAE: 192028.34\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0039\tVal Loss: 0.0040\tVal MAE: 174293.36\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0035\tVal Loss: 0.0037\tVal MAE: 172582.49\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0035\tVal Loss: 0.0037\tVal MAE: 174610.93\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0035\tVal Loss: 0.0037\tVal MAE: 174771.59\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0035\tVal Loss: 0.0037\tVal MAE: 174926.86\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 5\tVal loss: 0.0037\tVal MAE: 172582.49\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 27 from 1.0/2016.0 to 3.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0701\tVal Loss: 0.0683\tVal MAE: 837381.28\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0312\tVal Loss: 0.0299\tVal MAE: 439119.22\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0104\tVal Loss: 0.0097\tVal MAE: 229289.04\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0046\tVal Loss: 0.0043\tVal MAE: 175015.39\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0036\tVal Loss: 0.0034\tVal MAE: 167389.81\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0035\tVal Loss: 0.0033\tVal MAE: 168646.88\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0035\tVal Loss: 0.0034\tVal MAE: 171043.56\telapsed: 0.02 mins\n",
      "Epoch 8: Train Loss: 0.0035\tVal Loss: 0.0033\tVal MAE: 170032.67\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 5\tVal loss: 0.0034\tVal MAE: 167389.81\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 28 from 1.0/2016.0 to 4.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0634\tVal Loss: 0.0619\tVal MAE: 773719.25\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0240\tVal Loss: 0.0232\tVal MAE: 363850.16\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0074\tVal Loss: 0.0072\tVal MAE: 203265.03\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0038\tVal Loss: 0.0039\tVal MAE: 173845.95\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0034\tVal Loss: 0.0035\tVal MAE: 171329.96\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0034\tVal Loss: 0.0035\tVal MAE: 171144.12\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 171206.21\telapsed: 0.03 mins\n",
      "Epoch 8: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 171672.22\telapsed: 0.03 mins\n",
      "Epoch 9: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 171234.37\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 6\tVal loss: 0.0035\tVal MAE: 171144.12\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 29 from 1.0/2016.0 to 5.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0621\tVal Loss: 0.0603\tVal MAE: 758787.93\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0239\tVal Loss: 0.0233\tVal MAE: 359154.73\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0073\tVal Loss: 0.0074\tVal MAE: 199746.74\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0038\tVal Loss: 0.0042\tVal MAE: 169927.05\telapsed: 0.01 mins\n",
      "Epoch 5: Train Loss: 0.0034\tVal Loss: 0.0038\tVal MAE: 167945.98\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0034\tVal Loss: 0.0038\tVal MAE: 168107.28\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0034\tVal Loss: 0.0038\tVal MAE: 168674.16\telapsed: 0.03 mins\n",
      "Epoch 8: Train Loss: 0.0034\tVal Loss: 0.0038\tVal MAE: 168886.75\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 5\tVal loss: 0.0038\tVal MAE: 167945.98\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 30 from 1.0/2016.0 to 6.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0577\tVal Loss: 0.0562\tVal MAE: 740748.83\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0187\tVal Loss: 0.0180\tVal MAE: 336703.06\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0059\tVal Loss: 0.0059\tVal MAE: 213340.97\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0040\tVal Loss: 0.0041\tVal MAE: 195215.73\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0038\tVal Loss: 0.0040\tVal MAE: 195245.80\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0038\tVal Loss: 0.0040\tVal MAE: 193854.30\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0038\tVal Loss: 0.0040\tVal MAE: 195161.45\telapsed: 0.03 mins\n",
      "Epoch 8: Train Loss: 0.0038\tVal Loss: 0.0040\tVal MAE: 194453.31\telapsed: 0.03 mins\n",
      "Epoch 9: Train Loss: 0.0038\tVal Loss: 0.0040\tVal MAE: 194885.41\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 6\tVal loss: 0.0040\tVal MAE: 193854.30\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 31 from 1.0/2016.0 to 7.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0636\tVal Loss: 0.0623\tVal MAE: 844644.29\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0238\tVal Loss: 0.0231\tVal MAE: 424833.61\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0074\tVal Loss: 0.0073\tVal MAE: 257333.61\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0042\tVal Loss: 0.0043\tVal MAE: 222282.59\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0039\tVal Loss: 0.0040\tVal MAE: 215988.98\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0039\tVal Loss: 0.0040\tVal MAE: 216246.54\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0039\tVal Loss: 0.0040\tVal MAE: 215084.99\telapsed: 0.03 mins\n",
      "Epoch 8: Train Loss: 0.0039\tVal Loss: 0.0040\tVal MAE: 215697.51\telapsed: 0.03 mins\n",
      "Epoch 9: Train Loss: 0.0039\tVal Loss: 0.0040\tVal MAE: 215113.93\telapsed: 0.04 mins\n",
      "Epoch 10: Train Loss: 0.0039\tVal Loss: 0.0040\tVal MAE: 215844.69\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 7\tVal loss: 0.0040\tVal MAE: 215084.99\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 32 from 1.0/2016.0 to 8.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0547\tVal Loss: 0.0530\tVal MAE: 710574.31\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0158\tVal Loss: 0.0152\tVal MAE: 317617.56\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0052\tVal Loss: 0.0052\tVal MAE: 220022.63\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0041\tVal Loss: 0.0042\tVal MAE: 211545.65\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0040\tVal Loss: 0.0041\tVal MAE: 210663.08\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0040\tVal Loss: 0.0041\tVal MAE: 211213.65\telapsed: 0.02 mins\n",
      "Epoch 7: Train Loss: 0.0040\tVal Loss: 0.0041\tVal MAE: 211776.50\telapsed: 0.03 mins\n",
      "Epoch 8: Train Loss: 0.0040\tVal Loss: 0.0041\tVal MAE: 211547.66\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 5\tVal loss: 0.0041\tVal MAE: 210663.08\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 33 from 1.0/2016.0 to 9.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0563\tVal Loss: 0.0547\tVal MAE: 749161.52\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0181\tVal Loss: 0.0174\tVal MAE: 359064.71\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0057\tVal Loss: 0.0061\tVal MAE: 244664.83\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0042\tVal Loss: 0.0049\tVal MAE: 233392.08\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0041\tVal Loss: 0.0048\tVal MAE: 233420.08\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0041\tVal Loss: 0.0048\tVal MAE: 234340.60\telapsed: 0.03 mins\n",
      "Epoch 7: Train Loss: 0.0041\tVal Loss: 0.0048\tVal MAE: 234237.68\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0049\tVal MAE: 233392.08\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 34 from 1.0/2016.0 to 10.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0523\tVal Loss: 0.0504\tVal MAE: 660738.69\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0151\tVal Loss: 0.0140\tVal MAE: 279114.63\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0058\tVal Loss: 0.0051\tVal MAE: 187716.69\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0049\tVal Loss: 0.0042\tVal MAE: 179302.05\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0048\tVal Loss: 0.0042\tVal MAE: 180156.34\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0049\tVal Loss: 0.0042\tVal MAE: 180400.90\telapsed: 0.03 mins\n",
      "Epoch 7: Train Loss: 0.0049\tVal Loss: 0.0042\tVal MAE: 179425.30\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0042\tVal MAE: 179302.05\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 35 from 1.0/2016.0 to 11.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0535\tVal Loss: 0.0516\tVal MAE: 662367.79\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0153\tVal Loss: 0.0145\tVal MAE: 272361.20\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0051\tVal Loss: 0.0049\tVal MAE: 177255.95\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0041\tVal Loss: 0.0040\tVal MAE: 172053.80\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0041\tVal Loss: 0.0040\tVal MAE: 173108.73\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0041\tVal Loss: 0.0040\tVal MAE: 174536.13\telapsed: 0.03 mins\n",
      "Epoch 7: Train Loss: 0.0041\tVal Loss: 0.0040\tVal MAE: 174418.51\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0040\tVal MAE: 172053.80\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 36 from 1.0/2016.0 to 12.0/2018.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0497\tVal Loss: 0.0482\tVal MAE: 625318.51\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0129\tVal Loss: 0.0123\tVal MAE: 256115.39\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0045\tVal Loss: 0.0043\tVal MAE: 182700.06\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0039\tVal Loss: 0.0038\tVal MAE: 181394.28\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0039\tVal Loss: 0.0038\tVal MAE: 182838.87\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0039\tVal Loss: 0.0038\tVal MAE: 183252.82\telapsed: 0.03 mins\n",
      "Epoch 7: Train Loss: 0.0039\tVal Loss: 0.0038\tVal MAE: 183630.69\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0038\tVal MAE: 181394.28\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 37 from 1.0/2016.0 to 1.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0422\tVal Loss: 0.0407\tVal MAE: 563176.35\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0091\tVal Loss: 0.0089\tVal MAE: 229335.29\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0040\tVal Loss: 0.0042\tVal MAE: 186366.76\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0038\tVal Loss: 0.0041\tVal MAE: 187013.47\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0038\tVal Loss: 0.0041\tVal MAE: 188560.49\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0038\tVal Loss: 0.0041\tVal MAE: 188515.35\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 3\tVal loss: 0.0042\tVal MAE: 186366.76\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 38 from 1.0/2016.0 to 2.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0488\tVal Loss: 0.0472\tVal MAE: 584301.40\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0118\tVal Loss: 0.0110\tVal MAE: 206587.10\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0045\tVal Loss: 0.0039\tVal MAE: 144263.49\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0041\tVal Loss: 0.0034\tVal MAE: 142613.24\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0041\tVal Loss: 0.0034\tVal MAE: 143824.17\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0042\tVal Loss: 0.0034\tVal MAE: 143721.90\telapsed: 0.03 mins\n",
      "Epoch 7: Train Loss: 0.0042\tVal Loss: 0.0034\tVal MAE: 144317.04\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0034\tVal MAE: 142613.24\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 39 from 1.0/2016.0 to 3.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0500\tVal Loss: 0.0486\tVal MAE: 622486.10\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0132\tVal Loss: 0.0127\tVal MAE: 251879.60\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0041\tVal Loss: 0.0039\tVal MAE: 165657.47\telapsed: 0.01 mins\n",
      "Epoch 4: Train Loss: 0.0033\tVal Loss: 0.0032\tVal MAE: 160542.00\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0033\tVal Loss: 0.0032\tVal MAE: 163593.44\telapsed: 0.02 mins\n",
      "Epoch 6: Train Loss: 0.0033\tVal Loss: 0.0032\tVal MAE: 165017.65\telapsed: 0.03 mins\n",
      "Epoch 7: Train Loss: 0.0033\tVal Loss: 0.0032\tVal MAE: 164912.64\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0032\tVal MAE: 160542.00\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 40 from 1.0/2016.0 to 4.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0496\tVal Loss: 0.0481\tVal MAE: 622660.77\telapsed: 0.00 mins\n",
      "Epoch 2: Train Loss: 0.0114\tVal Loss: 0.0110\tVal MAE: 233577.41\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0037\tVal Loss: 0.0037\tVal MAE: 153170.46\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0032\tVal Loss: 0.0033\tVal MAE: 149834.47\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0032\tVal Loss: 0.0033\tVal MAE: 151573.81\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0032\tVal Loss: 0.0033\tVal MAE: 151678.15\telapsed: 0.03 mins\n",
      "Epoch 7: Train Loss: 0.0032\tVal Loss: 0.0033\tVal MAE: 152304.61\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0033\tVal MAE: 149834.47\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 41 from 1.0/2016.0 to 5.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0404\tVal Loss: 0.0392\tVal MAE: 530799.08\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0082\tVal Loss: 0.0080\tVal MAE: 201256.52\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0035\tVal Loss: 0.0036\tVal MAE: 158087.43\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0032\tVal Loss: 0.0034\tVal MAE: 156349.14\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0033\tVal Loss: 0.0034\tVal MAE: 157322.05\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0033\tVal Loss: 0.0034\tVal MAE: 157713.85\telapsed: 0.03 mins\n",
      "Epoch 7: Train Loss: 0.0033\tVal Loss: 0.0034\tVal MAE: 158005.46\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0034\tVal MAE: 156349.14\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 42 from 1.0/2016.0 to 6.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0387\tVal Loss: 0.0374\tVal MAE: 519285.36\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0069\tVal Loss: 0.0067\tVal MAE: 200736.25\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0034\tVal Loss: 0.0034\tVal MAE: 168379.48\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0033\tVal Loss: 0.0033\tVal MAE: 167144.66\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0033\tVal Loss: 0.0033\tVal MAE: 167419.33\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0033\tVal Loss: 0.0033\tVal MAE: 167692.62\telapsed: 0.03 mins\n",
      "Epoch 7: Train Loss: 0.0033\tVal Loss: 0.0033\tVal MAE: 167631.05\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0033\tVal MAE: 167144.66\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 43 from 1.0/2016.0 to 7.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0324\tVal Loss: 0.0312\tVal MAE: 460936.88\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0054\tVal Loss: 0.0054\tVal MAE: 186112.20\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0034\tVal Loss: 0.0034\tVal MAE: 166928.41\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0033\tVal Loss: 0.0034\tVal MAE: 165154.32\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0033\tVal Loss: 0.0033\tVal MAE: 164909.52\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0033\tVal Loss: 0.0033\tVal MAE: 164560.50\telapsed: 0.04 mins\n",
      "Epoch 7: Train Loss: 0.0033\tVal Loss: 0.0033\tVal MAE: 164568.77\telapsed: 0.04 mins\n",
      "Epoch 8: Train Loss: 0.0033\tVal Loss: 0.0033\tVal MAE: 164740.50\telapsed: 0.05 mins\n",
      "Epoch 9: Train Loss: 0.0033\tVal Loss: 0.0033\tVal MAE: 164850.80\telapsed: 0.05 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 6\tVal loss: 0.0033\tVal MAE: 164560.50\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 44 from 1.0/2016.0 to 8.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0366\tVal Loss: 0.0351\tVal MAE: 480900.11\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0060\tVal Loss: 0.0059\tVal MAE: 172835.03\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0034\tVal Loss: 0.0036\tVal MAE: 151664.77\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 150692.06\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 150373.25\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 150390.76\telapsed: 0.04 mins\n",
      "Epoch 7: Train Loss: 0.0033\tVal Loss: 0.0034\tVal MAE: 149633.09\telapsed: 0.04 mins\n",
      "Epoch 8: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 149477.29\telapsed: 0.05 mins\n",
      "Epoch 9: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 149557.79\telapsed: 0.06 mins\n",
      "Epoch 10: Train Loss: 0.0033\tVal Loss: 0.0034\tVal MAE: 149500.65\telapsed: 0.06 mins\n",
      "Epoch 11: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 149644.94\telapsed: 0.07 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0035\tVal MAE: 149477.29\tTotal time elapsed: 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 45 from 1.0/2016.0 to 9.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0370\tVal Loss: 0.0356\tVal MAE: 484161.32\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0065\tVal Loss: 0.0060\tVal MAE: 179470.07\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0036\tVal Loss: 0.0032\tVal MAE: 154514.29\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0035\tVal Loss: 0.0031\tVal MAE: 152900.45\telapsed: 0.03 mins\n",
      "Epoch 5: Train Loss: 0.0035\tVal Loss: 0.0031\tVal MAE: 153445.26\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0034\tVal Loss: 0.0031\tVal MAE: 153049.07\telapsed: 0.04 mins\n",
      "Epoch 7: Train Loss: 0.0035\tVal Loss: 0.0031\tVal MAE: 153334.36\telapsed: 0.05 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0031\tVal MAE: 152900.45\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 46 from 1.0/2016.0 to 10.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0413\tVal Loss: 0.0397\tVal MAE: 503563.83\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0071\tVal Loss: 0.0070\tVal MAE: 147895.14\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0031\tVal Loss: 0.0037\tVal MAE: 116483.93\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0031\tVal Loss: 0.0037\tVal MAE: 116766.15\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0030\tVal Loss: 0.0037\tVal MAE: 117641.46\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0030\tVal Loss: 0.0037\tVal MAE: 117739.01\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 3\tVal loss: 0.0037\tVal MAE: 116483.93\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 47 from 1.0/2016.0 to 11.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0435\tVal Loss: 0.0422\tVal MAE: 536808.69\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0077\tVal Loss: 0.0072\tVal MAE: 163940.76\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0037\tVal Loss: 0.0033\tVal MAE: 119569.14\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0037\tVal Loss: 0.0033\tVal MAE: 121908.91\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0037\tVal Loss: 0.0033\tVal MAE: 122505.65\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0037\tVal Loss: 0.0033\tVal MAE: 122652.47\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 3\tVal loss: 0.0033\tVal MAE: 119569.14\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 48 from 1.0/2016.0 to 12.0/2019.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0379\tVal Loss: 0.0369\tVal MAE: 496333.73\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0059\tVal Loss: 0.0060\tVal MAE: 189737.03\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0033\tVal Loss: 0.0036\tVal MAE: 170056.51\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 171045.81\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 173235.32\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0033\tVal Loss: 0.0035\tVal MAE: 172432.69\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 3\tVal loss: 0.0036\tVal MAE: 170056.51\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 49 from 1.0/2016.0 to 1.0/2021.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0425\tVal Loss: 0.0413\tVal MAE: 566521.31\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0078\tVal Loss: 0.0077\tVal MAE: 204250.60\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0035\tVal Loss: 0.0037\tVal MAE: 165986.36\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0034\tVal Loss: 0.0036\tVal MAE: 167212.66\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0034\tVal Loss: 0.0036\tVal MAE: 168686.17\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0034\tVal Loss: 0.0036\tVal MAE: 169173.19\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 3\tVal loss: 0.0037\tVal MAE: 165986.36\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 50 from 1.0/2016.0 to 2.0/2021.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0346\tVal Loss: 0.0333\tVal MAE: 456834.36\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0055\tVal Loss: 0.0050\tVal MAE: 164651.68\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0037\tVal Loss: 0.0032\tVal MAE: 145784.24\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0037\tVal Loss: 0.0032\tVal MAE: 146628.52\telapsed: 0.03 mins\n",
      "Epoch 5: Train Loss: 0.0037\tVal Loss: 0.0032\tVal MAE: 147967.62\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0037\tVal Loss: 0.0032\tVal MAE: 147863.80\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 3\tVal loss: 0.0032\tVal MAE: 145784.24\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 51 from 1.0/2016.0 to 3.0/2021.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0387\tVal Loss: 0.0376\tVal MAE: 510301.42\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0061\tVal Loss: 0.0060\tVal MAE: 184657.80\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0033\tVal Loss: 0.0034\tVal MAE: 163214.47\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0032\tVal Loss: 0.0034\tVal MAE: 163972.56\telapsed: 0.02 mins\n",
      "Epoch 5: Train Loss: 0.0032\tVal Loss: 0.0034\tVal MAE: 165245.00\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0032\tVal Loss: 0.0034\tVal MAE: 165557.81\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 3\tVal loss: 0.0034\tVal MAE: 163214.47\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 52 from 1.0/2016.0 to 4.0/2021.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0312\tVal Loss: 0.0305\tVal MAE: 456533.55\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0044\tVal Loss: 0.0049\tVal MAE: 198236.82\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0034\tVal Loss: 0.0041\tVal MAE: 194145.92\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0035\tVal Loss: 0.0041\tVal MAE: 194960.18\telapsed: 0.03 mins\n",
      "Epoch 5: Train Loss: 0.0035\tVal Loss: 0.0042\tVal MAE: 195340.36\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0035\tVal Loss: 0.0042\tVal MAE: 196817.07\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 3\tVal loss: 0.0041\tVal MAE: 194145.92\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 53 from 1.0/2016.0 to 5.0/2021.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0314\tVal Loss: 0.0301\tVal MAE: 438671.97\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0052\tVal Loss: 0.0046\tVal MAE: 170362.69\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0042\tVal Loss: 0.0036\tVal MAE: 159939.44\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0043\tVal Loss: 0.0036\tVal MAE: 160019.84\telapsed: 0.03 mins\n",
      "Epoch 5: Train Loss: 0.0043\tVal Loss: 0.0036\tVal MAE: 160228.11\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0043\tVal Loss: 0.0036\tVal MAE: 160423.03\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 3\tVal loss: 0.0036\tVal MAE: 159939.44\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 54 from 1.0/2016.0 to 6.0/2021.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0263\tVal Loss: 0.0253\tVal MAE: 365544.93\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0041\tVal Loss: 0.0043\tVal MAE: 141859.19\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0036\tVal Loss: 0.0038\tVal MAE: 136547.35\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0036\tVal Loss: 0.0038\tVal MAE: 136113.56\telapsed: 0.03 mins\n",
      "Epoch 5: Train Loss: 0.0037\tVal Loss: 0.0039\tVal MAE: 136757.38\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0037\tVal Loss: 0.0039\tVal MAE: 136720.43\telapsed: 0.04 mins\n",
      "Epoch 7: Train Loss: 0.0036\tVal Loss: 0.0039\tVal MAE: 137042.54\telapsed: 0.05 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0038\tVal MAE: 136113.56\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 55 from 1.0/2016.0 to 7.0/2021.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0248\tVal Loss: 0.0247\tVal MAE: 360919.24\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0041\tVal Loss: 0.0056\tVal MAE: 147437.26\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0036\tVal Loss: 0.0051\tVal MAE: 141286.61\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0036\tVal Loss: 0.0052\tVal MAE: 140906.98\telapsed: 0.03 mins\n",
      "Epoch 5: Train Loss: 0.0037\tVal Loss: 0.0052\tVal MAE: 141412.02\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0037\tVal Loss: 0.0052\tVal MAE: 140957.97\telapsed: 0.04 mins\n",
      "Epoch 7: Train Loss: 0.0037\tVal Loss: 0.0052\tVal MAE: 141091.21\telapsed: 0.05 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 4\tVal loss: 0.0052\tVal MAE: 140906.98\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 56 from 1.0/2016.0 to 8.0/2021.0:\n",
      "\n",
      "\n",
      "Epoch 1: Train Loss: 0.0241\tVal Loss: 0.0224\tVal MAE: 300080.22\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0055\tVal Loss: 0.0041\tVal MAE: 92686.76\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0051\tVal Loss: 0.0036\tVal MAE: 88942.43\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0051\tVal Loss: 0.0036\tVal MAE: 89065.81\telapsed: 0.03 mins\n",
      "Epoch 5: Train Loss: 0.0051\tVal Loss: 0.0036\tVal MAE: 88251.50\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0052\tVal Loss: 0.0037\tVal MAE: 89387.30\telapsed: 0.04 mins\n",
      "Epoch 7: Train Loss: 0.0052\tVal Loss: 0.0037\tVal MAE: 90007.83\telapsed: 0.05 mins\n",
      "Epoch 8: Train Loss: 0.0052\tVal Loss: 0.0038\tVal MAE: 89217.54\telapsed: 0.06 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 5\tVal loss: 0.0036\tVal MAE: 88251.50\tTotal time elapsed: 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Average Val loss: 0.0042\tAverage Val MAE: 170167.7595\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fossil.preprocessing import FossilDataset, collate_fn\n",
    "from fossil.models.blender import FossilBlender\n",
    "from fossil.utils.pipeline import FossilPipeline\n",
    "\n",
    "expanding_window = True\n",
    "models, metrics_list, loss_list,  = [], [], []\n",
    "train_set, val_set = [], []\n",
    "\n",
    "for ix in range(len(meta_dates[:-1])):\n",
    "    if expanding_window:\n",
    "        print('Training step {} from {}/{} to {}/{}:'.format(ix+1, meta_dates[0][0], meta_dates[0][1], meta_dates[ix][0], meta_dates[ix][1]))\n",
    "        print('\\n')\n",
    "        train_set += [meta_train[ix]]    \n",
    "        val_set += [meta_val[ix]]\n",
    "\n",
    "    # else:\n",
    "    #     print('Training step {} from {}/{} to {}/{}:'.format(ix+1, dates[ix-window][0], dates[ix-window][1], dates[ix][0], dates[ix][1]))\n",
    "    #     print('\\n')\n",
    "    #     train_set = [train_norm[i] for i in range(ix-window,ix)] if ix>window else train_norm[:ix+1]\n",
    "    #     val_set = [val_norm[i] for i in range(ix-window,ix)] if ix>window else val_norm[:ix+1]\n",
    "       \n",
    "    meta_train_set = FossilDataset(train_set)\n",
    "    meta_val_set = FossilDataset(val_set)\n",
    "\n",
    "    meta_train_loader = DataLoader(meta_train_set, collate_fn=lambda x: collate_fn(x, len(sku_encoder)), shuffle=False)\n",
    "    meta_val_loader = DataLoader(meta_val_set, collate_fn=lambda x: collate_fn(x, len(sku_encoder)), shuffle=False)\n",
    "    # model = pipe.model if ix>0 else FossilModel(len(features), 4, len(sku_encoder)+1, N_STEPS).double()\n",
    "    \n",
    "    model = FossilBlender(len(meta_features), 4, len(sku_encoder)+1, ModelsConfig.N_STEPS).double()\n",
    "    pipe = FossilPipeline(meta_train_loader, meta_val_loader, model, y_scaler)\n",
    "    \n",
    "    torch.manual_seed(ModelsConfig.SEED)\n",
    "    pipe.train_model(3, 1)\n",
    "    best_model, val_loss, metrics = pipe.model_checkpoints[-1]\n",
    "    \n",
    "    models.append(best_model)\n",
    "    loss_list.append(val_loss)\n",
    "    metrics_list.append(metrics)\n",
    "print('Average Val loss: {:.4f}\\tAverage Val MAE: {:.4f}'.format(np.mean(loss_list), np.mean(metrics_list)))\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fb6a6a5-7e6a-4f70-b36d-8808fab72cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fossil.preprocessing import FossilDataset, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "meta_train_set = FossilDataset(meta_train)\n",
    "meta_val_set = FossilDataset(meta_val)\n",
    "\n",
    "meta_train_loader = DataLoader(meta_train_set, collate_fn=lambda x: collate_fn(x, len(sku_encoder)), shuffle=False)\n",
    "meta_val_loader = DataLoader(meta_val_set, collate_fn=lambda x: collate_fn(x, len(sku_encoder)), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "648480cf-3df3-4241-9c99-604a04b92724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fossil.models.blender import FossilBlender\n",
    "from fossil.utils.pipeline import FossilPipeline\n",
    "\n",
    "model = FossilBlender(len(meta_features), 4, len(sku_encoder)+1, ModelsConfig.N_STEPS).double()\n",
    "pipe = FossilPipeline(meta_train_loader, meta_val_loader, model, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf731de3-6d4e-4268-9d0c-f6f2acdec205",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0271\tVal Loss: 0.0254\tVal MAE: 337114.90\telapsed: 0.01 mins\n",
      "Epoch 2: Train Loss: 0.0058\tVal Loss: 0.0045\tVal MAE: 100048.10\telapsed: 0.01 mins\n",
      "Epoch 3: Train Loss: 0.0053\tVal Loss: 0.0039\tVal MAE: 90469.68\telapsed: 0.02 mins\n",
      "Epoch 4: Train Loss: 0.0053\tVal Loss: 0.0039\tVal MAE: 89237.33\telapsed: 0.03 mins\n",
      "Epoch 5: Train Loss: 0.0054\tVal Loss: 0.0040\tVal MAE: 89337.09\telapsed: 0.03 mins\n",
      "Epoch 6: Train Loss: 0.0054\tVal Loss: 0.0040\tVal MAE: 89189.73\telapsed: 0.04 mins\n",
      "Epoch 7: Train Loss: 0.0055\tVal Loss: 0.0041\tVal MAE: 89448.32\telapsed: 0.05 mins\n",
      "Epoch 8: Train Loss: 0.0055\tVal Loss: 0.0041\tVal MAE: 89061.62\telapsed: 0.06 mins\n",
      "Epoch 9: Train Loss: 0.0055\tVal Loss: 0.0041\tVal MAE: 89004.81\telapsed: 0.06 mins\n",
      "Epoch 10: Train Loss: 0.0055\tVal Loss: 0.0041\tVal MAE: 88362.44\telapsed: 0.07 mins\n",
      "Epoch 11: Train Loss: 0.0055\tVal Loss: 0.0042\tVal MAE: 89442.07\telapsed: 0.08 mins\n",
      "Epoch 12: Train Loss: 0.0056\tVal Loss: 0.0043\tVal MAE: 89163.07\telapsed: 0.08 mins\n",
      "Epoch 13: Train Loss: 0.0055\tVal Loss: 0.0042\tVal MAE: 88935.50\telapsed: 0.09 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0041\tVal MAE: 88362.44\tTotal time elapsed: 0.09 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(ModelsConfig.SEED)\n",
    "pipe.train_model(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32408f5b-5582-4621-833d-f020dd06fb50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# primary_preds = primary_preds[primary_preds.sku_name.isin(test.sku_name.unique())]\n",
    "\n",
    "for i, preds in enumerate([primary_preds[target_cols].values, secondary_preds['Target'].values]):\n",
    "    meta_base = test_data.copy()\n",
    "    meta_base[pred_cols] = preds.reshape(-1, ModelsConfig.N_STEPS)\n",
    "    meta_padded = meta_base.groupby(['month','year']).apply(fossil_preproc.pad_sku_sequence, pad_value=np.nan)\n",
    "    meta_padded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    meta_primary = meta_padded.drop(columns=pred_cols)\n",
    "    meta_oof = meta_padded[pred_cols].values\n",
    "    meta_expanded = fossil_preproc.expand_primary_data(meta_primary, meta_oof, target_cols, pred_cols)\n",
    "\n",
    "    if i==0:\n",
    "        meta_data = meta_expanded.drop(columns=['preds']).copy()\n",
    "\n",
    "    meta_data[f'preds_{i}'] = meta_expanded['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d9b539a-5d35-4c14-aa9c-d05f8091feed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "meta_data['target'] = meta_data.groupby('sku_name')['target'].progress_transform(lambda x: x.fillna(x.median()))\n",
    "meta_data['target'] = meta_data.groupby(['month','year'])['target'].progress_transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982d3b7-a8bf-4dd6-ba84-d0fffcee13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data.fillna(0, inplace=True)\n",
    "\n",
    "meta_dates = sorted([(m, y) for y,m in meta_data.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9018e553-135a-4e76-9d0d-c0b129e9dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_features = [c for c in meta_data.columns if 'preds' in c]\n",
    "meta_targets = 'target'\n",
    "\n",
    "meta_data[meta_features] = x_scaler.transform(meta_data[meta_features])\n",
    "meta_data[meta_targets] = y_scaler.transform(meta_data[meta_targets].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "109dc8d8-4f62-4180-8b16-ebc25a8fb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data['time_step'] = meta_data.groupby(['sku_name','sku_coded','month','year']).cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af7e41f4-c903-4b2c-8480-8526cc95fd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_test = get_data(meta_data, test_dates, meta_features, meta_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee1273a2-13a1-4262-99da-f7f73cb8846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(features, targets), y_test = meta_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5a170c4-fef8-4633-b670-7a8e19c7954b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kMcvIW1lUAtw",
    "outputId": "63587449-77d0-4130-c358-3f069c0a9ac7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model.load_state_dict(pipe.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20b4f9a3-00f9-4782-94d8-d0d1899cd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_preds = model(torch.as_tensor(features).transpose(0,2).unsqueeze(0).to(ModelsConfig.device))\n",
    "pred_arr = meta_preds.detach().numpy().reshape(-1, ModelsConfig.N_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd70ba4b-b62e-440a-8ba4-39c8a3195678",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = meta_data[meta_data['sku_name']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d994e8f7-788b-4b63-97e7-3e104d7baa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28561/3396961515.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['Target'] = pred_arr[sub_df['sku_coded'].astype(int).values, sub_df['time_step'].values]\n",
      "/tmp/ipykernel_28561/3396961515.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['Target'] = y_scaler.inverse_transform(sub_df['Target'].values.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "sub_df['Target'] = pred_arr[sub_df['sku_coded'].astype(int).values, sub_df['time_step'].values]\n",
    "sub_df['Target'] = y_scaler.inverse_transform(sub_df['Target'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c107a1ba-79ca-4c40-9467-513d10757957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28561/4129148662.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['month'], sub_df['year'] = meta_secondary['month'], meta_secondary['year']\n"
     ]
    }
   ],
   "source": [
    "meta_secondary = fossil_preproc.prepare_secondary_data(meta_primary, meta_oof, target_cols, pred_cols)\n",
    "sub_df['month'], sub_df['year'] = meta_secondary['month'], meta_secondary['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "53ab602b-506f-4405-8d40-2bae21058d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28561/2410078269.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['target'] = y_scaler.inverse_transform(sub_df['target'].values.reshape(-1, 1));sub_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>starting_inventory</th>\n",
       "      <th>sellin</th>\n",
       "      <th>sellin_channel_1</th>\n",
       "      <th>sellin_channel_2</th>\n",
       "      <th>sellin_channel_3</th>\n",
       "      <th>sellin_channel_4</th>\n",
       "      <th>sellin_channel_5</th>\n",
       "      <th>...</th>\n",
       "      <th>leftover_inventory_9month_MM</th>\n",
       "      <th>price_6month_MM</th>\n",
       "      <th>price_9month_MM</th>\n",
       "      <th>sku_coded</th>\n",
       "      <th>target</th>\n",
       "      <th>time_step</th>\n",
       "      <th>preds_0</th>\n",
       "      <th>preds_1</th>\n",
       "      <th>Target</th>\n",
       "      <th>Item_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>69536.362080</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011278</td>\n",
       "      <td>0.015553</td>\n",
       "      <td>12447.405347</td>\n",
       "      <td>ABEAHAMASHL_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>123358.571244</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019387</td>\n",
       "      <td>0.027411</td>\n",
       "      <td>31869.806485</td>\n",
       "      <td>ABEAHAMASHL_12_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>100678.584110</td>\n",
       "      <td>2</td>\n",
       "      <td>0.015970</td>\n",
       "      <td>0.017177</td>\n",
       "      <td>32274.814645</td>\n",
       "      <td>ABEAHAMASHL_1_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>109051.794236</td>\n",
       "      <td>3</td>\n",
       "      <td>0.017231</td>\n",
       "      <td>0.019645</td>\n",
       "      <td>18126.535988</td>\n",
       "      <td>ABEAHAMASHL_2_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABEANNAONEIZZ</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>168158.0</td>\n",
       "      <td>76988.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9117.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>2241.0</td>\n",
       "      <td>61056.682074</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.012935</td>\n",
       "      <td>-6948.350256</td>\n",
       "      <td>ABEANNAONEIZZ_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>YOSHRENECARL</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>1527604.0</td>\n",
       "      <td>288705.0</td>\n",
       "      <td>199561.0</td>\n",
       "      <td>79014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>214756.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>289041.621090</td>\n",
       "      <td>3</td>\n",
       "      <td>0.044350</td>\n",
       "      <td>0.064525</td>\n",
       "      <td>189049.415290</td>\n",
       "      <td>YOSHRENECARL_2_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>283174.838744</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043466</td>\n",
       "      <td>0.072075</td>\n",
       "      <td>95887.859734</td>\n",
       "      <td>YOSHTLYNYOSHZZ_11_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>302929.766945</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046443</td>\n",
       "      <td>0.079573</td>\n",
       "      <td>41056.740146</td>\n",
       "      <td>YOSHTLYNYOSHZZ_12_2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>246794.438821</td>\n",
       "      <td>2</td>\n",
       "      <td>0.037985</td>\n",
       "      <td>0.047513</td>\n",
       "      <td>53121.115963</td>\n",
       "      <td>YOSHTLYNYOSHZZ_1_2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>156002.0</td>\n",
       "      <td>163093.0</td>\n",
       "      <td>122573.0</td>\n",
       "      <td>30390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>200574.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>222611.474320</td>\n",
       "      <td>3</td>\n",
       "      <td>0.034341</td>\n",
       "      <td>0.046823</td>\n",
       "      <td>98900.548081</td>\n",
       "      <td>YOSHTLYNYOSHZZ_2_2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2564 rows  85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sku_name  month    year  starting_inventory    sellin  \\\n",
       "0        ABEAHAMASHL   11.0  2021.0            410265.0   18234.0   \n",
       "1        ABEAHAMASHL   12.0  2021.0            410265.0   18234.0   \n",
       "2        ABEAHAMASHL    1.0  2022.0            410265.0   18234.0   \n",
       "3        ABEAHAMASHL    2.0  2022.0            410265.0   18234.0   \n",
       "4      ABEANNAONEIZZ   11.0  2021.0            168158.0   76988.0   \n",
       "...              ...    ...     ...                 ...       ...   \n",
       "2559    YOSHRENECARL    2.0  2022.0           1527604.0  288705.0   \n",
       "2560  YOSHTLYNYOSHZZ   11.0  2021.0            156002.0  163093.0   \n",
       "2561  YOSHTLYNYOSHZZ   12.0  2021.0            156002.0  163093.0   \n",
       "2562  YOSHTLYNYOSHZZ    1.0  2022.0            156002.0  163093.0   \n",
       "2563  YOSHTLYNYOSHZZ    2.0  2022.0            156002.0  163093.0   \n",
       "\n",
       "      sellin_channel_1  sellin_channel_2  sellin_channel_3  sellin_channel_4  \\\n",
       "0                  0.0               0.0               0.0            1013.0   \n",
       "1                  0.0               0.0               0.0            1013.0   \n",
       "2                  0.0               0.0               0.0            1013.0   \n",
       "3                  0.0               0.0               0.0            1013.0   \n",
       "4                  0.0           10130.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "2559          199561.0           79014.0               0.0               0.0   \n",
       "2560          122573.0           30390.0               0.0               0.0   \n",
       "2561          122573.0           30390.0               0.0               0.0   \n",
       "2562          122573.0           30390.0               0.0               0.0   \n",
       "2563          122573.0           30390.0               0.0               0.0   \n",
       "\n",
       "      sellin_channel_5  ...  leftover_inventory_9month_MM  price_6month_MM  \\\n",
       "0                  0.0  ...                      -16208.0            149.0   \n",
       "1                  0.0  ...                      -16208.0            149.0   \n",
       "2                  0.0  ...                      -16208.0            149.0   \n",
       "3                  0.0  ...                      -16208.0            149.0   \n",
       "4                  0.0  ...                       -9117.0            129.0   \n",
       "...                ...  ...                           ...              ...   \n",
       "2559               0.0  ...                      214756.0            129.0   \n",
       "2560               0.0  ...                      200574.0            149.0   \n",
       "2561               0.0  ...                      200574.0            149.0   \n",
       "2562               0.0  ...                      200574.0            149.0   \n",
       "2563               0.0  ...                      200574.0            149.0   \n",
       "\n",
       "      price_9month_MM  sku_coded         target  time_step   preds_0  \\\n",
       "0               149.0       74.0   69536.362080          0  0.011278   \n",
       "1               149.0       74.0  123358.571244          1  0.019387   \n",
       "2               149.0       74.0  100678.584110          2  0.015970   \n",
       "3               149.0       74.0  109051.794236          3  0.017231   \n",
       "4               129.0     2241.0   61056.682074          0  0.010000   \n",
       "...               ...        ...            ...        ...       ...   \n",
       "2559            129.0      248.0  289041.621090          3  0.044350   \n",
       "2560            149.0     1852.0  283174.838744          0  0.043466   \n",
       "2561            149.0     1852.0  302929.766945          1  0.046443   \n",
       "2562            149.0     1852.0  246794.438821          2  0.037985   \n",
       "2563            149.0     1852.0  222611.474320          3  0.034341   \n",
       "\n",
       "       preds_1         Target                 Item_ID  \n",
       "0     0.015553   12447.405347     ABEAHAMASHL_11_2021  \n",
       "1     0.027411   31869.806485     ABEAHAMASHL_12_2021  \n",
       "2     0.017177   32274.814645      ABEAHAMASHL_1_2022  \n",
       "3     0.019645   18126.535988      ABEAHAMASHL_2_2022  \n",
       "4     0.012935   -6948.350256   ABEANNAONEIZZ_11_2021  \n",
       "...        ...            ...                     ...  \n",
       "2559  0.064525  189049.415290     YOSHRENECARL_2_2022  \n",
       "2560  0.072075   95887.859734  YOSHTLYNYOSHZZ_11_2021  \n",
       "2561  0.079573   41056.740146  YOSHTLYNYOSHZZ_12_2021  \n",
       "2562  0.047513   53121.115963   YOSHTLYNYOSHZZ_1_2022  \n",
       "2563  0.046823   98900.548081   YOSHTLYNYOSHZZ_2_2022  \n",
       "\n",
       "[2564 rows x 85 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df['target'] = y_scaler.inverse_transform(sub_df['target'].values.reshape(-1, 1));sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2772c327-9043-4e10-81cf-6b6ca4c382b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>starting_inventory</th>\n",
       "      <th>sellin</th>\n",
       "      <th>sellin_channel_1</th>\n",
       "      <th>sellin_channel_2</th>\n",
       "      <th>sellin_channel_3</th>\n",
       "      <th>sellin_channel_4</th>\n",
       "      <th>sellin_channel_5</th>\n",
       "      <th>...</th>\n",
       "      <th>onhand_inventory_6month_MM</th>\n",
       "      <th>onhand_inventory_9month_MM</th>\n",
       "      <th>leftover_inventory_6month_MM</th>\n",
       "      <th>leftover_inventory_9month_MM</th>\n",
       "      <th>price_6month_MM</th>\n",
       "      <th>price_9month_MM</th>\n",
       "      <th>sku_coded</th>\n",
       "      <th>target</th>\n",
       "      <th>preds</th>\n",
       "      <th>time_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229444.5</td>\n",
       "      <td>291744.0</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>69536.362080</td>\n",
       "      <td>43735.480111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229444.5</td>\n",
       "      <td>291744.0</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>123358.571244</td>\n",
       "      <td>77079.006822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229444.5</td>\n",
       "      <td>291744.0</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>100678.584110</td>\n",
       "      <td>48301.075780</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>410265.0</td>\n",
       "      <td>18234.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229444.5</td>\n",
       "      <td>291744.0</td>\n",
       "      <td>-13675.5</td>\n",
       "      <td>-16208.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>109051.794236</td>\n",
       "      <td>55242.775411</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABEANNAONEIZZ</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>168158.0</td>\n",
       "      <td>76988.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75468.5</td>\n",
       "      <td>176262.0</td>\n",
       "      <td>-8104.0</td>\n",
       "      <td>-9117.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>2241.0</td>\n",
       "      <td>61056.682074</td>\n",
       "      <td>36373.702684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15363</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3839.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15364</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15365</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15366</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15367</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15368 rows  82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sku_name  month    year  starting_inventory   sellin  \\\n",
       "0        ABEAHAMASHL   10.0  2021.0            410265.0  18234.0   \n",
       "1        ABEAHAMASHL   10.0  2021.0            410265.0  18234.0   \n",
       "2        ABEAHAMASHL   10.0  2021.0            410265.0  18234.0   \n",
       "3        ABEAHAMASHL   10.0  2021.0            410265.0  18234.0   \n",
       "4      ABEANNAONEIZZ   10.0  2021.0            168158.0  76988.0   \n",
       "...              ...    ...     ...                 ...      ...   \n",
       "15363            NaN   10.0  2021.0                 NaN      NaN   \n",
       "15364            NaN   10.0  2021.0                 NaN      NaN   \n",
       "15365            NaN   10.0  2021.0                 NaN      NaN   \n",
       "15366            NaN   10.0  2021.0                 NaN      NaN   \n",
       "15367            NaN   10.0  2021.0                 NaN      NaN   \n",
       "\n",
       "       sellin_channel_1  sellin_channel_2  sellin_channel_3  sellin_channel_4  \\\n",
       "0                   0.0               0.0               0.0            1013.0   \n",
       "1                   0.0               0.0               0.0            1013.0   \n",
       "2                   0.0               0.0               0.0            1013.0   \n",
       "3                   0.0               0.0               0.0            1013.0   \n",
       "4                   0.0           10130.0               0.0               0.0   \n",
       "...                 ...               ...               ...               ...   \n",
       "15363               NaN               NaN               NaN               NaN   \n",
       "15364               NaN               NaN               NaN               NaN   \n",
       "15365               NaN               NaN               NaN               NaN   \n",
       "15366               NaN               NaN               NaN               NaN   \n",
       "15367               NaN               NaN               NaN               NaN   \n",
       "\n",
       "       sellin_channel_5  ...  onhand_inventory_6month_MM  \\\n",
       "0                   0.0  ...                    229444.5   \n",
       "1                   0.0  ...                    229444.5   \n",
       "2                   0.0  ...                    229444.5   \n",
       "3                   0.0  ...                    229444.5   \n",
       "4                   0.0  ...                     75468.5   \n",
       "...                 ...  ...                         ...   \n",
       "15363               NaN  ...                         NaN   \n",
       "15364               NaN  ...                         NaN   \n",
       "15365               NaN  ...                         NaN   \n",
       "15366               NaN  ...                         NaN   \n",
       "15367               NaN  ...                         NaN   \n",
       "\n",
       "       onhand_inventory_9month_MM  leftover_inventory_6month_MM  \\\n",
       "0                        291744.0                      -13675.5   \n",
       "1                        291744.0                      -13675.5   \n",
       "2                        291744.0                      -13675.5   \n",
       "3                        291744.0                      -13675.5   \n",
       "4                        176262.0                       -8104.0   \n",
       "...                           ...                           ...   \n",
       "15363                         NaN                           NaN   \n",
       "15364                         NaN                           NaN   \n",
       "15365                         NaN                           NaN   \n",
       "15366                         NaN                           NaN   \n",
       "15367                         NaN                           NaN   \n",
       "\n",
       "       leftover_inventory_9month_MM  price_6month_MM  price_9month_MM  \\\n",
       "0                          -16208.0            149.0            149.0   \n",
       "1                          -16208.0            149.0            149.0   \n",
       "2                          -16208.0            149.0            149.0   \n",
       "3                          -16208.0            149.0            149.0   \n",
       "4                           -9117.0            129.0            129.0   \n",
       "...                             ...              ...              ...   \n",
       "15363                           NaN              NaN              NaN   \n",
       "15364                           NaN              NaN              NaN   \n",
       "15365                           NaN              NaN              NaN   \n",
       "15366                           NaN              NaN              NaN   \n",
       "15367                           NaN              NaN              NaN   \n",
       "\n",
       "       sku_coded         target         preds  time_step  \n",
       "0           74.0   69536.362080  43735.480111          0  \n",
       "1           74.0  123358.571244  77079.006822          1  \n",
       "2           74.0  100678.584110  48301.075780          2  \n",
       "3           74.0  109051.794236  55242.775411          3  \n",
       "4         2241.0   61056.682074  36373.702684          0  \n",
       "...          ...            ...           ...        ...  \n",
       "15363     3839.0            NaN           NaN      12799  \n",
       "15364     3840.0            NaN           NaN      12800  \n",
       "15365     3840.0            NaN           NaN      12801  \n",
       "15366     3840.0            NaN           NaN      12802  \n",
       "15367     3840.0            NaN           NaN      12803  \n",
       "\n",
       "[15368 rows x 82 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_secondary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
