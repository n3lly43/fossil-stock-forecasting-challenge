{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6cdpG2ZoV1ig"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './'\n",
    "\n",
    "# -----------------------------------\n",
    "# Test and train folder\n",
    "# Data folder: drive > My Drive > Zindi-UmojaHackathon > data \n",
    "# -----------------------------------\n",
    "TRAIN_DIR = f'{DATA_DIR}/train'\n",
    "TEST_DIR = f'{DATA_DIR}/test'\n",
    "\n",
    "PLOTS_DIR = f'{DATA_DIR}/plots'\n",
    "\n",
    "OUTPUT_DIR = f'{DATA_DIR}/output'\n",
    "MODEL_CHECKPOINT_DIR = f'{DATA_DIR}/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQ9U3_fQ_7X9",
    "tags": []
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SKh73z010CFl"
   },
   "outputs": [],
   "source": [
    "class LabelEncoder:\n",
    "    def __init__(self, items) -> None:\n",
    "        self.items = items\n",
    "        self.size = int(len(self.items))\n",
    "        self.items = {item:ix for ix,item in enumerate(self.items)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self.encode(input)\n",
    "\n",
    "    def encode(self, item):\n",
    "        if item in self.items.keys():\n",
    "            return int(self.items[item])  \n",
    "\n",
    "        # self.OOV    \n",
    "        return self.size\n",
    "\n",
    "    def decode(self, ix):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Is0TQzbs7SJG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def pad_sequence(sequence: pd.DataFrame, encoder: LabelEncoder, pad_value: int=0):\n",
    "    pad_list = sorted(set(range(0, len(encoder) + 1)).difference(set(sequence['sku_coded'].values)))\n",
    "    pad_df = pd.DataFrame(pad_value, index=range(len(pad_list)), columns=sequence.columns)\n",
    "    pad_df['sku_coded'] = pad_list\n",
    "    pad_df['month'] = sequence['month'].iloc[0].astype(int)\n",
    "    pad_df['year'] = sequence['year'].iloc[0].astype(int)\n",
    "    \n",
    "    return pad_df\n",
    "\n",
    "def pad_sku_sequence(features: pd.DataFrame, encoder:LabelEncoder, pad_value: int=0):\n",
    "    features = features.groupby('sku_name').mean().reset_index()\n",
    "\n",
    "    return features.append(pad_sequence(features, encoder, pad_value))\n",
    "\n",
    "def pad_targets(targets: pd.DataFrame, target_series, pad_value: int=np.nan):\n",
    "    missing = set(target_series).difference(set(targets[['month', 'year']].apply(tuple, axis=1).values))\n",
    "    \n",
    "    if missing:\n",
    "        pad_df = pd.DataFrame(pad_value, index=range(len(missing)), columns=targets.columns)\n",
    "\n",
    "        pad_df['sku_name'] = targets['sku_name'].values[0]\n",
    "        # pad_df['sku_coded'] = targets['sku_coded'].values[0]\n",
    "        pad_df['month'] , pad_df['year'] = zip(*missing)\n",
    "        targets = targets.append(pad_df)\n",
    "\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-Qwh3VDzo5DR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_sequence_index(train: pd.DataFrame, n_steps: int, lookback:int):\n",
    "    x_train_idx, y_train_idx = [], []\n",
    "    x_val_idx, y_val_idx = [], []\n",
    "    dates = sorted([(m, y) for y,m in train.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "    size = train.groupby(['year', 'month']).ngroups//LOOKBACK\n",
    "    \n",
    "    for i in tqdm(range(size), desc='retrieving walk forward indices'):\n",
    "        start = i*lookback\n",
    "        train_ix = (i+1)*lookback\n",
    "        val_ix = (i+2)*lookback\n",
    "        \n",
    "\n",
    "        x_train_date = dates[start:train_ix]\n",
    "        y_train_date = dates[train_ix: train_ix+n_steps]\n",
    "\n",
    "        x_val_date = dates[train_ix:val_ix]\n",
    "        y_val_date = dates[val_ix: val_ix+n_steps]\n",
    "        \n",
    "        if val_ix+n_steps>train.groupby(['year', 'month']).ngroups:\n",
    "            break\n",
    "            \n",
    "        x_train_idx.append(train[train[['month', 'year']].apply(tuple, axis=1).isin(x_train_date)].index)\n",
    "        y_train_idx.append(train[train[['month', 'year']].apply(tuple, axis=1).isin(y_train_date)].index)\n",
    "\n",
    "        x_val_idx.append(train[train[['month', 'year']].apply(tuple, axis=1).isin(x_val_date)].index)\n",
    "        y_val_idx.append(train[train[['month', 'year']].apply(tuple, axis=1).isin(y_val_date)].index)\n",
    "        \n",
    "    return (x_train_idx, y_train_idx), (x_val_idx, y_val_idx)\n",
    " \n",
    "def _pad_target(x,y, encoder: LabelEncoder):\n",
    "    final_date = sorted([(m,yr) for yr,m in x[['year', 'month']].apply(tuple, axis=1)],\\\n",
    "                        key=lambda g: (g[1], g[0]), reverse=True)[0]\n",
    "    \n",
    "    target_dates = [(final_date[0]+n, final_date[1])\\\n",
    "                    if final_date[0]<12 else (final_date[0]-(12-1),final_date[1]+1)\\\n",
    "                    for n in range(1,N_STEPS+1)]\n",
    "    \n",
    "    y = y.groupby(['year', 'month']).apply(pad_sku_sequence, \n",
    "                                                    encoder=encoder, \n",
    "                                                    pad_value=np.nan).reset_index(drop=True)\n",
    "    return y\n",
    "\n",
    "def prepare_sequence(x_train: pd.DataFrame,\n",
    "                     x_val: pd.DataFrame,\n",
    "                     y_train: tuple,\n",
    "                     y_val: tuple, \n",
    "                     encoder: LabelEncoder, \n",
    "                     target_cols: list,\n",
    "                     step: int,\n",
    "                     x_scaler,\n",
    "                     y_scale):\n",
    " \n",
    "    y_test = y_val.copy()\n",
    "    x_train['sku_coded'], x_val['sku_coded'] = x_train['sku_name'].apply(encoder), x_val['sku_name'].apply(encoder) \n",
    "    y_train['sku_coded'], y_val['sku_coded'] = y_train['sku_name'].apply(encoder), y_val['sku_name'].apply(encoder) \n",
    "\n",
    "    # print(y_train, y_val)\n",
    "    y_train = _pad_target(x_train, y_train, encoder)\n",
    "    y_val = _pad_target(x_val, y_val, encoder)\n",
    "    # print(y_train, y_val)\n",
    "    x_train = x_train.groupby(['year', 'month']).apply(pad_sku_sequence, encoder=encoder).reset_index(drop=True)\n",
    "    x_val = x_val.groupby(['year', 'month']).apply(pad_sku_sequence, encoder=encoder).reset_index(drop=True)\n",
    "    \n",
    "    y_test['sku_coded'] = y_test.sku_name.apply(encoder)\n",
    "    y_test = y_test.sort_values(by=['sku_name','year','month'])[['sku_coded']+target_cols]\n",
    "    \n",
    "    y_test['idx'] = SEED\n",
    "    y_test = y_test.groupby('sku_name').apply(lambda x: func(x, x.name)).reset_index(drop=True)\n",
    "\n",
    "    y_train = y_train.groupby(['sku_coded','year','month']).mean().sort_values(by=['sku_coded','year','month']).reset_index()\n",
    "    y_val = y_val.groupby(['sku_coded','year','month']).mean().sort_values(by=['sku_coded','year','month']).reset_index()\n",
    "    \n",
    "    return (x_train.sort_values(by=['year','month','sku_coded']).reset_index(drop=True), y_train[target_cols]), (x_val.sort_values(by=['year','month','sku_coded']).reset_index(drop=True), y_val[target_cols]), y_test\n",
    "\n",
    "  \n",
    "def func(gp, name):\n",
    "    dates = sorted([(m,y) for y,m in gp.groupby(['year','month']).groups.keys()], key= lambda g: (g[1],g[0]))\n",
    "    gp = gp.sort_values(['year','month'])\n",
    "\n",
    "    if len(gp)<N_STEPS:\n",
    "        idx = [dates.index((date[1],date[0])) for date in gp[['year','month']].apply(tuple, axis=1)]\n",
    "        # gp['rule'] = 0\n",
    "        gp['idx'] = idx\n",
    "\n",
    "        return gp\n",
    "\n",
    "    if len(gp)==N_STEPS:\n",
    "        # gp['rule'] = 1\n",
    "        # gp['n_repeat'] = int(len(gp)/N_STEPS)\n",
    "        # print(gp)\n",
    "        gp['idx'] = list(range(N_STEPS))\n",
    "\n",
    "        return gp\n",
    "\n",
    "    for date in dates:\n",
    "        idx = dates.index(date)\n",
    "        # gp.at[(gp.month==date[0])&(gp.year==date[1]), 'rule'] = 2\n",
    "        gp.loc[(gp.month==date[0])&(gp.year==date[1]), 'idx'] = idx\n",
    "        # gp.at[(gp.month==date[0])&(gp.year==date[1]), 'n_repeat'] = len(gp[(gp.month==date[0])&(gp.year==date[1])])\n",
    "    \n",
    "    return gp\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SfWyEKv7oxpV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def impute_missing(data, cols):\n",
    "    temp = pd.DataFrame()\n",
    "    for d in tqdm(data, desc='imputing missing values'):\n",
    "        temp = pd.concat([temp, d[0][1]])\n",
    "        d[0][1][cols] = temp.groupby('sku_name')[cols].transform(lambda x: x.fillna(x.median())).iloc[-len(d[0][1]):]\n",
    "        d[0][1][cols] = temp.groupby(['year', 'month'])[cols].transform(lambda x: x.fillna(x.median())).iloc[-len(d[0][1]):]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x, y, y_test = [],[],[]\n",
    "    # print(len(batch[0][0]))\n",
    "    for xi, yi, yt in batch:\n",
    "        # print(xi.shape)\n",
    "        x.append(xi)\n",
    "        y.append(yi)\n",
    "        y_test.append(yt)\n",
    "\n",
    "    return torch.stack(x), torch.stack(y), y_test\n",
    "\n",
    "def prepare_data_scaler(x_scaler, y_scaler, target_scaler, train_data, val_data):\n",
    "    x_df = pd.concat([d[0][0] for d in train_data+val_data], axis=0)\n",
    "    y_df = pd.concat([d[0][1] for d in train_data+val_data], axis=0)\n",
    "      \n",
    "    x_scaler.fit(x_df.drop(columns=['sku_name', 'sku_coded']))\n",
    "    y_scaler.fit(y_df.drop(columns=['sku_name', 'sku_coded']))\n",
    "    target_scaler.fit(x_df['sellin'].values.reshape(-1,1))\n",
    "\n",
    "    return x_scaler, y_scaler, target_scaler\n",
    "\n",
    "def normalize_data(train_data: list, val_data:list, x_scaler, y_scaler):\n",
    "    train_list, val_list = [], []\n",
    "\n",
    "    for t,v in tqdm(zip(train_data,val_data), desc='normalizing data', total=len(train_data)): \n",
    "        xt, yt = t[0][0].copy(), t[0][1].copy()\n",
    "        xv, yv = v[0][0].copy(), v[0][1].copy()\n",
    "        x_cols = [c for c in xt.columns if c not in ['sku_name','sku_coded']]\n",
    "        y_cols = [c for c in yt.columns if c not in ['sku_name','sku_coded']]\n",
    "\n",
    "        xt[x_cols], yt[y_cols] = x_scaler.transform(xt[x_cols]), y_scaler.transform(yt[y_cols])\n",
    "        xv[x_cols], yv[y_cols] = x_scaler.transform(xv[x_cols]), y_scaler.transform(yv[y_cols])\n",
    "\n",
    "        train_list.append(((xt,yt),t[1]))\n",
    "        val_list.append(((xv,yv),v[1]))\n",
    "\n",
    "    return train_list, val_list\n",
    "\n",
    "def get_data(train: pd.DataFrame, train_idx, val_idx, encoder: LabelEncoder, target_cols: list):\n",
    "    train_list, val_list = [], []\n",
    "\n",
    "    for ix in tqdm(range(len(train_idx[0])), desc='preparing sequences'):\n",
    "        x_train, x_val = train.iloc[train_idx[0][ix]].reset_index(drop=True), train.iloc[val_idx[0][ix]].reset_index(drop=True)\n",
    "        y_train, y_val = train.iloc[train_idx[1][ix]].reset_index(drop=True), train.iloc[val_idx[1][ix]].reset_index(drop=True)\n",
    "\n",
    "        train_data, val_data, y_test = prepare_sequence(x_train, x_val, y_train, y_val, encoder, target_cols, ix, x_scaler, y_scaler)\n",
    "\n",
    "        train_list.append((train_data, y_test))\n",
    "        val_list.append((val_data, y_test))\n",
    "\n",
    "    return train_list, val_list, [c for c in train_data[0].columns if c not in ['sku_name','year','month','sku_coded']]\n",
    "\n",
    "\n",
    "# class FossilDataset(Dataset):\n",
    "#     def __init__(self, \n",
    "#                  features,\n",
    "#                  targets,\n",
    "#                  y_test,\n",
    "#                  encoder:LabelEncoder):\n",
    "      \n",
    "#         self.features = features\n",
    "#         self.targets = targets\n",
    "#         self.y_test = y_test\n",
    "#         self.encoder = encoder\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return 1#len(self.features)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # (self.features, self.targets), y_test = self.data[idx]\n",
    "        \n",
    "#         return torch.as_tensor(self.features), torch.as_tensor(self.targets).squeeze(), self.y_test\n",
    "\n",
    "\n",
    "class FossilDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data: list, \n",
    "                 encoder:LabelEncoder,\n",
    "                 training: bool=True):\n",
    "      \n",
    "        self.data = data\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (self.features, self.targets), y_test = self.data[idx]\n",
    "        \n",
    "        feat_group = self.features.sort_values(['sku_name','year','month']).groupby(['month','year'])\n",
    "\n",
    "        seq = np.array([v.drop(columns=['sku_name','year','month','sku_coded']) for k,v in feat_group]) #(timesteps, products, features)\n",
    "        seq = np.transpose(seq, (2,0,1)) #(features, timesteps, products)\n",
    "        \n",
    "        targets = self.targets.sort_values(['sku_name','year','month'])\n",
    "        targets = targets.drop(columns=['month', 'year', 'sku_name','sku_coded']).values #(timesteps*products, target)\n",
    "        # print(torch.as_tensor(targets), torch.as_tensor(targets.reshape(-1)).shape)\n",
    "        return torch.as_tensor(seq), torch.as_tensor(targets.reshape(-1)), y_test\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ru9o_gIPv8IR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FossilEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_features: int, \n",
    "                 n_extracts: int,\n",
    "                 n_products: int,\n",
    "                 n_steps: int,\n",
    "                 n_targets: int):\n",
    "        super(FossilEncoder, self).__init__()\n",
    "\n",
    "        self.monthly_conv2d = nn.Conv2d(n_features, n_extracts, (1,1))\n",
    "        self.periodic_conv2d = nn.Conv2d(n_extracts, n_extracts, (1, n_products))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.fc1 = nn.Linear(n_extracts*LOOKBACK, n_steps*12)\n",
    "        self.fc2 = nn.Linear(n_steps*12, n_steps*n_products*n_targets)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.monthly_conv2d(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "\n",
    "        x = self.periodic_conv2d(x)\n",
    "        x = self.relu(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        \n",
    "        out = self.fc1(self.dropout(x))\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return self.fc2(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FossilDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_extracts, \n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 n_steps,\n",
    "                 n_products,\n",
    "                 n_features,\n",
    "                 num_layers=2,\n",
    "                 bidirectional=True):\n",
    "        super(FossilDecoder, self).__init__()\n",
    "        \n",
    "        self.monthly_conv2d = nn.Conv2d(n_features, n_extracts, (1,1))\n",
    "        self.periodic_conv2d = nn.Conv2d(n_extracts, n_extracts, (1, n_products))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.rnn = nn.GRU(n_extracts, hidden_size, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size*2, output_size) if bidirectional else nn.Linear(hidden_size, output_size)\n",
    "        self.fc2 = nn.Linear(output_size, n_steps*n_products)       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        inputs = []\n",
    "        for t in range(x.size(2)):\n",
    "            xi = self.monthly_conv2d(x[:, :, t, :].unsqueeze(2))\n",
    "            xi = self.relu(xi)\n",
    "\n",
    "\n",
    "            xi = self.periodic_conv2d(xi)\n",
    "            xi = self.relu(xi).squeeze(-1)\n",
    "            inputs.append(xi)\n",
    "            \n",
    "        x = torch.cat(inputs, 2).transpose(1,2)\n",
    "        \n",
    "        x,_ = self.rnn(x)\n",
    "        out = self.relu(self.fc1(x))\n",
    "        \n",
    "        return torch.mean(self.fc2(out),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hzJfkgWu2HI3"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "        \n",
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "class ModelTrainer:\n",
    "    def train_model(self, early_stop=3, verbose=10, save_path=None):\n",
    "        \n",
    "        stop_eps = 0 \n",
    "        best_epoch = None\n",
    "        metrics = {'MAE':[]}\n",
    "        start_time = time()\n",
    "        accum_loss = torch.tensor(float('inf')) if not self.resume_train else torch.tensor(float(re.findall(\"\\d+\\.\\d+\", self.model_weights)[0]))\n",
    "        try:\n",
    "            for epoch in range(1, ModelsConfig.EPOCHS + 1):\n",
    "                train_loss = self.train_step(epoch, verbose)             \n",
    "                test_loss, w8, mae = self.val_step(epoch)\n",
    "\n",
    "                if test_loss < accum_loss:\n",
    "                    best_epoch = epoch\n",
    "                    accum_loss = test_loss\n",
    "\n",
    "                    self.model_checkpoints.append((w8, test_loss, mae))\n",
    "                    stop_eps = 0\n",
    "                else:\n",
    "                    stop_eps += 1\n",
    "                if epoch % verbose ==0:\n",
    "                    print('Epoch {}: Train Loss: {:.4f}\\tVal Loss: {:.4f}\\tVal MAE: {:.2f}\\telapsed: {:.2f} mins'.format(\n",
    "                    epoch, torch.mean(train_loss), test_loss, mae, (time()-start_time)/60))\n",
    "                \n",
    "\n",
    "                if stop_eps >= early_stop:\n",
    "                    self.save_model_checkpoint(save_path)\n",
    "                    print('\\n')\n",
    "                    print('Early stopping: Best Epoch: {}\\tVal loss: {:.4f}\\tVal MAE: {:.2f}\\tTotal time elapsed: {:.2f} mins'.format(\n",
    "                        best_epoch, accum_loss, mae, (time()-start_time)/60))\n",
    "                    print('-'*50)\n",
    "                    print('\\n')\n",
    "                    break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            self.save_model_checkpoint(save_path) if self.model_checkpoints else print(\"first epoch not completed, model checkpoint will not be saved\")\n",
    "            \n",
    "            if stop_eps<early_stop:\n",
    "                print(f'Training stopped, early stop not reached. Best Iteration: {best_epoch} Val loss: {accum_loss} Val MAE: {mae}')\n",
    "\n",
    "    def save_model_checkpoint(self, save_path):\n",
    "        best_model, accum_loss, metrics = self.model_checkpoints[-1]\n",
    "\n",
    "        if save_path is not None:\n",
    "            torch.save(best_model, Path(save_path).joinpath(f'loss_{accum_loss}_MAE_{metrics}_fossil.pth'))\n",
    "\n",
    "    def load_model_checkpoint(self, save_path):\n",
    "        self.model.load_state_dict(torch.load(Path(save_path)))            \n",
    "\n",
    "class Pipeline(ModelTrainer):\n",
    "    def __init__(self, train_dataloader, test_dataloader, model, target_scaler, resume_from_checkpoint=False, model_weights=None):\n",
    "        \n",
    "        self.model = model.to(ModelsConfig.device) \n",
    "        self.resume_train = resume_from_checkpoint\n",
    "        self.model_weights = model_weights\n",
    "        self.target_scaler = target_scaler\n",
    "\n",
    "        if self.resume_train:\n",
    "            assert model_weights is not None, \"specify model weights save location\"\n",
    "            self.load_model_checkpoint(self.model_weights)\n",
    "\n",
    "        self.train_loader = train_dataloader\n",
    "        self.test_loader = test_dataloader\n",
    "\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=ModelsConfig.learning_rate)\n",
    "        \n",
    "        self.model_checkpoints = []\n",
    "\n",
    "    def train_step(self, epoch, verbose):\n",
    "        self.model.train()\n",
    "        \n",
    "        train_loss=[]\n",
    "        start_time = time()\n",
    "        data_len = len(self.train_loader.dataset)\n",
    "\n",
    "        for batch_idx, (inputs,labels, y_) in enumerate(self.train_loader):\n",
    "            inputs = inputs.to(ModelsConfig.device)\n",
    "            labels = labels.to(ModelsConfig.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            preds = self.model(inputs)\n",
    "            \n",
    "            # na_idx = list(labels.isnan().nonzero().transpose(0,1))\n",
    "            # labels[na_idx] = preds[na_idx]\n",
    "\n",
    "            loss = self.criterion(preds,labels)\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        return loss\n",
    "            # if batch_idx % verbose == 0 or batch_idx == len(self.train_loader)-1:\n",
    "                # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\telapsed: {:.2f} mins'.format(\n",
    "                #     epoch, batch_idx * len(inputs), data_len, 100. * batch_idx / len(self.train_loader), loss.item(), (time()-start_time)/60))\n",
    "                \n",
    "    def val_step(self, epoch):\n",
    "        # print('\\nevaluatingâ€¦')\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        mae = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels, y_) in enumerate(self.test_loader):\n",
    "                if batch_idx==(len(self.test_loader)-1):\n",
    "                    inputs = inputs.to(ModelsConfig.device)\n",
    "                    labels = labels.to(ModelsConfig.device)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                    preds = self.model(inputs)\n",
    "                    # na_idx = list(labels.isnan().nonzero().transpose(0,1))\n",
    "                    # labels[na_idx] = preds[na_idx]\n",
    "\n",
    "                    loss = self.criterion(preds,labels)\n",
    "\n",
    "                    test_loss += loss.item()# / len(self.test_loader)\n",
    "                    mae += self.eval_step(y_, preds)# / len(self.test_loader)\n",
    "\n",
    "        # print('Test set: Average loss: {:.4f} | MAE: {:.4f}\\n'.format(test_loss, mae))\n",
    "\n",
    "        return round(test_loss, 4), self.model.state_dict(), mae\n",
    "\n",
    "    def eval_step(self, labels: list, preds):\n",
    "        mae = 0\n",
    "        preds = preds.cpu().detach().numpy().reshape(len(labels), -1)\n",
    "        \n",
    "        # pandarallel.initialize(use_memory_fs=False)\n",
    "\n",
    "        for i,label in enumerate(labels):\n",
    "            pred_arr = preds[i].reshape(-1, N_STEPS, 6)\n",
    "            label['y_true'] = label['sellin'].copy()\n",
    "            label['y_pred'] = pred_arr[label.sku_coded.values, label.idx.values, 0]\n",
    "            \n",
    "            y_true = label['y_true'].values.reshape(1, -1)\n",
    "            y_pred = self.target_scaler.inverse_transform(label['y_pred'].values.reshape(1, -1))\n",
    "            label['y_pred'] = y_pred.reshape(-1,)\n",
    "            mae += np.absolute(np.subtract(y_true, y_pred)).mean()/len(labels)\n",
    "\n",
    "        return mae\n",
    "\n",
    "    def func(self, gp, name, preds):\n",
    "        dates = sorted([(m,y) for y,m in gp.groupby(['year','month']).groups.keys()], key= lambda g: (g[1],g[0]))\n",
    "        gp = gp.sort_values(['year','month'])\n",
    "\n",
    "        if len(gp)<N_STEPS:\n",
    "            idx = [dates.index((date[1],date[0])) for date in gp[['year','month']].apply(tuple, axis=1)]\n",
    "            gp['sellin'] = preds[name, :len(gp)]\n",
    "\n",
    "            return gp\n",
    "\n",
    "        if len(gp)%N_STEPS==0:\n",
    "            gp['sellin'] = np.tile(preds[name, :], int(len(gp)/N_STEPS))\n",
    "\n",
    "            return gp\n",
    "\n",
    "        for date in dates:\n",
    "            idx = dates.index(date)\n",
    "            gp.loc[(gp.month==date[0])&(gp.year==date[1]), 'sellin'] = np.tile(preds[name, idx], len(gp[(gp.month==date[0])&(gp.year==date[1])]))\n",
    "        \n",
    "        return gp\n",
    "\n",
    "    def make_oof_preds(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels, y_) in enumerate(self.test_loader):\n",
    "                if batch_idx==(len(self.test_loader)-1):\n",
    "                    inputs = inputs.to(ModelsConfig.device)\n",
    "                    labels = labels.to(ModelsConfig.device)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_8SvqhKw628J"
   },
   "outputs": [],
   "source": [
    "def prepare_test_data(train: pd.DataFrame, test_dates:list, x_scaler=None):\n",
    "    test_context = train[train[['month','year']].apply(tuple, axis=1).isin(test_dates)].reset_index(drop=True)\n",
    "    scale_cols = [c for c in test_context.columns if c not in ['sku_name','sku_coded']]\n",
    "\n",
    "    test_context['sku_coded'] = test_context['sku_name'].apply(sku_encoder)\n",
    "    test_context = test_context.groupby(['year', 'month']).apply(pad_sku_sequence, encoder=sku_encoder).reset_index(drop=True)\n",
    "    test_context[scale_cols] = x_scaler.transform(test_context[scale_cols])\n",
    "\n",
    "    test_gp = test_context.sort_values(['sku_name','year','month']).groupby(['month','year'])\n",
    "    test_seq = np.array([v.drop(columns=['sku_name','year','month','sku_coded']) for k,v in test_gp])\n",
    "\n",
    "    return np.transpose(test_seq, (2,0,1))\n",
    "\n",
    "def merge_func(gp, name, pred_array):\n",
    "    gp = gp.sort_values(['year','month'])\n",
    "    gp['sellin'] = np.tile(pred_array[name, :], int(len(gp)/N_STEPS))\n",
    "    \n",
    "    return gp\n",
    "\n",
    "def make_predictions(seq: np.array, model: FossilEncoder, test_df: pd.DataFrame):\n",
    "    preds = model(torch.as_tensor(seq).unsqueeze(0).to(ModelsConfig.device))\n",
    "    pred_arr = preds.detach().numpy().reshape(-1, N_STEPS)\n",
    "    test_df['sellin'] = pred_arr[test_df.sku_coded.values, test_df.idx.values]\n",
    "    \n",
    "    return test_df.copy()\n",
    "\n",
    "def make_submission(predictions: pd.DataFrame, dates: list, target_scaler=None):\n",
    "    predictions['Target'] = predictions['sellin']\n",
    "    predictions['Item_ID'] = predictions['sku_name'].astype(str)+'_'+predictions['month'].astype(str)+'_'+predictions['year'].astype(str)\n",
    "    \n",
    "    if target_scaler is not None:\n",
    "        predictions['Target'] = target_scaler.inverse_transform(predictions['Target'].values.reshape(-1,1))\n",
    "    \n",
    "    date_id = [f'{m}_{y}' for m,y in dates]\n",
    "    sub = pd.DataFrame(predictions.sort_values('sku_name', ascending=False).sku_name.astype(str)+'_'+date_id*predictions.sku_name.nunique())\n",
    "    sub = sub.reset_index(drop=True).rename(columns={'sku_name':'Item_ID'})\n",
    "    \n",
    "    return sub.merge(predictions[['Item_ID', 'Target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JR6xbkxoGT0H"
   },
   "outputs": [],
   "source": [
    "\n",
    "def cyclic_encode(df: pd.DataFrame, columns: list):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    for col in columns:\n",
    "        max_val = df[col].max()\n",
    "\n",
    "        df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "        df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ow4eKCjoxImC"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _add_periodic_indices(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.sort_values(['year','month'])\n",
    "    # get indices of observations from each product\n",
    "    # df['idx'] = df.cumcount()\n",
    "\n",
    "    df['annual_series'] = df['month'] % 12     \n",
    "    df['9month_series'] = df['month'] % 9 \n",
    "    df['semi_annual_series'] = df['month'] % 6 \n",
    "    df['quarterly_series'] = df['month'] % 3 \n",
    "    df['2month_series'] = df['month'] % 2 \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4q36pdANaag_"
   },
   "outputs": [],
   "source": [
    "def lag_shift(data: pd.DataFrame, \n",
    "              group_by_col_name: str, \n",
    "              features: list, \n",
    "              lag_shift_periods: list):\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        df = data.copy()\n",
    "        df = df.sort_values(by=['year','month'])\n",
    "\n",
    "        for lp in lag_shift_periods:\n",
    "            for feat in features:\n",
    "                forward = f'{lp}month_fwd_lag_{feat}'\n",
    "                backward = f'{lp}month_bwd_lag_{feat}'\n",
    "                forw_diff, back_diff = f'{forward}_diff', f'{backward}_diff'\n",
    "\n",
    "                group = df.groupby(group_by_col_name)[feat]\n",
    "\n",
    "                # Perform forward lag shift\n",
    "                df[forward] = group.shift(lp)\n",
    "\n",
    "                # Perform backwards lag shift\n",
    "                # df[backward] = group.shift(-lp)\n",
    "\n",
    "                # Shift difference\n",
    "                df[forw_diff] = df[feat] - df[forward]\n",
    "                # df[back_diff] = df[backward] - df[feat]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2Fj3ppyMpqJ4"
   },
   "outputs": [],
   "source": [
    "SEED = 1121\n",
    "N_STEPS = 4\n",
    "LOOKBACK = 1\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "vJNGosx52jkR"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ModelsConfig():\n",
    "\n",
    "    # setting random seed\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # checks if gpu is available\n",
    "    use_cuda = torch.cuda.is_available() \n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    EPOCHS = 100000\n",
    "\n",
    "class DataConfig():\n",
    "    dataloader_params = dict(        \n",
    "    batch_size=3,\n",
    "    collate_fn= lambda x: collate_fn(x),\n",
    "    shuffle=False,\n",
    "    # num_workers=2, \n",
    "    # pin_memory=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQnFN2bA_lBR",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "id": "L8bT88rdz-xj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "train = pd.read_csv(f'{TRAIN_DIR}/Train.csv')\n",
    "desc = pd.read_csv(f'{DATA_DIR}/DataDictionary.csv')\n",
    "\n",
    "CAT = desc[36:]['Column Name'].tolist()\n",
    "channel_cols = [c for c in train.columns if all(f'_{i}' not in c for i in range(1,11)) and c not in CAT+['sku_name','month','year']]\n",
    "train_df = train.drop(columns=CAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "I9KQVSjpWekU"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "np.random.seed(SEED)\n",
    "sku_encoder = LabelEncoder(train.sku_name.sample(frac=0.95).unique())\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted([(m, y) for y,m in train_df.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "final_date = dates[-1]\n",
    "xtra = [(final_date[0]+i,final_date[1]) if final_date[0]+i<=12 else (final_date[0]+i-12, final_date[1]+1) for i in range(1,N_STEPS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "xtra_df = pd.DataFrame(np.nan, index=range(train_df.sku_name.nunique()), columns=train_df.columns)\n",
    "xtra_df['sku_name'] = train_df.sku_name.unique()\n",
    "xtra_rep = pd.DataFrame(np.repeat(xtra_df.values, N_STEPS, axis=0), columns=train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_dates(gp, dates):\n",
    "    gp['month'],gp['year'] = zip(*dates)\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, xtra_rep.groupby('sku_name').apply(assign_dates, xtra)]).reset_index(drop=True)\n",
    "train_df[[c for c in train_df.columns if 'sku_name' not in c]] = train_df.groupby(['sku_name']).transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bSMDe8HBR6IE"
   },
   "outputs": [],
   "source": [
    "# train_df = lag_shift(train_df, 'sku_name', channel_cols, [3, 6, 9])\n",
    "\n",
    "# lag_cols = [c for c in train_df.columns if 'lag' in c]\n",
    "# train_df.loc[:, lag_cols] = train_df[lag_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "E8YtszVFZi9Y"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "f25671d37804416f9dc918126c34d515",
      "fa725963d10448d3a1b49e160ebe7814",
      "e2517ad8550645a786cfb47ef7c7b25d",
      "ddfc896a0bfb4981a8cce9e6ed1a094a",
      "72229050ac7b44a1a9d98d22671b84ee",
      "2b0d4258562746aa9e6bf280d3735b2b",
      "50d6d46313cc4c26babac4203ec07c3d",
      "23225794fe3e491d9d6db3360b01eb3e",
      "959bb1bd1291458c8e7a6b86f7ed6d2e",
      "d7a120e280fa438482f09c7cd79abc89",
      "4ba922a6711241b2991cfef8bbd3c5e8",
      "e359990d14324ff9adf74dd51ed53584",
      "aee2972fb8204b8c8e4b1cac5bb5aadf",
      "c632e524e100468885fd0d0f939ed3ec",
      "ba0f0f6acac149afb42db0931a177b90",
      "5f8adbd613c14b518b7571eea5612929",
      "3bd6fed385dd4b299aa56026411c783e",
      "7671bf9c19494a2b9f0ee5e2337f48c9",
      "61183c0aced54263988111244d5955e8",
      "0d771d3e5b064dae886a3e1a3eed17e2",
      "099cda2b44d040b9b461c2107d3d9ff3",
      "e517d5baae4242cfbda65f877f0bba46"
     ]
    },
    "id": "U_fYoqHiyOSI",
    "outputId": "df5001e0-efd9-4c14-de4e-c29c5893a6a0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd0c8337ea44238a5b8ac689d22924d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preparing sequences:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "cols = ['sku_name', 'month', 'year', 'sku_coded']\n",
    "target_cols = channel_cols+cols\n",
    "\n",
    "train_idx, val_idx = get_sequence_index(train_df, N_STEPS, LOOKBACK)\n",
    "train_data, val_data, features = get_data(train_df, train_idx, val_idx, sku_encoder, target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "imputing missing values:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "imputing missing values:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "impute_missing(train_data, channel_cols)\n",
    "impute_missing(val_data, channel_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDVeOw1vsa0O",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Walk Forward Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "48e0bcd890854a18b4710ba90d643830",
      "9e0a23c6f6574573929267c26790f0e8",
      "7585eee2183f4fa99749275bef109d60",
      "d9ea1817d5e24f01a6b85ce8b7f0d697",
      "b91173e97c4247c48c741d563ef17894",
      "a8b9bbd32dfe4f569374b8b878f715c4",
      "6529afe4759e41629a9d25e0cb2c530a",
      "ba2f651c4e864397b50a669b0ae23198",
      "5f849ad724954d64aba622d3799fd6b1",
      "1e131596ef834f13b0847b6a516c2064",
      "6896dcc1086c407c9bb987d026a9b335",
      "caf599e669e84e83bd02fe59add8593d",
      "f0787d2ae1434df786ae30a5cad26238",
      "4c44a3c1654745b6bc8db58ecdd99940",
      "a21dcb1036414c74af5cf15adcb102c0",
      "7a8c942666bc41dc86885e0cafe21f2a",
      "e261b8adb1a248b9b616d3b5655061b9",
      "0abb324774c148eaa19fac4fb665fe9b",
      "c0ba0ca588d64a15829ec04cc41ebf49",
      "8d382f3aec96437ea005b17f976f811c",
      "aab41a09b24d48dc83684dbacccf245f",
      "48cc49781ad34644b06a70c602688976",
      "dbca7372a9e64e02be2f4f1975b50e9b",
      "3bcf2e19c7464341a5f0eb8eed8c9916",
      "0ef75938ce8e4a68af5e1ecc7079639c",
      "0ce3ea3d2f554ee4920633b27749cfd6",
      "e9a809c9294d40d2803d997bb6efc09b",
      "8b87b5635a1c4f20a39de50b42b97747",
      "c7fd994b956c42a28ddc0302aade887d",
      "0a0bf323f759488881cb36c6348ee711",
      "46f52779db1b4ecda445c105b6e7d939",
      "b9aeb9eb7e3649b78e206df552f0c78d",
      "9f33d91db81c4f7084a5336821d56dc6"
     ]
    },
    "id": "DR7LUn5v1uN0",
    "outputId": "d6ab677d-8d48-43af-b0e9-6e9d469b145f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a125ee98e514423c94e80343c221acb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizing data:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_scaler, y_scaler, target_scaler = prepare_data_scaler(x_scaler, y_scaler, target_scaler, train_data, val_data)\n",
    "train_norm, val_norm = normalize_data(train_data, val_data, x_scaler, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sk5qDw7Iu_bt",
    "outputId": "c519a09f-100a-4256-df1a-55bea9334f53",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 1 from 1/2016 to 1/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.1274\tVal Loss: 0.1164\tVal MAE: 800424.52\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0638\tVal Loss: 0.0564\tVal MAE: 478052.22\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0353\tVal Loss: 0.0342\tVal MAE: 416119.68\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0217\tVal Loss: 0.0222\tVal MAE: 339049.80\telapsed: 0.03 mins\n",
      "Epoch 25: Train Loss: 0.0162\tVal Loss: 0.0180\tVal MAE: 303221.91\telapsed: 0.04 mins\n",
      "Epoch 30: Train Loss: 0.0112\tVal Loss: 0.0139\tVal MAE: 282197.85\telapsed: 0.05 mins\n",
      "Epoch 35: Train Loss: 0.0087\tVal Loss: 0.0117\tVal MAE: 263727.05\telapsed: 0.05 mins\n",
      "Epoch 40: Train Loss: 0.0067\tVal Loss: 0.0103\tVal MAE: 252766.27\telapsed: 0.06 mins\n",
      "Epoch 45: Train Loss: 0.0055\tVal Loss: 0.0092\tVal MAE: 247056.33\telapsed: 0.07 mins\n",
      "Epoch 50: Train Loss: 0.0048\tVal Loss: 0.0088\tVal MAE: 243030.63\telapsed: 0.07 mins\n",
      "Epoch 55: Train Loss: 0.0043\tVal Loss: 0.0084\tVal MAE: 240999.93\telapsed: 0.08 mins\n",
      "Epoch 60: Train Loss: 0.0040\tVal Loss: 0.0081\tVal MAE: 238845.72\telapsed: 0.09 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 59\tVal loss: 0.0081\tVal MAE: 238588.06\tTotal time elapsed: 0.09 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 2 from 1/2016 to 2/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0440\tVal Loss: 0.0403\tVal MAE: 458115.28\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0211\tVal Loss: 0.0222\tVal MAE: 322762.26\telapsed: 0.03 mins\n",
      "Epoch 15: Train Loss: 0.0121\tVal Loss: 0.0143\tVal MAE: 278051.86\telapsed: 0.04 mins\n",
      "Epoch 20: Train Loss: 0.0085\tVal Loss: 0.0114\tVal MAE: 251582.75\telapsed: 0.06 mins\n",
      "Epoch 25: Train Loss: 0.0070\tVal Loss: 0.0103\tVal MAE: 239927.20\telapsed: 0.07 mins\n",
      "Epoch 30: Train Loss: 0.0064\tVal Loss: 0.0098\tVal MAE: 235494.80\telapsed: 0.09 mins\n",
      "Epoch 35: Train Loss: 0.0061\tVal Loss: 0.0096\tVal MAE: 232791.55\telapsed: 0.10 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 33\tVal loss: 0.0096\tVal MAE: 232114.43\tTotal time elapsed: 0.11 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 3 from 1/2016 to 3/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0257\tVal Loss: 0.0255\tVal MAE: 365045.02\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0119\tVal Loss: 0.0129\tVal MAE: 286577.98\telapsed: 0.05 mins\n",
      "Epoch 15: Train Loss: 0.0090\tVal Loss: 0.0104\tVal MAE: 264004.39\telapsed: 0.07 mins\n",
      "Epoch 20: Train Loss: 0.0082\tVal Loss: 0.0099\tVal MAE: 258347.64\telapsed: 0.09 mins\n",
      "Epoch 25: Train Loss: 0.0080\tVal Loss: 0.0096\tVal MAE: 255870.30\telapsed: 0.12 mins\n",
      "Epoch 30: Train Loss: 0.0078\tVal Loss: 0.0095\tVal MAE: 254582.14\telapsed: 0.14 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 27\tVal loss: 0.0095\tVal MAE: 254582.14\tTotal time elapsed: 0.14 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 4 from 1/2016 to 4/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0385\tVal Loss: 0.0357\tVal MAE: 441307.47\telapsed: 0.03 mins\n",
      "Epoch 10: Train Loss: 0.0137\tVal Loss: 0.0149\tVal MAE: 310107.66\telapsed: 0.05 mins\n",
      "Epoch 15: Train Loss: 0.0088\tVal Loss: 0.0103\tVal MAE: 271805.91\telapsed: 0.08 mins\n",
      "Epoch 20: Train Loss: 0.0079\tVal Loss: 0.0096\tVal MAE: 264753.30\telapsed: 0.11 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 21\tVal loss: 0.0095\tVal MAE: 263730.19\tTotal time elapsed: 0.13 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 5 from 1/2016 to 5/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0158\tVal Loss: 0.0160\tVal MAE: 319368.63\telapsed: 0.03 mins\n",
      "Epoch 10: Train Loss: 0.0098\tVal Loss: 0.0105\tVal MAE: 273948.26\telapsed: 0.07 mins\n",
      "Epoch 15: Train Loss: 0.0091\tVal Loss: 0.0099\tVal MAE: 267804.60\telapsed: 0.10 mins\n",
      "Epoch 20: Train Loss: 0.0089\tVal Loss: 0.0097\tVal MAE: 266349.05\telapsed: 0.13 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 21\tVal loss: 0.0096\tVal MAE: 264455.40\tTotal time elapsed: 0.16 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 6 from 1/2016 to 6/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0136\tVal Loss: 0.0139\tVal MAE: 315076.27\telapsed: 0.04 mins\n",
      "Epoch 10: Train Loss: 0.0095\tVal Loss: 0.0102\tVal MAE: 284577.60\telapsed: 0.08 mins\n",
      "Epoch 15: Train Loss: 0.0092\tVal Loss: 0.0099\tVal MAE: 280938.93\telapsed: 0.12 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 15\tVal loss: 0.0099\tVal MAE: 279993.33\tTotal time elapsed: 0.14 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 7 from 1/2016 to 7/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0125\tVal Loss: 0.0131\tVal MAE: 323153.31\telapsed: 0.05 mins\n",
      "Epoch 10: Train Loss: 0.0092\tVal Loss: 0.0102\tVal MAE: 299320.28\telapsed: 0.09 mins\n",
      "Epoch 15: Train Loss: 0.0090\tVal Loss: 0.0099\tVal MAE: 297103.80\telapsed: 0.14 mins\n",
      "Epoch 20: Train Loss: 0.0088\tVal Loss: 0.0098\tVal MAE: 294840.66\telapsed: 0.19 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 17\tVal loss: 0.0098\tVal MAE: 294840.66\tTotal time elapsed: 0.19 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 8 from 1/2016 to 8/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0126\tVal Loss: 0.0125\tVal MAE: 355427.38\telapsed: 0.05 mins\n",
      "Epoch 10: Train Loss: 0.0096\tVal Loss: 0.0096\tVal MAE: 333222.79\telapsed: 0.11 mins\n",
      "Epoch 15: Train Loss: 0.0094\tVal Loss: 0.0094\tVal MAE: 327464.58\telapsed: 0.16 mins\n",
      "Epoch 20: Train Loss: 0.0092\tVal Loss: 0.0092\tVal MAE: 327630.41\telapsed: 0.21 mins\n",
      "Epoch 25: Train Loss: 0.0090\tVal Loss: 0.0091\tVal MAE: 326610.02\telapsed: 0.27 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 26\tVal loss: 0.0090\tVal MAE: 326133.19\tTotal time elapsed: 0.31 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 9 from 1/2016 to 9/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0105\tVal Loss: 0.0113\tVal MAE: 332208.21\telapsed: 0.06 mins\n",
      "Epoch 10: Train Loss: 0.0093\tVal Loss: 0.0102\tVal MAE: 324635.93\telapsed: 0.12 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0102\tVal MAE: 322902.10\tTotal time elapsed: 0.16 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 10 from 1/2016 to 10/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0118\tVal Loss: 0.0117\tVal MAE: 322530.51\telapsed: 0.07 mins\n",
      "Epoch 10: Train Loss: 0.0105\tVal Loss: 0.0105\tVal MAE: 312976.66\telapsed: 0.13 mins\n",
      "Epoch 15: Train Loss: 0.0101\tVal Loss: 0.0102\tVal MAE: 307270.75\telapsed: 0.20 mins\n",
      "Epoch 20: Train Loss: 0.0098\tVal Loss: 0.0099\tVal MAE: 304760.35\telapsed: 0.26 mins\n",
      "Epoch 25: Train Loss: 0.0095\tVal Loss: 0.0096\tVal MAE: 304220.01\telapsed: 0.33 mins\n",
      "Epoch 30: Train Loss: 0.0095\tVal Loss: 0.0095\tVal MAE: 303913.06\telapsed: 0.40 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 31\tVal loss: 0.0093\tVal MAE: 302814.08\tTotal time elapsed: 0.45 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 11 from 1/2016 to 11/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0108\tVal Loss: 0.0105\tVal MAE: 293657.77\telapsed: 0.07 mins\n",
      "Epoch 10: Train Loss: 0.0097\tVal Loss: 0.0095\tVal MAE: 286559.03\telapsed: 0.14 mins\n",
      "Epoch 15: Train Loss: 0.0094\tVal Loss: 0.0092\tVal MAE: 284827.00\telapsed: 0.22 mins\n",
      "Epoch 20: Train Loss: 0.0092\tVal Loss: 0.0091\tVal MAE: 285218.43\telapsed: 0.29 mins\n",
      "Epoch 25: Train Loss: 0.0091\tVal Loss: 0.0091\tVal MAE: 282307.80\telapsed: 0.36 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 23\tVal loss: 0.0089\tVal MAE: 282574.46\tTotal time elapsed: 0.38 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 12 from 1/2016 to 12/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0105\tVal Loss: 0.0099\tVal MAE: 282575.74\telapsed: 0.08 mins\n",
      "Epoch 10: Train Loss: 0.0100\tVal Loss: 0.0094\tVal MAE: 276633.66\telapsed: 0.16 mins\n",
      "Epoch 15: Train Loss: 0.0097\tVal Loss: 0.0092\tVal MAE: 274710.06\telapsed: 0.24 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0092\tVal MAE: 274710.06\tTotal time elapsed: 0.24 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 13 from 1/2016 to 1/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0104\tVal Loss: 0.0096\tVal MAE: 301380.35\telapsed: 0.09 mins\n",
      "Epoch 10: Train Loss: 0.0097\tVal Loss: 0.0088\tVal MAE: 298982.71\telapsed: 0.17 mins\n",
      "Epoch 15: Train Loss: 0.0092\tVal Loss: 0.0083\tVal MAE: 295921.59\telapsed: 0.26 mins\n",
      "Epoch 20: Train Loss: 0.0091\tVal Loss: 0.0082\tVal MAE: 295356.44\telapsed: 0.34 mins\n",
      "Epoch 25: Train Loss: 0.0087\tVal Loss: 0.0079\tVal MAE: 292583.54\telapsed: 0.43 mins\n",
      "Epoch 30: Train Loss: 0.0085\tVal Loss: 0.0077\tVal MAE: 292422.07\telapsed: 0.52 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 28\tVal loss: 0.0076\tVal MAE: 291415.03\tTotal time elapsed: 0.54 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 14 from 1/2016 to 2/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0093\tVal Loss: 0.0088\tVal MAE: 287872.07\telapsed: 0.09 mins\n",
      "Epoch 10: Train Loss: 0.0087\tVal Loss: 0.0082\tVal MAE: 284651.00\telapsed: 0.19 mins\n",
      "Epoch 15: Train Loss: 0.0083\tVal Loss: 0.0079\tVal MAE: 281730.14\telapsed: 0.28 mins\n",
      "Epoch 20: Train Loss: 0.0080\tVal Loss: 0.0078\tVal MAE: 281923.15\telapsed: 0.37 mins\n",
      "Epoch 25: Train Loss: 0.0078\tVal Loss: 0.0074\tVal MAE: 278990.83\telapsed: 0.47 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 24\tVal loss: 0.0074\tVal MAE: 279833.08\tTotal time elapsed: 0.51 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 15 from 1/2016 to 3/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0091\tVal Loss: 0.0088\tVal MAE: 269625.41\telapsed: 0.10 mins\n",
      "Epoch 10: Train Loss: 0.0084\tVal Loss: 0.0082\tVal MAE: 265151.87\telapsed: 0.20 mins\n",
      "Epoch 15: Train Loss: 0.0079\tVal Loss: 0.0079\tVal MAE: 263957.07\telapsed: 0.30 mins\n",
      "Epoch 20: Train Loss: 0.0076\tVal Loss: 0.0075\tVal MAE: 263109.23\telapsed: 0.41 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 19\tVal loss: 0.0075\tVal MAE: 261707.18\tTotal time elapsed: 0.45 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 16 from 1/2016 to 4/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0091\tVal Loss: 0.0110\tVal MAE: 315745.19\telapsed: 0.11 mins\n",
      "Epoch 10: Train Loss: 0.0087\tVal Loss: 0.0104\tVal MAE: 313823.29\telapsed: 0.21 mins\n",
      "Epoch 15: Train Loss: 0.0079\tVal Loss: 0.0098\tVal MAE: 311709.69\telapsed: 0.32 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 15\tVal loss: 0.0098\tVal MAE: 312302.10\tTotal time elapsed: 0.39 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 17 from 1/2016 to 5/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0111\tVal Loss: 0.0111\tVal MAE: 354018.00\telapsed: 0.11 mins\n",
      "Epoch 10: Train Loss: 0.0107\tVal Loss: 0.0105\tVal MAE: 351512.91\telapsed: 0.23 mins\n",
      "Epoch 15: Train Loss: 0.0102\tVal Loss: 0.0100\tVal MAE: 349791.02\telapsed: 0.34 mins\n",
      "Epoch 20: Train Loss: 0.0100\tVal Loss: 0.0098\tVal MAE: 349634.26\telapsed: 0.45 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 21\tVal loss: 0.0097\tVal MAE: 349128.30\tTotal time elapsed: 0.55 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 18 from 1/2016 to 6/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0111\tVal Loss: 0.0120\tVal MAE: 360060.22\telapsed: 0.12 mins\n",
      "Epoch 10: Train Loss: 0.0106\tVal Loss: 0.0113\tVal MAE: 356937.69\telapsed: 0.24 mins\n",
      "Epoch 15: Train Loss: 0.0103\tVal Loss: 0.0106\tVal MAE: 355514.12\telapsed: 0.36 mins\n",
      "Epoch 20: Train Loss: 0.0101\tVal Loss: 0.0102\tVal MAE: 355800.68\telapsed: 0.48 mins\n",
      "Epoch 25: Train Loss: 0.0098\tVal Loss: 0.0100\tVal MAE: 354367.74\telapsed: 0.60 mins\n",
      "Epoch 30: Train Loss: 0.0098\tVal Loss: 0.0102\tVal MAE: 355163.44\telapsed: 0.72 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 28\tVal loss: 0.0099\tVal MAE: 355289.19\tTotal time elapsed: 0.75 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 19 from 1/2016 to 7/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0122\tVal Loss: 0.0130\tVal MAE: 433529.26\telapsed: 0.13 mins\n",
      "Epoch 10: Train Loss: 0.0116\tVal Loss: 0.0122\tVal MAE: 432080.98\telapsed: 0.25 mins\n",
      "Epoch 15: Train Loss: 0.0111\tVal Loss: 0.0116\tVal MAE: 429954.10\telapsed: 0.38 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 16\tVal loss: 0.0115\tVal MAE: 428961.01\tTotal time elapsed: 0.49 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 20 from 1/2016 to 8/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0124\tVal Loss: 0.0104\tVal MAE: 362301.95\telapsed: 0.13 mins\n",
      "Epoch 10: Train Loss: 0.0120\tVal Loss: 0.0098\tVal MAE: 360596.71\telapsed: 0.27 mins\n",
      "Epoch 15: Train Loss: 0.0119\tVal Loss: 0.0096\tVal MAE: 357324.78\telapsed: 0.41 mins\n",
      "Epoch 20: Train Loss: 0.0117\tVal Loss: 0.0092\tVal MAE: 357310.41\telapsed: 0.54 mins\n",
      "Epoch 25: Train Loss: 0.0115\tVal Loss: 0.0092\tVal MAE: 357854.57\telapsed: 0.68 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 22\tVal loss: 0.0091\tVal MAE: 357854.57\tTotal time elapsed: 0.68 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 21 from 1/2016 to 9/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0104\tVal Loss: 0.0103\tVal MAE: 348248.16\telapsed: 0.14 mins\n",
      "Epoch 10: Train Loss: 0.0098\tVal Loss: 0.0098\tVal MAE: 344739.34\telapsed: 0.29 mins\n",
      "Epoch 15: Train Loss: 0.0095\tVal Loss: 0.0093\tVal MAE: 346294.63\telapsed: 0.43 mins\n",
      "Epoch 20: Train Loss: 0.0094\tVal Loss: 0.0088\tVal MAE: 342545.96\telapsed: 0.58 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 20\tVal loss: 0.0088\tVal MAE: 345910.75\tTotal time elapsed: 0.66 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 22 from 1/2016 to 10/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0105\tVal Loss: 0.0100\tVal MAE: 331774.56\telapsed: 0.15 mins\n",
      "Epoch 10: Train Loss: 0.0098\tVal Loss: 0.0092\tVal MAE: 325794.10\telapsed: 0.31 mins\n",
      "Epoch 15: Train Loss: 0.0095\tVal Loss: 0.0089\tVal MAE: 325034.17\telapsed: 0.46 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 16\tVal loss: 0.0088\tVal MAE: 324826.60\tTotal time elapsed: 0.58 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 23 from 1/2016 to 11/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0096\tVal Loss: 0.0087\tVal MAE: 291206.05\telapsed: 0.16 mins\n",
      "Epoch 10: Train Loss: 0.0092\tVal Loss: 0.0083\tVal MAE: 289720.88\telapsed: 0.32 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 11\tVal loss: 0.0082\tVal MAE: 289899.31\tTotal time elapsed: 0.45 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 24 from 1/2016 to 12/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0087\tVal Loss: 0.0083\tVal MAE: 280491.62\telapsed: 0.17 mins\n",
      "Epoch 10: Train Loss: 0.0082\tVal Loss: 0.0078\tVal MAE: 275765.52\telapsed: 0.33 mins\n",
      "Epoch 15: Train Loss: 0.0080\tVal Loss: 0.0075\tVal MAE: 272958.01\telapsed: 0.50 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 15\tVal loss: 0.0075\tVal MAE: 272025.56\tTotal time elapsed: 0.60 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 25 from 1/2016 to 1/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0081\tVal Loss: 0.0079\tVal MAE: 259684.73\telapsed: 0.17 mins\n",
      "Epoch 10: Train Loss: 0.0075\tVal Loss: 0.0074\tVal MAE: 255445.52\telapsed: 0.34 mins\n",
      "Epoch 15: Train Loss: 0.0073\tVal Loss: 0.0072\tVal MAE: 253986.08\telapsed: 0.52 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 14\tVal loss: 0.0071\tVal MAE: 253634.31\tTotal time elapsed: 0.59 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 26 from 1/2016 to 2/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0079\tVal Loss: 0.0078\tVal MAE: 267461.67\telapsed: 0.18 mins\n",
      "Epoch 10: Train Loss: 0.0072\tVal Loss: 0.0070\tVal MAE: 260713.87\telapsed: 0.36 mins\n",
      "Epoch 15: Train Loss: 0.0071\tVal Loss: 0.0068\tVal MAE: 259413.61\telapsed: 0.54 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 16\tVal loss: 0.0067\tVal MAE: 255882.37\tTotal time elapsed: 0.68 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 27 from 1/2016 to 3/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0079\tVal Loss: 0.0080\tVal MAE: 266482.20\telapsed: 0.18 mins\n",
      "Epoch 10: Train Loss: 0.0071\tVal Loss: 0.0071\tVal MAE: 259096.49\telapsed: 0.37 mins\n",
      "Epoch 15: Train Loss: 0.0068\tVal Loss: 0.0068\tVal MAE: 259577.56\telapsed: 0.56 mins\n",
      "Epoch 20: Train Loss: 0.0067\tVal Loss: 0.0067\tVal MAE: 255743.59\telapsed: 0.74 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 18\tVal loss: 0.0066\tVal MAE: 255895.33\tTotal time elapsed: 0.78 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 28 from 1/2016 to 4/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0079\tVal Loss: 0.0079\tVal MAE: 269665.29\telapsed: 0.19 mins\n",
      "Epoch 10: Train Loss: 0.0071\tVal Loss: 0.0072\tVal MAE: 264893.02\telapsed: 0.38 mins\n",
      "Epoch 15: Train Loss: 0.0068\tVal Loss: 0.0068\tVal MAE: 260644.25\telapsed: 0.57 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 15\tVal loss: 0.0068\tVal MAE: 261212.62\tTotal time elapsed: 0.69 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 29 from 1/2016 to 5/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0078\tVal Loss: 0.0078\tVal MAE: 287636.01\telapsed: 0.20 mins\n",
      "Epoch 10: Train Loss: 0.0072\tVal Loss: 0.0072\tVal MAE: 280136.35\telapsed: 0.40 mins\n",
      "Epoch 15: Train Loss: 0.0070\tVal Loss: 0.0069\tVal MAE: 278385.44\telapsed: 0.60 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 13\tVal loss: 0.0069\tVal MAE: 279712.62\tTotal time elapsed: 0.64 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 30 from 1/2016 to 6/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0079\tVal Loss: 0.0077\tVal MAE: 294758.23\telapsed: 0.21 mins\n",
      "Epoch 10: Train Loss: 0.0073\tVal Loss: 0.0072\tVal MAE: 289375.01\telapsed: 0.41 mins\n",
      "Epoch 15: Train Loss: 0.0072\tVal Loss: 0.0070\tVal MAE: 287597.62\telapsed: 0.62 mins\n",
      "Epoch 20: Train Loss: 0.0068\tVal Loss: 0.0067\tVal MAE: 287348.77\telapsed: 0.83 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 17\tVal loss: 0.0067\tVal MAE: 287348.77\tTotal time elapsed: 0.83 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 31 from 1/2016 to 7/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0075\tVal Loss: 0.0074\tVal MAE: 288420.06\telapsed: 0.21 mins\n",
      "Epoch 10: Train Loss: 0.0070\tVal Loss: 0.0069\tVal MAE: 284243.44\telapsed: 0.43 mins\n",
      "Epoch 15: Train Loss: 0.0068\tVal Loss: 0.0067\tVal MAE: 284088.37\telapsed: 0.64 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 16\tVal loss: 0.0065\tVal MAE: 282841.39\tTotal time elapsed: 0.81 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 32 from 1/2016 to 8/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0074\tVal Loss: 0.0073\tVal MAE: 291590.87\telapsed: 0.22 mins\n",
      "Epoch 10: Train Loss: 0.0070\tVal Loss: 0.0068\tVal MAE: 286030.13\telapsed: 0.44 mins\n",
      "Epoch 15: Train Loss: 0.0067\tVal Loss: 0.0065\tVal MAE: 283713.21\telapsed: 0.66 mins\n",
      "Epoch 20: Train Loss: 0.0063\tVal Loss: 0.0062\tVal MAE: 282471.24\telapsed: 0.88 mins\n",
      "Epoch 25: Train Loss: 0.0062\tVal Loss: 0.0061\tVal MAE: 282553.36\telapsed: 1.10 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 22\tVal loss: 0.0061\tVal MAE: 282553.36\tTotal time elapsed: 1.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 33 from 1/2016 to 9/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0075\tVal Loss: 0.0075\tVal MAE: 291222.45\telapsed: 0.23 mins\n",
      "Epoch 10: Train Loss: 0.0069\tVal Loss: 0.0068\tVal MAE: 287886.83\telapsed: 0.45 mins\n",
      "Epoch 15: Train Loss: 0.0068\tVal Loss: 0.0065\tVal MAE: 282627.31\telapsed: 0.68 mins\n",
      "Epoch 20: Train Loss: 0.0063\tVal Loss: 0.0063\tVal MAE: 280224.11\telapsed: 0.90 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 17\tVal loss: 0.0063\tVal MAE: 280224.11\tTotal time elapsed: 0.90 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 34 from 1/2016 to 10/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0076\tVal Loss: 0.0076\tVal MAE: 280727.32\telapsed: 0.23 mins\n",
      "Epoch 10: Train Loss: 0.0068\tVal Loss: 0.0069\tVal MAE: 274084.65\telapsed: 0.47 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0069\tVal MAE: 273308.81\tTotal time elapsed: 0.56 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 35 from 1/2016 to 11/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0074\tVal Loss: 0.0072\tVal MAE: 284048.43\telapsed: 0.24 mins\n",
      "Epoch 10: Train Loss: 0.0070\tVal Loss: 0.0068\tVal MAE: 274658.49\telapsed: 0.48 mins\n",
      "Epoch 15: Train Loss: 0.0067\tVal Loss: 0.0066\tVal MAE: 272371.29\telapsed: 0.72 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0066\tVal MAE: 272371.29\tTotal time elapsed: 0.72 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 36 from 1/2016 to 12/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0071\tVal Loss: 0.0072\tVal MAE: 272484.77\telapsed: 0.25 mins\n",
      "Epoch 10: Train Loss: 0.0066\tVal Loss: 0.0067\tVal MAE: 256847.31\telapsed: 0.49 mins\n",
      "Epoch 15: Train Loss: 0.0066\tVal Loss: 0.0065\tVal MAE: 257032.79\telapsed: 0.74 mins\n",
      "Epoch 20: Train Loss: 0.0065\tVal Loss: 0.0063\tVal MAE: 249872.58\telapsed: 0.98 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 17\tVal loss: 0.0063\tVal MAE: 249872.58\tTotal time elapsed: 0.98 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 37 from 1/2016 to 1/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0073\tVal Loss: 0.0071\tVal MAE: 257257.11\telapsed: 0.25 mins\n",
      "Epoch 10: Train Loss: 0.0068\tVal Loss: 0.0066\tVal MAE: 244192.77\telapsed: 0.51 mins\n",
      "Epoch 15: Train Loss: 0.0066\tVal Loss: 0.0063\tVal MAE: 239651.94\telapsed: 0.76 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 13\tVal loss: 0.0062\tVal MAE: 240643.46\tTotal time elapsed: 0.82 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 38 from 1/2016 to 2/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0070\tVal Loss: 0.0066\tVal MAE: 259983.84\telapsed: 0.26 mins\n",
      "Epoch 10: Train Loss: 0.0065\tVal Loss: 0.0062\tVal MAE: 245346.24\telapsed: 0.52 mins\n",
      "Epoch 15: Train Loss: 0.0061\tVal Loss: 0.0060\tVal MAE: 240180.03\telapsed: 0.78 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 13\tVal loss: 0.0059\tVal MAE: 239700.38\tTotal time elapsed: 0.83 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 39 from 1/2016 to 3/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0067\tVal Loss: 0.0066\tVal MAE: 253521.47\telapsed: 0.27 mins\n",
      "Epoch 10: Train Loss: 0.0068\tVal Loss: 0.0064\tVal MAE: 246289.76\telapsed: 0.53 mins\n",
      "Epoch 15: Train Loss: 0.0063\tVal Loss: 0.0059\tVal MAE: 239220.25\telapsed: 0.80 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 15\tVal loss: 0.0059\tVal MAE: 238571.59\tTotal time elapsed: 0.96 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 40 from 1/2016 to 4/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0065\tVal Loss: 0.0066\tVal MAE: 262804.58\telapsed: 0.27 mins\n",
      "Epoch 10: Train Loss: 0.0061\tVal Loss: 0.0062\tVal MAE: 250642.44\telapsed: 0.55 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 11\tVal loss: 0.0060\tVal MAE: 247011.49\tTotal time elapsed: 0.77 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 41 from 1/2016 to 5/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0066\tVal Loss: 0.0067\tVal MAE: 305295.22\telapsed: 0.28 mins\n",
      "Epoch 10: Train Loss: 0.0063\tVal Loss: 0.0064\tVal MAE: 297125.38\telapsed: 0.57 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 11\tVal loss: 0.0061\tVal MAE: 294963.43\tTotal time elapsed: 0.80 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 42 from 1/2016 to 6/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0068\tVal Loss: 0.0072\tVal MAE: 297536.54\telapsed: 0.29 mins\n",
      "Epoch 10: Train Loss: 0.0064\tVal Loss: 0.0070\tVal MAE: 291911.31\telapsed: 0.58 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0067\tVal MAE: 289294.73\tTotal time elapsed: 0.69 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 43 from 1/2016 to 7/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0072\tVal Loss: 0.0079\tVal MAE: 289051.54\telapsed: 0.29 mins\n",
      "Epoch 10: Train Loss: 0.0067\tVal Loss: 0.0074\tVal MAE: 284972.62\telapsed: 0.59 mins\n",
      "Epoch 15: Train Loss: 0.0066\tVal Loss: 0.0073\tVal MAE: 283089.13\telapsed: 0.88 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0073\tVal MAE: 283089.13\tTotal time elapsed: 0.88 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 44 from 1/2016 to 8/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0081\tVal Loss: 0.0086\tVal MAE: 292281.60\telapsed: 0.30 mins\n",
      "Epoch 10: Train Loss: 0.0078\tVal Loss: 0.0080\tVal MAE: 286193.83\telapsed: 0.60 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0080\tVal MAE: 283191.71\tTotal time elapsed: 0.78 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 45 from 1/2016 to 9/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0081\tVal Loss: 0.0084\tVal MAE: 225496.58\telapsed: 0.31 mins\n",
      "Epoch 10: Train Loss: 0.0079\tVal Loss: 0.0082\tVal MAE: 222938.80\telapsed: 0.62 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0082\tVal MAE: 222527.63\tTotal time elapsed: 0.69 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 46 from 1/2016 to 10/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0085\tVal Loss: 0.0084\tVal MAE: 233323.77\telapsed: 0.32 mins\n",
      "Epoch 10: Train Loss: 0.0084\tVal Loss: 0.0083\tVal MAE: 234323.55\telapsed: 0.64 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 7\tVal loss: 0.0083\tVal MAE: 234323.55\tTotal time elapsed: 0.64 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 47 from 1/2016 to 11/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0084\tVal Loss: 0.0080\tVal MAE: 233424.22\telapsed: 0.32 mins\n",
      "Epoch 10: Train Loss: 0.0081\tVal Loss: 0.0076\tVal MAE: 233359.27\telapsed: 0.65 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0076\tVal MAE: 233161.16\tTotal time elapsed: 0.84 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 48 from 1/2016 to 12/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0085\tVal Loss: 0.0078\tVal MAE: 234883.47\telapsed: 0.33 mins\n",
      "Epoch 10: Train Loss: 0.0079\tVal Loss: 0.0074\tVal MAE: 236857.36\telapsed: 0.66 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0074\tVal MAE: 239126.79\tTotal time elapsed: 0.86 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 49 from 1/2016 to 1/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0076\tVal Loss: 0.0074\tVal MAE: 224577.05\telapsed: 0.34 mins\n",
      "Epoch 10: Train Loss: 0.0074\tVal Loss: 0.0072\tVal MAE: 227582.49\telapsed: 0.67 mins\n",
      "Epoch 15: Train Loss: 0.0073\tVal Loss: 0.0070\tVal MAE: 231048.08\telapsed: 1.01 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 14\tVal loss: 0.0069\tVal MAE: 228242.75\tTotal time elapsed: 1.14 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 50 from 1/2016 to 2/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0075\tVal Loss: 0.0073\tVal MAE: 218897.48\telapsed: 0.34 mins\n",
      "Epoch 10: Train Loss: 0.0073\tVal Loss: 0.0069\tVal MAE: 221754.98\telapsed: 0.69 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0069\tVal MAE: 219530.83\tTotal time elapsed: 0.89 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 51 from 1/2016 to 3/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0073\tVal Loss: 0.0077\tVal MAE: 211000.43\telapsed: 0.35 mins\n",
      "Epoch 10: Train Loss: 0.0069\tVal Loss: 0.0075\tVal MAE: 216099.33\telapsed: 0.71 mins\n",
      "Epoch 15: Train Loss: 0.0068\tVal Loss: 0.0074\tVal MAE: 214258.80\telapsed: 1.06 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0073\tVal MAE: 214258.80\tTotal time elapsed: 1.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 52 from 1/2016 to 4/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0077\tVal Loss: 0.0082\tVal MAE: 225017.40\telapsed: 0.36 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 5\tVal loss: 0.0082\tVal MAE: 225411.48\tTotal time elapsed: 0.58 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 53 from 1/2016 to 5/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0083\tVal Loss: 0.0087\tVal MAE: 236864.64\telapsed: 0.37 mins\n",
      "Epoch 10: Train Loss: 0.0081\tVal Loss: 0.0087\tVal MAE: 234694.57\telapsed: 0.74 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 7\tVal loss: 0.0086\tVal MAE: 234694.57\tTotal time elapsed: 0.74 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 54 from 1/2016 to 6/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0086\tVal Loss: 0.0109\tVal MAE: 148932.46\telapsed: 0.37 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 5\tVal loss: 0.0109\tVal MAE: 151826.05\tTotal time elapsed: 0.60 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 55 from 1/2016 to 7/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0110\tVal Loss: 0.0127\tVal MAE: 118079.20\telapsed: 0.38 mins\n",
      "Epoch 10: Train Loss: 0.0111\tVal Loss: 0.0124\tVal MAE: 118664.42\telapsed: 0.76 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0124\tVal MAE: 121425.44\tTotal time elapsed: 0.92 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 56 from 1/2016 to 8/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0129\tVal Loss: 0.0143\tVal MAE: 99420.92\telapsed: 0.39 mins\n",
      "Epoch 10: Train Loss: 0.0124\tVal Loss: 0.0138\tVal MAE: 94686.76\telapsed: 0.77 mins\n",
      "Epoch 15: Train Loss: 0.0123\tVal Loss: 0.0135\tVal MAE: 95244.79\telapsed: 1.16 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 14\tVal loss: 0.0135\tVal MAE: 95351.65\tTotal time elapsed: 1.32 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 57 from 1/2016 to 9/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0144\tVal Loss: 0.0156\tVal MAE: 85182.75\telapsed: 0.40 mins\n",
      "Epoch 10: Train Loss: 0.0138\tVal Loss: 0.0150\tVal MAE: 78959.00\telapsed: 0.80 mins\n",
      "Epoch 15: Train Loss: 0.0138\tVal Loss: 0.0147\tVal MAE: 78141.23\telapsed: 1.20 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 14\tVal loss: 0.0145\tVal MAE: 77426.34\tTotal time elapsed: 1.36 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Average Val loss: 0.0083\tAverage Val MAE: 264614.3379\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "window = 3\n",
    "expanding_window = True\n",
    "models, metrics_list, loss_list,  = [], [], []\n",
    "train_set, val_set = [], []\n",
    "\n",
    "dates = sorted([(m, y) for y,m in train_df.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "\n",
    "for ix in range(len(train_norm)):\n",
    "    if expanding_window:\n",
    "        print('Training step {} from {}/{} to {}/{}:'.format(ix+1, dates[0][0], dates[0][1], dates[ix][0], dates[ix][1]))\n",
    "        print('\\n')\n",
    "        train_set += [train_norm[ix]]\n",
    "        val_set += [val_norm[ix]]\n",
    "    else:\n",
    "        print('Training step {} from {}/{} to {}/{}:'.format(ix+1, dates[ix-window][0], dates[ix-window][1], dates[ix][0], dates[ix][1]))\n",
    "        print('\\n')\n",
    "        train_set = [train_norm[i] for i in range(ix-window,ix)] if ix>window else train_norm[:ix+1]\n",
    "        val_set = [val_norm[i] for i in range(ix-window,ix)] if ix>window else val_norm[:ix+1]\n",
    "        \n",
    "    train_dataset = FossilDataset(train_set, sku_encoder)\n",
    "    val_dataset = FossilDataset(val_set, sku_encoder, False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "    valid_loader = DataLoader(val_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    # model = pipe.model if ix>0 else FossilModel(len(features), 4, len(sku_encoder)+1, N_STEPS).double()\n",
    "    model = FossilEncoder(len(features), 4, len(sku_encoder)+1, N_STEPS, len(channel_cols)).double()\n",
    "    pipe = Pipeline(train_loader, valid_loader, model, target_scaler)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    pipe.train_model(3, 5)\n",
    "    best_model, val_loss, metrics = pipe.model_checkpoints[-1]\n",
    "    \n",
    "    models.append(best_model)\n",
    "    loss_list.append(val_loss)\n",
    "    metrics_list.append(metrics)\n",
    "print('Average Val loss: {:.4f}\\tAverage Val MAE: {:.4f}'.format(np.mean(loss_list), np.mean(metrics_list)))\n",
    "print('-'*50)\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_coded</th>\n",
       "      <th>starting_inventory</th>\n",
       "      <th>sellin</th>\n",
       "      <th>sellout</th>\n",
       "      <th>onhand_inventory</th>\n",
       "      <th>leftover_inventory</th>\n",
       "      <th>price</th>\n",
       "      <th>sku_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>idx</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>6462940.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>473071.0</td>\n",
       "      <td>3553604.0</td>\n",
       "      <td>704035.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>3.350403e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74</td>\n",
       "      <td>5217963.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>448759.0</td>\n",
       "      <td>3381394.0</td>\n",
       "      <td>519669.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>3.330252e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>4785412.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>552085.0</td>\n",
       "      <td>3310484.0</td>\n",
       "      <td>-82053.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>2.681489e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>4423771.0</td>\n",
       "      <td>400135.0</td>\n",
       "      <td>529799.0</td>\n",
       "      <td>2860712.0</td>\n",
       "      <td>-129664.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>400135.0</td>\n",
       "      <td>2.456778e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39507.0</td>\n",
       "      <td>175249.0</td>\n",
       "      <td>1116326.0</td>\n",
       "      <td>-135742.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>ABEANHARLE</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>39507.0</td>\n",
       "      <td>-8.503907e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>3301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2026.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>2026.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>YOSHTLEYMICH</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2026.0</td>\n",
       "      <td>-2.014215e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>1574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35455.0</td>\n",
       "      <td>36468.0</td>\n",
       "      <td>564241.0</td>\n",
       "      <td>-1013.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>YOSHTLYNYOSH</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>35455.0</td>\n",
       "      <td>-1.047577e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>1574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23299.0</td>\n",
       "      <td>19247.0</td>\n",
       "      <td>558163.0</td>\n",
       "      <td>4052.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>YOSHTLYNYOSH</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>23299.0</td>\n",
       "      <td>1.361640e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>1574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29377.0</td>\n",
       "      <td>16208.0</td>\n",
       "      <td>557150.0</td>\n",
       "      <td>13169.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>YOSHTLYNYOSH</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>29377.0</td>\n",
       "      <td>-2.417666e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>1574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105352.0</td>\n",
       "      <td>115482.0</td>\n",
       "      <td>526760.0</td>\n",
       "      <td>-10130.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>YOSHTLYNYOSH</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>105352.0</td>\n",
       "      <td>4.628278e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4370 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sku_coded  starting_inventory     sellin   sellout  onhand_inventory  \\\n",
       "0            74           6462940.0  1177106.0  473071.0         3553604.0   \n",
       "1            74           5217963.0   968428.0  448759.0         3381394.0   \n",
       "2            74           4785412.0   470032.0  552085.0         3310484.0   \n",
       "3            74           4423771.0   400135.0  529799.0         2860712.0   \n",
       "4          1223                 0.0    39507.0  175249.0         1116326.0   \n",
       "...         ...                 ...        ...       ...               ...   \n",
       "4365       3301                 0.0     2026.0    1013.0            2026.0   \n",
       "4366       1574                 0.0    35455.0   36468.0          564241.0   \n",
       "4367       1574                 0.0    23299.0   19247.0          558163.0   \n",
       "4368       1574                 0.0    29377.0   16208.0          557150.0   \n",
       "4369       1574                 0.0   105352.0  115482.0          526760.0   \n",
       "\n",
       "      leftover_inventory  price      sku_name  month  year  idx     y_true  \\\n",
       "0               704035.0  145.0   ABEAHAMASHL      3  2016    0  1177106.0   \n",
       "1               519669.0  145.0   ABEAHAMASHL      4  2016    1   968428.0   \n",
       "2               -82053.0  145.0   ABEAHAMASHL      5  2016    2   470032.0   \n",
       "3              -129664.0  145.0   ABEAHAMASHL      6  2016    3   400135.0   \n",
       "4              -135742.0  125.0    ABEANHARLE      3  2016    0    39507.0   \n",
       "...                  ...    ...           ...    ...   ...  ...        ...   \n",
       "4365              1013.0  135.0  YOSHTLEYMICH      6  2016    1     2026.0   \n",
       "4366             -1013.0  245.0  YOSHTLYNYOSH      3  2016    0    35455.0   \n",
       "4367              4052.0  245.0  YOSHTLYNYOSH      4  2016    1    23299.0   \n",
       "4368             13169.0  245.0  YOSHTLYNYOSH      5  2016    2    29377.0   \n",
       "4369            -10130.0  245.0  YOSHTLYNYOSH      6  2016    3   105352.0   \n",
       "\n",
       "            y_pred  \n",
       "0     3.350403e+06  \n",
       "1     3.330252e+06  \n",
       "2     2.681489e+06  \n",
       "3     2.456778e+06  \n",
       "4    -8.503907e+04  \n",
       "...            ...  \n",
       "4365 -2.014215e+04  \n",
       "4366 -1.047577e+05  \n",
       "4367  1.361640e+04  \n",
       "4368 -2.417666e+04  \n",
       "4369  4.628278e+04  \n",
       "\n",
       "[4370 rows x 13 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sk5qDw7Iu_bt",
    "outputId": "c519a09f-100a-4256-df1a-55bea9334f53",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 1 from 1/2016 to 1/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0350\tVal Loss: 0.0355\tVal MAE: 442838.31\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0225\tVal Loss: 0.0220\tVal MAE: 297948.53\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0152\tVal Loss: 0.0157\tVal MAE: 231307.07\telapsed: 0.03 mins\n",
      "Epoch 20: Train Loss: 0.0106\tVal Loss: 0.0116\tVal MAE: 190676.41\telapsed: 0.04 mins\n",
      "Epoch 25: Train Loss: 0.0078\tVal Loss: 0.0091\tVal MAE: 171576.92\telapsed: 0.05 mins\n",
      "Epoch 30: Train Loss: 0.0060\tVal Loss: 0.0074\tVal MAE: 155902.98\telapsed: 0.06 mins\n",
      "Epoch 35: Train Loss: 0.0047\tVal Loss: 0.0064\tVal MAE: 147593.07\telapsed: 0.07 mins\n",
      "Epoch 40: Train Loss: 0.0039\tVal Loss: 0.0056\tVal MAE: 141476.70\telapsed: 0.08 mins\n",
      "Epoch 45: Train Loss: 0.0034\tVal Loss: 0.0051\tVal MAE: 138246.45\telapsed: 0.09 mins\n",
      "Epoch 50: Train Loss: 0.0030\tVal Loss: 0.0048\tVal MAE: 135374.86\telapsed: 0.10 mins\n",
      "Epoch 55: Train Loss: 0.0027\tVal Loss: 0.0046\tVal MAE: 133744.58\telapsed: 0.11 mins\n",
      "Epoch 60: Train Loss: 0.0026\tVal Loss: 0.0044\tVal MAE: 133192.90\telapsed: 0.12 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 60\tVal loss: 0.0044\tVal MAE: 133418.16\tTotal time elapsed: 0.12 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 2 from 1/2016 to 2/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0235\tVal Loss: 0.0226\tVal MAE: 310516.56\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0118\tVal Loss: 0.0119\tVal MAE: 202079.40\telapsed: 0.04 mins\n",
      "Epoch 15: Train Loss: 0.0071\tVal Loss: 0.0076\tVal MAE: 160182.77\telapsed: 0.05 mins\n",
      "Epoch 20: Train Loss: 0.0050\tVal Loss: 0.0058\tVal MAE: 144180.23\telapsed: 0.07 mins\n",
      "Epoch 25: Train Loss: 0.0042\tVal Loss: 0.0049\tVal MAE: 136404.69\telapsed: 0.09 mins\n",
      "Epoch 30: Train Loss: 0.0037\tVal Loss: 0.0045\tVal MAE: 135199.74\telapsed: 0.11 mins\n",
      "Epoch 35: Train Loss: 0.0035\tVal Loss: 0.0043\tVal MAE: 133648.08\telapsed: 0.13 mins\n",
      "Epoch 40: Train Loss: 0.0033\tVal Loss: 0.0042\tVal MAE: 131910.54\telapsed: 0.15 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 37\tVal loss: 0.0042\tVal MAE: 131910.54\tTotal time elapsed: 0.15 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 3 from 1/2016 to 3/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0341\tVal Loss: 0.0313\tVal MAE: 417785.32\telapsed: 0.03 mins\n",
      "Epoch 10: Train Loss: 0.0116\tVal Loss: 0.0112\tVal MAE: 201815.09\telapsed: 0.05 mins\n",
      "Epoch 15: Train Loss: 0.0060\tVal Loss: 0.0061\tVal MAE: 159869.12\telapsed: 0.08 mins\n",
      "Epoch 20: Train Loss: 0.0043\tVal Loss: 0.0046\tVal MAE: 148350.13\telapsed: 0.11 mins\n",
      "Epoch 25: Train Loss: 0.0038\tVal Loss: 0.0042\tVal MAE: 144509.46\telapsed: 0.14 mins\n",
      "Epoch 30: Train Loss: 0.0037\tVal Loss: 0.0040\tVal MAE: 143531.98\telapsed: 0.16 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 29\tVal loss: 0.0040\tVal MAE: 143220.83\tTotal time elapsed: 0.17 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 4 from 1/2016 to 4/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0145\tVal Loss: 0.0142\tVal MAE: 247765.27\telapsed: 0.04 mins\n",
      "Epoch 10: Train Loss: 0.0058\tVal Loss: 0.0064\tVal MAE: 173131.61\telapsed: 0.07 mins\n",
      "Epoch 15: Train Loss: 0.0041\tVal Loss: 0.0048\tVal MAE: 163187.74\telapsed: 0.11 mins\n",
      "Epoch 20: Train Loss: 0.0037\tVal Loss: 0.0044\tVal MAE: 157120.43\telapsed: 0.14 mins\n",
      "Epoch 25: Train Loss: 0.0035\tVal Loss: 0.0042\tVal MAE: 156329.16\telapsed: 0.18 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 24\tVal loss: 0.0042\tVal MAE: 155326.46\tTotal time elapsed: 0.19 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 5 from 1/2016 to 5/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0129\tVal Loss: 0.0125\tVal MAE: 235187.18\telapsed: 0.04 mins\n",
      "Epoch 10: Train Loss: 0.0051\tVal Loss: 0.0052\tVal MAE: 177188.80\telapsed: 0.09 mins\n",
      "Epoch 15: Train Loss: 0.0041\tVal Loss: 0.0043\tVal MAE: 169268.81\telapsed: 0.13 mins\n",
      "Epoch 20: Train Loss: 0.0039\tVal Loss: 0.0041\tVal MAE: 168048.36\telapsed: 0.18 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 19\tVal loss: 0.0041\tVal MAE: 167752.53\tTotal time elapsed: 0.20 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 6 from 1/2016 to 6/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0095\tVal Loss: 0.0094\tVal MAE: 221669.33\telapsed: 0.05 mins\n",
      "Epoch 10: Train Loss: 0.0045\tVal Loss: 0.0050\tVal MAE: 184824.43\telapsed: 0.11 mins\n",
      "Epoch 15: Train Loss: 0.0040\tVal Loss: 0.0045\tVal MAE: 181762.51\telapsed: 0.16 mins\n",
      "Epoch 20: Train Loss: 0.0039\tVal Loss: 0.0044\tVal MAE: 180006.13\telapsed: 0.21 mins\n",
      "Epoch 25: Train Loss: 0.0037\tVal Loss: 0.0042\tVal MAE: 178189.57\telapsed: 0.27 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 23\tVal loss: 0.0042\tVal MAE: 178456.36\tTotal time elapsed: 0.28 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 7 from 1/2016 to 7/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0078\tVal Loss: 0.0079\tVal MAE: 226678.78\telapsed: 0.06 mins\n",
      "Epoch 10: Train Loss: 0.0044\tVal Loss: 0.0048\tVal MAE: 197087.23\telapsed: 0.12 mins\n",
      "Epoch 15: Train Loss: 0.0041\tVal Loss: 0.0046\tVal MAE: 195023.85\telapsed: 0.19 mins\n",
      "Epoch 20: Train Loss: 0.0040\tVal Loss: 0.0044\tVal MAE: 193225.06\telapsed: 0.25 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 19\tVal loss: 0.0044\tVal MAE: 193824.76\tTotal time elapsed: 0.27 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 8 from 1/2016 to 8/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0072\tVal Loss: 0.0071\tVal MAE: 252505.37\telapsed: 0.07 mins\n",
      "Epoch 10: Train Loss: 0.0049\tVal Loss: 0.0050\tVal MAE: 230555.27\telapsed: 0.14 mins\n",
      "Epoch 15: Train Loss: 0.0045\tVal Loss: 0.0046\tVal MAE: 226869.44\telapsed: 0.21 mins\n",
      "Epoch 20: Train Loss: 0.0043\tVal Loss: 0.0044\tVal MAE: 225359.29\telapsed: 0.28 mins\n",
      "Epoch 25: Train Loss: 0.0041\tVal Loss: 0.0042\tVal MAE: 222479.44\telapsed: 0.35 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 23\tVal loss: 0.0042\tVal MAE: 222666.10\tTotal time elapsed: 0.37 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 9 from 1/2016 to 9/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0060\tVal Loss: 0.0060\tVal MAE: 249893.61\telapsed: 0.09 mins\n",
      "Epoch 10: Train Loss: 0.0046\tVal Loss: 0.0048\tVal MAE: 240631.94\telapsed: 0.17 mins\n",
      "Epoch 15: Train Loss: 0.0044\tVal Loss: 0.0045\tVal MAE: 237609.29\telapsed: 0.26 mins\n",
      "Epoch 20: Train Loss: 0.0041\tVal Loss: 0.0043\tVal MAE: 235773.00\telapsed: 0.35 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 20\tVal loss: 0.0043\tVal MAE: 233494.72\tTotal time elapsed: 0.41 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 10 from 1/2016 to 10/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0056\tVal Loss: 0.0054\tVal MAE: 237752.44\telapsed: 0.10 mins\n",
      "Epoch 10: Train Loss: 0.0046\tVal Loss: 0.0045\tVal MAE: 231669.35\telapsed: 0.21 mins\n",
      "Epoch 15: Train Loss: 0.0044\tVal Loss: 0.0043\tVal MAE: 226959.63\telapsed: 0.31 mins\n",
      "Epoch 20: Train Loss: 0.0041\tVal Loss: 0.0041\tVal MAE: 226799.48\telapsed: 0.42 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 18\tVal loss: 0.0041\tVal MAE: 225788.68\tTotal time elapsed: 0.44 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 11 from 1/2016 to 11/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0052\tVal Loss: 0.0050\tVal MAE: 227704.52\telapsed: 0.11 mins\n",
      "Epoch 10: Train Loss: 0.0044\tVal Loss: 0.0042\tVal MAE: 219033.01\telapsed: 0.22 mins\n",
      "Epoch 15: Train Loss: 0.0042\tVal Loss: 0.0040\tVal MAE: 216388.64\telapsed: 0.32 mins\n",
      "Epoch 20: Train Loss: 0.0041\tVal Loss: 0.0039\tVal MAE: 215666.22\telapsed: 0.42 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 18\tVal loss: 0.0038\tVal MAE: 215684.43\tTotal time elapsed: 0.44 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 12 from 1/2016 to 12/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0049\tVal Loss: 0.0047\tVal MAE: 220987.67\telapsed: 0.11 mins\n",
      "Epoch 10: Train Loss: 0.0042\tVal Loss: 0.0041\tVal MAE: 210269.21\telapsed: 0.22 mins\n",
      "Epoch 15: Train Loss: 0.0039\tVal Loss: 0.0038\tVal MAE: 211044.07\telapsed: 0.33 mins\n",
      "Epoch 20: Train Loss: 0.0038\tVal Loss: 0.0037\tVal MAE: 206750.58\telapsed: 0.43 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 18\tVal loss: 0.0037\tVal MAE: 207820.75\tTotal time elapsed: 0.46 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 13 from 1/2016 to 1/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0044\tVal Loss: 0.0043\tVal MAE: 214381.40\telapsed: 0.12 mins\n",
      "Epoch 10: Train Loss: 0.0039\tVal Loss: 0.0038\tVal MAE: 208446.24\telapsed: 0.23 mins\n",
      "Epoch 15: Train Loss: 0.0037\tVal Loss: 0.0036\tVal MAE: 207160.61\telapsed: 0.35 mins\n",
      "Epoch 20: Train Loss: 0.0036\tVal Loss: 0.0035\tVal MAE: 203626.38\telapsed: 0.46 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 17\tVal loss: 0.0035\tVal MAE: 203626.38\tTotal time elapsed: 0.46 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 14 from 1/2016 to 2/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0042\tVal Loss: 0.0041\tVal MAE: 208805.36\telapsed: 0.12 mins\n",
      "Epoch 10: Train Loss: 0.0038\tVal Loss: 0.0037\tVal MAE: 202304.43\telapsed: 0.26 mins\n",
      "Epoch 15: Train Loss: 0.0036\tVal Loss: 0.0034\tVal MAE: 198903.32\telapsed: 0.39 mins\n",
      "Epoch 20: Train Loss: 0.0034\tVal Loss: 0.0033\tVal MAE: 199313.11\telapsed: 0.53 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 18\tVal loss: 0.0033\tVal MAE: 198178.35\tTotal time elapsed: 0.56 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 15 from 1/2016 to 3/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0041\tVal Loss: 0.0041\tVal MAE: 198780.63\telapsed: 0.15 mins\n",
      "Epoch 10: Train Loss: 0.0036\tVal Loss: 0.0036\tVal MAE: 191479.09\telapsed: 0.28 mins\n",
      "Epoch 15: Train Loss: 0.0034\tVal Loss: 0.0034\tVal MAE: 191748.50\telapsed: 0.42 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 13\tVal loss: 0.0034\tVal MAE: 188091.82\tTotal time elapsed: 0.44 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 16 from 1/2016 to 4/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0040\tVal Loss: 0.0054\tVal MAE: 250816.64\telapsed: 0.14 mins\n",
      "Epoch 10: Train Loss: 0.0035\tVal Loss: 0.0050\tVal MAE: 245025.53\telapsed: 0.29 mins\n",
      "Epoch 15: Train Loss: 0.0033\tVal Loss: 0.0048\tVal MAE: 242878.11\telapsed: 0.43 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 16\tVal loss: 0.0047\tVal MAE: 240835.43\tTotal time elapsed: 0.55 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 17 from 1/2016 to 5/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0054\tVal Loss: 0.0056\tVal MAE: 282322.85\telapsed: 0.15 mins\n",
      "Epoch 10: Train Loss: 0.0050\tVal Loss: 0.0051\tVal MAE: 274029.86\telapsed: 0.31 mins\n",
      "Epoch 15: Train Loss: 0.0048\tVal Loss: 0.0050\tVal MAE: 274552.20\telapsed: 0.46 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0050\tVal MAE: 274552.20\tTotal time elapsed: 0.46 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 18 from 1/2016 to 6/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0056\tVal Loss: 0.0059\tVal MAE: 288389.87\telapsed: 0.16 mins\n",
      "Epoch 10: Train Loss: 0.0051\tVal Loss: 0.0055\tVal MAE: 282820.86\telapsed: 0.33 mins\n",
      "Epoch 15: Train Loss: 0.0050\tVal Loss: 0.0053\tVal MAE: 280950.55\telapsed: 0.49 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 13\tVal loss: 0.0053\tVal MAE: 280431.17\tTotal time elapsed: 0.52 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 19 from 1/2016 to 7/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0060\tVal Loss: 0.0065\tVal MAE: 341642.15\telapsed: 0.18 mins\n",
      "Epoch 10: Train Loss: 0.0055\tVal Loss: 0.0061\tVal MAE: 338214.84\telapsed: 0.35 mins\n",
      "Epoch 15: Train Loss: 0.0053\tVal Loss: 0.0059\tVal MAE: 334663.59\telapsed: 0.52 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 16\tVal loss: 0.0058\tVal MAE: 334603.42\tTotal time elapsed: 0.66 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 20 from 1/2016 to 8/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0064\tVal Loss: 0.0052\tVal MAE: 284711.65\telapsed: 0.18 mins\n",
      "Epoch 10: Train Loss: 0.0061\tVal Loss: 0.0047\tVal MAE: 275927.26\telapsed: 0.37 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 11\tVal loss: 0.0046\tVal MAE: 276657.91\tTotal time elapsed: 0.51 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 21 from 1/2016 to 9/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0051\tVal Loss: 0.0050\tVal MAE: 284398.63\telapsed: 0.19 mins\n",
      "Epoch 10: Train Loss: 0.0047\tVal Loss: 0.0046\tVal MAE: 276537.48\telapsed: 0.39 mins\n",
      "Epoch 15: Train Loss: 0.0045\tVal Loss: 0.0044\tVal MAE: 277200.08\telapsed: 0.60 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 13\tVal loss: 0.0044\tVal MAE: 276284.78\tTotal time elapsed: 0.64 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 22 from 1/2016 to 10/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0049\tVal Loss: 0.0046\tVal MAE: 274452.33\telapsed: 0.22 mins\n",
      "Epoch 10: Train Loss: 0.0045\tVal Loss: 0.0042\tVal MAE: 272764.87\telapsed: 0.42 mins\n",
      "Epoch 15: Train Loss: 0.0044\tVal Loss: 0.0041\tVal MAE: 271290.04\telapsed: 0.62 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0041\tVal MAE: 271290.04\tTotal time elapsed: 0.62 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 23 from 1/2016 to 11/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0047\tVal Loss: 0.0042\tVal MAE: 253949.39\telapsed: 0.21 mins\n",
      "Epoch 10: Train Loss: 0.0042\tVal Loss: 0.0037\tVal MAE: 250331.19\telapsed: 0.42 mins\n",
      "Epoch 15: Train Loss: 0.0041\tVal Loss: 0.0036\tVal MAE: 249915.79\telapsed: 0.64 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0036\tVal MAE: 249915.79\tTotal time elapsed: 0.64 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 24 from 1/2016 to 12/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0041\tVal Loss: 0.0040\tVal MAE: 239556.93\telapsed: 0.22 mins\n",
      "Epoch 10: Train Loss: 0.0037\tVal Loss: 0.0036\tVal MAE: 237119.32\telapsed: 0.44 mins\n",
      "Epoch 15: Train Loss: 0.0036\tVal Loss: 0.0035\tVal MAE: 231681.48\telapsed: 0.66 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0035\tVal MAE: 231681.48\tTotal time elapsed: 0.66 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 25 from 1/2016 to 1/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0040\tVal Loss: 0.0039\tVal MAE: 215433.56\telapsed: 0.24 mins\n",
      "Epoch 10: Train Loss: 0.0036\tVal Loss: 0.0035\tVal MAE: 209984.53\telapsed: 0.47 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0035\tVal MAE: 208150.27\tTotal time elapsed: 0.62 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 26 from 1/2016 to 2/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0039\tVal Loss: 0.0039\tVal MAE: 218333.68\telapsed: 0.25 mins\n",
      "Epoch 10: Train Loss: 0.0035\tVal Loss: 0.0035\tVal MAE: 211956.77\telapsed: 0.49 mins\n",
      "Epoch 15: Train Loss: 0.0034\tVal Loss: 0.0034\tVal MAE: 211562.52\telapsed: 0.74 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0034\tVal MAE: 211562.52\tTotal time elapsed: 0.74 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 27 from 1/2016 to 3/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0039\tVal Loss: 0.0038\tVal MAE: 208045.57\telapsed: 0.26 mins\n",
      "Epoch 10: Train Loss: 0.0035\tVal Loss: 0.0035\tVal MAE: 205445.67\telapsed: 0.51 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0035\tVal MAE: 202905.55\tTotal time elapsed: 0.56 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 28 from 1/2016 to 4/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0039\tVal Loss: 0.0039\tVal MAE: 212936.10\telapsed: 0.26 mins\n",
      "Epoch 10: Train Loss: 0.0035\tVal Loss: 0.0035\tVal MAE: 205436.22\telapsed: 0.52 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0035\tVal MAE: 205229.15\tTotal time elapsed: 0.67 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 29 from 1/2016 to 5/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0039\tVal Loss: 0.0039\tVal MAE: 235952.61\telapsed: 0.28 mins\n",
      "Epoch 10: Train Loss: 0.0036\tVal Loss: 0.0035\tVal MAE: 224279.63\telapsed: 0.55 mins\n",
      "Epoch 15: Train Loss: 0.0034\tVal Loss: 0.0034\tVal MAE: 226271.50\telapsed: 0.82 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 13\tVal loss: 0.0034\tVal MAE: 224222.54\tTotal time elapsed: 0.87 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 30 from 1/2016 to 6/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0040\tVal Loss: 0.0041\tVal MAE: 241389.00\telapsed: 0.28 mins\n",
      "Epoch 10: Train Loss: 0.0035\tVal Loss: 0.0036\tVal MAE: 234418.36\telapsed: 0.56 mins\n",
      "Epoch 15: Train Loss: 0.0034\tVal Loss: 0.0035\tVal MAE: 237129.87\telapsed: 0.84 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0035\tVal MAE: 237129.87\tTotal time elapsed: 0.84 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 31 from 1/2016 to 7/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0039\tVal Loss: 0.0039\tVal MAE: 234149.02\telapsed: 0.29 mins\n",
      "Epoch 10: Train Loss: 0.0035\tVal Loss: 0.0035\tVal MAE: 229926.15\telapsed: 0.58 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0035\tVal MAE: 229508.02\tTotal time elapsed: 0.75 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 32 from 1/2016 to 8/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0038\tVal Loss: 0.0038\tVal MAE: 234664.53\telapsed: 0.30 mins\n",
      "Epoch 10: Train Loss: 0.0035\tVal Loss: 0.0035\tVal MAE: 232258.50\telapsed: 0.60 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 11\tVal loss: 0.0034\tVal MAE: 230370.11\tTotal time elapsed: 0.84 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 33 from 1/2016 to 9/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0038\tVal Loss: 0.0037\tVal MAE: 237410.63\telapsed: 0.31 mins\n",
      "Epoch 10: Train Loss: 0.0035\tVal Loss: 0.0034\tVal MAE: 233250.17\telapsed: 0.62 mins\n",
      "Epoch 15: Train Loss: 0.0034\tVal Loss: 0.0033\tVal MAE: 235135.37\telapsed: 0.92 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0033\tVal MAE: 235135.37\tTotal time elapsed: 0.92 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 34 from 1/2016 to 10/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0037\tVal Loss: 0.0036\tVal MAE: 218246.42\telapsed: 0.32 mins\n",
      "Epoch 10: Train Loss: 0.0034\tVal Loss: 0.0033\tVal MAE: 216273.98\telapsed: 0.64 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0033\tVal MAE: 215350.08\tTotal time elapsed: 0.77 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 35 from 1/2016 to 11/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0036\tVal Loss: 0.0035\tVal MAE: 222212.58\telapsed: 0.33 mins\n",
      "Epoch 10: Train Loss: 0.0033\tVal Loss: 0.0032\tVal MAE: 221016.35\telapsed: 0.67 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0032\tVal MAE: 221590.75\tTotal time elapsed: 0.74 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 36 from 1/2016 to 12/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0035\tVal Loss: 0.0034\tVal MAE: 209353.11\telapsed: 0.34 mins\n",
      "Epoch 10: Train Loss: 0.0032\tVal Loss: 0.0031\tVal MAE: 209687.62\telapsed: 0.69 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0031\tVal MAE: 209592.27\tTotal time elapsed: 0.82 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 37 from 1/2016 to 1/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0034\tVal Loss: 0.0034\tVal MAE: 192606.34\telapsed: 0.35 mins\n",
      "Epoch 10: Train Loss: 0.0031\tVal Loss: 0.0031\tVal MAE: 194593.72\telapsed: 0.70 mins\n",
      "Epoch 15: Train Loss: 0.0030\tVal Loss: 0.0030\tVal MAE: 193549.75\telapsed: 1.05 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0030\tVal MAE: 193549.75\tTotal time elapsed: 1.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 38 from 1/2016 to 2/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0034\tVal Loss: 0.0033\tVal MAE: 190772.15\telapsed: 0.36 mins\n",
      "Epoch 10: Train Loss: 0.0031\tVal Loss: 0.0030\tVal MAE: 195403.92\telapsed: 0.73 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 11\tVal loss: 0.0029\tVal MAE: 193880.06\tTotal time elapsed: 1.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 39 from 1/2016 to 3/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0033\tVal Loss: 0.0033\tVal MAE: 185866.54\telapsed: 0.37 mins\n",
      "Epoch 10: Train Loss: 0.0030\tVal Loss: 0.0030\tVal MAE: 190824.97\telapsed: 0.74 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 11\tVal loss: 0.0029\tVal MAE: 190758.94\tTotal time elapsed: 1.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 40 from 1/2016 to 4/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0032\tVal Loss: 0.0034\tVal MAE: 191615.26\telapsed: 0.38 mins\n",
      "Epoch 10: Train Loss: 0.0029\tVal Loss: 0.0031\tVal MAE: 194225.05\telapsed: 0.76 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0031\tVal MAE: 193900.08\tTotal time elapsed: 0.92 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 41 from 1/2016 to 5/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0034\tVal Loss: 0.0034\tVal MAE: 232049.51\telapsed: 0.39 mins\n",
      "Epoch 10: Train Loss: 0.0031\tVal Loss: 0.0031\tVal MAE: 234671.84\telapsed: 0.79 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0031\tVal MAE: 233545.12\tTotal time elapsed: 1.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 42 from 1/2016 to 6/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0034\tVal Loss: 0.0034\tVal MAE: 232550.07\telapsed: 0.40 mins\n",
      "Epoch 10: Train Loss: 0.0031\tVal Loss: 0.0031\tVal MAE: 229974.65\telapsed: 0.80 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0031\tVal MAE: 229184.45\tTotal time elapsed: 1.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 43 from 1/2016 to 7/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0034\tVal Loss: 0.0036\tVal MAE: 227339.10\telapsed: 0.41 mins\n",
      "Epoch 10: Train Loss: 0.0032\tVal Loss: 0.0033\tVal MAE: 227334.38\telapsed: 0.82 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0033\tVal MAE: 225572.39\tTotal time elapsed: 1.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 44 from 1/2016 to 8/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0035\tVal Loss: 0.0034\tVal MAE: 231323.28\telapsed: 0.42 mins\n",
      "Epoch 10: Train Loss: 0.0033\tVal Loss: 0.0032\tVal MAE: 229121.70\telapsed: 0.84 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0032\tVal MAE: 228603.34\tTotal time elapsed: 1.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 45 from 1/2016 to 9/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0034\tVal Loss: 0.0034\tVal MAE: 194535.43\telapsed: 0.43 mins\n",
      "Epoch 10: Train Loss: 0.0032\tVal Loss: 0.0032\tVal MAE: 195286.43\telapsed: 0.85 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 11\tVal loss: 0.0031\tVal MAE: 193570.50\tTotal time elapsed: 1.20 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 46 from 1/2016 to 10/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0035\tVal Loss: 0.0035\tVal MAE: 198149.04\telapsed: 0.43 mins\n",
      "Epoch 10: Train Loss: 0.0032\tVal Loss: 0.0032\tVal MAE: 198680.94\telapsed: 0.87 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0032\tVal MAE: 198225.41\tTotal time elapsed: 1.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 47 from 1/2016 to 11/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0034\tVal Loss: 0.0032\tVal MAE: 191363.27\telapsed: 0.45 mins\n",
      "Epoch 10: Train Loss: 0.0032\tVal Loss: 0.0030\tVal MAE: 194018.76\telapsed: 0.89 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0030\tVal MAE: 196184.95\tTotal time elapsed: 0.98 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 48 from 1/2016 to 12/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0033\tVal Loss: 0.0032\tVal MAE: 184923.68\telapsed: 0.46 mins\n",
      "Epoch 10: Train Loss: 0.0030\tVal Loss: 0.0029\tVal MAE: 188535.23\telapsed: 0.91 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0029\tVal MAE: 189589.31\tTotal time elapsed: 1.09 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 49 from 1/2016 to 1/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0031\tVal Loss: 0.0031\tVal MAE: 170712.24\telapsed: 0.46 mins\n",
      "Epoch 10: Train Loss: 0.0029\tVal Loss: 0.0029\tVal MAE: 176370.95\telapsed: 0.93 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0029\tVal MAE: 176216.33\tTotal time elapsed: 1.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 50 from 1/2016 to 2/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0030\tVal Loss: 0.0030\tVal MAE: 159545.18\telapsed: 0.47 mins\n",
      "Epoch 10: Train Loss: 0.0028\tVal Loss: 0.0028\tVal MAE: 159618.40\telapsed: 0.95 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 7\tVal loss: 0.0028\tVal MAE: 159618.40\tTotal time elapsed: 0.95 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 51 from 1/2016 to 3/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0029\tVal Loss: 0.0029\tVal MAE: 148589.34\telapsed: 0.48 mins\n",
      "Epoch 10: Train Loss: 0.0028\tVal Loss: 0.0028\tVal MAE: 152482.48\telapsed: 0.97 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 7\tVal loss: 0.0028\tVal MAE: 152482.48\tTotal time elapsed: 0.97 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 52 from 1/2016 to 4/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0029\tVal Loss: 0.0031\tVal MAE: 172387.46\telapsed: 0.49 mins\n",
      "Epoch 10: Train Loss: 0.0028\tVal Loss: 0.0029\tVal MAE: 175699.61\telapsed: 0.98 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0029\tVal MAE: 173568.55\tTotal time elapsed: 1.18 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 53 from 1/2016 to 5/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0032\tVal Loss: 0.0032\tVal MAE: 186664.61\telapsed: 0.50 mins\n",
      "Epoch 10: Train Loss: 0.0030\tVal Loss: 0.0030\tVal MAE: 188040.42\telapsed: 1.00 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0030\tVal MAE: 189004.68\tTotal time elapsed: 1.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 54 from 1/2016 to 6/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0031\tVal Loss: 0.0032\tVal MAE: 110999.69\telapsed: 0.51 mins\n",
      "Epoch 10: Train Loss: 0.0030\tVal Loss: 0.0030\tVal MAE: 108639.04\telapsed: 1.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0030\tVal MAE: 108144.82\tTotal time elapsed: 1.24 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 55 from 1/2016 to 7/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0032\tVal Loss: 0.0032\tVal MAE: 91681.29\telapsed: 0.56 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 6\tVal loss: 0.0031\tVal MAE: 90135.79\tTotal time elapsed: 1.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 56 from 1/2016 to 8/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0032\tVal Loss: 0.0030\tVal MAE: 74296.10\telapsed: 0.58 mins\n",
      "Epoch 10: Train Loss: 0.0030\tVal Loss: 0.0028\tVal MAE: 72884.53\telapsed: 1.12 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0028\tVal MAE: 73088.78\tTotal time elapsed: 1.34 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 57 from 1/2016 to 9/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0032\tVal Loss: 0.0031\tVal MAE: 63378.93\telapsed: 0.54 mins\n",
      "Epoch 10: Train Loss: 0.0028\tVal Loss: 0.0028\tVal MAE: 61428.44\telapsed: 1.08 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 11\tVal loss: 0.0027\tVal MAE: 61087.28\tTotal time elapsed: 1.51 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Average Val loss: 0.0036\tAverage Val MAE: 202330.4096\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "window = 3\n",
    "expanding_window = True\n",
    "models, metrics_list, loss_list,  = [], [], []\n",
    "train_set, val_set = [], []\n",
    "\n",
    "dates = sorted([(m, y) for y,m in train_df.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "\n",
    "for ix in range(len(train_norm)):\n",
    "    if expanding_window:\n",
    "        print('Training step {} from {}/{} to {}/{}:'.format(ix+1, dates[0][0], dates[0][1], dates[ix][0], dates[ix][1]))\n",
    "        print('\\n')\n",
    "        train_set += [train_norm[ix]]\n",
    "        val_set += [val_norm[ix]]\n",
    "    else:\n",
    "        print('Training step {} from {}/{} to {}/{}:'.format(ix+1, dates[ix-window][0], dates[ix-window][1], dates[ix][0], dates[ix][1]))\n",
    "        print('\\n')\n",
    "        train_set = [train_norm[i] for i in range(ix-window,ix)] if ix>window else train_norm[:ix+1]\n",
    "        val_set = [val_norm[i] for i in range(ix-window,ix)] if ix>window else val_norm[:ix+1]\n",
    "        \n",
    "    train_dataset = FossilDataset(train_set, sku_encoder)\n",
    "    val_dataset = FossilDataset(val_set, sku_encoder, False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "    valid_loader = DataLoader(val_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    # model = pipe.model if ix>0 else FossilModel(len(features), 4, len(sku_encoder)+1, N_STEPS).double()\n",
    "    model = FossilEncoder(len(features), 4, len(sku_encoder)+1, N_STEPS, len(sellin_cols)).double()\n",
    "    pipe = Pipeline(train_loader, valid_loader, model, target_scaler)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    pipe.train_model(3, 5)\n",
    "    best_model, val_loss, metrics = pipe.model_checkpoints[-1]\n",
    "    \n",
    "    models.append(best_model)\n",
    "    loss_list.append(val_loss)\n",
    "    metrics_list.append(metrics)\n",
    "print('Average Val loss: {:.4f}\\tAverage Val MAE: {:.4f}'.format(np.mean(loss_list), np.mean(metrics_list)))\n",
    "print('-'*50)\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sk5qDw7Iu_bt",
    "outputId": "c519a09f-100a-4256-df1a-55bea9334f53",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 1 from 1/2016 to 1/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0442\tVal Loss: 0.0339\tVal MAE: 468770.82\telapsed: 0.03 mins\n",
      "Epoch 10: Train Loss: 0.0154\tVal Loss: 0.0170\tVal MAE: 285942.68\telapsed: 0.03 mins\n",
      "Epoch 15: Train Loss: 0.0105\tVal Loss: 0.0124\tVal MAE: 236815.89\telapsed: 0.03 mins\n",
      "Epoch 20: Train Loss: 0.0066\tVal Loss: 0.0098\tVal MAE: 212474.88\telapsed: 0.03 mins\n",
      "Epoch 25: Train Loss: 0.0058\tVal Loss: 0.0092\tVal MAE: 207472.53\telapsed: 0.04 mins\n",
      "Epoch 30: Train Loss: 0.0049\tVal Loss: 0.0077\tVal MAE: 193624.53\telapsed: 0.04 mins\n",
      "Epoch 35: Train Loss: 0.0035\tVal Loss: 0.0075\tVal MAE: 191328.87\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 33\tVal loss: 0.0072\tVal MAE: 188486.35\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 2 from 1/2016 to 2/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0166\tVal Loss: 0.0189\tVal MAE: 342315.13\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0091\tVal Loss: 0.0112\tVal MAE: 274349.47\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0063\tVal Loss: 0.0089\tVal MAE: 255959.13\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0051\tVal Loss: 0.0081\tVal MAE: 250198.19\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0049\tVal Loss: 0.0083\tVal MAE: 253151.51\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0046\tVal Loss: 0.0074\tVal MAE: 244866.31\telapsed: 0.03 mins\n",
      "Epoch 35: Train Loss: 0.0043\tVal Loss: 0.0075\tVal MAE: 246539.30\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 33\tVal loss: 0.0073\tVal MAE: 244488.39\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 3 from 1/2016 to 3/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0138\tVal Loss: 0.0131\tVal MAE: 281008.92\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0086\tVal Loss: 0.0104\tVal MAE: 253744.17\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0065\tVal Loss: 0.0073\tVal MAE: 230125.26\telapsed: 0.03 mins\n",
      "Epoch 20: Train Loss: 0.0057\tVal Loss: 0.0067\tVal MAE: 223953.81\telapsed: 0.04 mins\n",
      "Epoch 25: Train Loss: 0.0055\tVal Loss: 0.0065\tVal MAE: 223126.70\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 23\tVal loss: 0.0065\tVal MAE: 223236.66\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 4 from 1/2016 to 4/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0104\tVal Loss: 0.0100\tVal MAE: 244215.77\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0063\tVal Loss: 0.0063\tVal MAE: 214925.38\telapsed: 0.02 mins\n",
      "Epoch 15: Train Loss: 0.0062\tVal Loss: 0.0059\tVal MAE: 211301.36\telapsed: 0.03 mins\n",
      "Epoch 20: Train Loss: 0.0056\tVal Loss: 0.0055\tVal MAE: 205990.79\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 19\tVal loss: 0.0055\tVal MAE: 205817.04\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 5 from 1/2016 to 5/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0083\tVal Loss: 0.0140\tVal MAE: 289189.34\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0051\tVal Loss: 0.0115\tVal MAE: 263996.60\telapsed: 0.03 mins\n",
      "Epoch 15: Train Loss: 0.0048\tVal Loss: 0.0111\tVal MAE: 261416.48\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 15\tVal loss: 0.0111\tVal MAE: 261838.45\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 6 from 1/2016 to 6/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0123\tVal Loss: 0.0097\tVal MAE: 288630.64\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0107\tVal Loss: 0.0086\tVal MAE: 277764.93\telapsed: 0.03 mins\n",
      "Epoch 15: Train Loss: 0.0105\tVal Loss: 0.0082\tVal MAE: 276167.65\telapsed: 0.05 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 13\tVal loss: 0.0082\tVal MAE: 277425.36\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 7 from 1/2016 to 7/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0087\tVal Loss: 0.0078\tVal MAE: 255990.82\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0080\tVal Loss: 0.0068\tVal MAE: 245183.12\telapsed: 0.04 mins\n",
      "Epoch 15: Train Loss: 0.0079\tVal Loss: 0.0065\tVal MAE: 245209.53\telapsed: 0.05 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0065\tVal MAE: 245209.53\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 8 from 1/2016 to 8/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0070\tVal Loss: 0.0066\tVal MAE: 210816.94\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0061\tVal Loss: 0.0059\tVal MAE: 205490.25\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0059\tVal MAE: 205966.66\tTotal time elapsed: 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 9 from 1/2016 to 9/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0070\tVal Loss: 0.0066\tVal MAE: 221795.88\telapsed: 0.02 mins\n",
      "Epoch 10: Train Loss: 0.0056\tVal Loss: 0.0058\tVal MAE: 220368.78\telapsed: 0.05 mins\n",
      "Epoch 15: Train Loss: 0.0055\tVal Loss: 0.0057\tVal MAE: 217502.09\telapsed: 0.07 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0057\tVal MAE: 217502.09\tTotal time elapsed: 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 10 from 1/2016 to 10/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0067\tVal Loss: 0.0066\tVal MAE: 238134.02\telapsed: 0.03 mins\n",
      "Epoch 10: Train Loss: 0.0058\tVal Loss: 0.0062\tVal MAE: 234051.15\telapsed: 0.06 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0062\tVal MAE: 234193.76\tTotal time elapsed: 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 11 from 1/2016 to 11/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0062\tVal Loss: 0.0061\tVal MAE: 221582.45\telapsed: 0.03 mins\n",
      "Epoch 10: Train Loss: 0.0064\tVal Loss: 0.0061\tVal MAE: 217350.21\telapsed: 0.06 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0059\tVal MAE: 224400.86\tTotal time elapsed: 0.07 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 12 from 1/2016 to 12/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0061\tVal Loss: 0.0055\tVal MAE: 192439.79\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 5\tVal loss: 0.0055\tVal MAE: 192129.42\tTotal time elapsed: 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 13 from 1/2016 to 1/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0057\tVal Loss: 0.0063\tVal MAE: 233596.14\telapsed: 0.04 mins\n",
      "Epoch 10: Train Loss: 0.0053\tVal Loss: 0.0058\tVal MAE: 230707.28\telapsed: 0.07 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 8\tVal loss: 0.0058\tVal MAE: 233836.25\tTotal time elapsed: 0.08 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 14 from 1/2016 to 2/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0059\tVal Loss: 0.0061\tVal MAE: 235607.74\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 5\tVal loss: 0.0061\tVal MAE: 235479.41\tTotal time elapsed: 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 15 from 1/2016 to 3/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0060\tVal Loss: 0.0057\tVal MAE: 208222.13\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 6\tVal loss: 0.0056\tVal MAE: 208348.52\tTotal time elapsed: 0.08 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 16 from 1/2016 to 4/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0055\tVal Loss: 0.0055\tVal MAE: 193821.37\telapsed: 0.05 mins\n",
      "Epoch 10: Train Loss: 0.0054\tVal Loss: 0.0053\tVal MAE: 193457.84\telapsed: 0.09 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 9\tVal loss: 0.0053\tVal MAE: 192577.63\tTotal time elapsed: 0.11 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 17 from 1/2016 to 5/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0054\tVal Loss: 0.0061\tVal MAE: 226111.54\telapsed: 0.05 mins\n",
      "Epoch 10: Train Loss: 0.0052\tVal Loss: 0.0059\tVal MAE: 227995.95\telapsed: 0.10 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 7\tVal loss: 0.0059\tVal MAE: 227995.95\tTotal time elapsed: 0.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 18 from 1/2016 to 6/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0063\tVal Loss: 0.0054\tVal MAE: 69504.58\telapsed: 0.05 mins\n",
      "Epoch 10: Train Loss: 0.0059\tVal Loss: 0.0052\tVal MAE: 66713.73\telapsed: 0.10 mins\n",
      "Epoch 15: Train Loss: 0.0058\tVal Loss: 0.0052\tVal MAE: 66692.80\telapsed: 0.16 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0051\tVal MAE: 66692.80\tTotal time elapsed: 0.16 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Average Val loss: 0.0064\tAverage Val MAE: 215423.8673\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "window = 3\n",
    "expanding_window = True\n",
    "models, metrics_list, loss_list,  = [], [], []\n",
    "train_set, val_set = [], []\n",
    "\n",
    "dates = sorted([(m, y) for y,m in train_df.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "\n",
    "for ix in range(len(train_norm)):\n",
    "    if expanding_window:\n",
    "        print('Training step {} from {}/{} to {}/{}:'.format(ix+1, dates[0][0], dates[0][1], dates[ix][0], dates[ix][1]))\n",
    "        print('\\n')\n",
    "        train_set += [train_norm[ix]]\n",
    "        val_set += [val_norm[ix]]\n",
    "    else:\n",
    "        print('Training step {} from {}/{} to {}/{}:'.format(ix+1, dates[ix-window][0], dates[ix-window][1], dates[ix][0], dates[ix][1]))\n",
    "        print('\\n')\n",
    "        train_set = [train_norm[i] for i in range(ix-window,ix)] if ix>window else train_norm[:ix+1]\n",
    "        val_set = [val_norm[i] for i in range(ix-window,ix)] if ix>window else val_norm[:ix+1]\n",
    "        \n",
    "    train_dataset = FossilDataset(train_set, sku_encoder)\n",
    "    val_dataset = FossilDataset(val_set, sku_encoder, False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "    valid_loader = DataLoader(val_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    # model = pipe.model if ix>0 else FossilModel(len(features), 4, len(sku_encoder)+1, N_STEPS).double()\n",
    "    # model = FossilEncoder(len(features), 4, len(sku_encoder)+1, N_STEPS).double()\n",
    "    model = FossilDecoder(4, 10, 30, N_STEPS, len(sku_encoder)+1, len(features)).double()\n",
    "    # with torch.no_grad():\n",
    "    #     model.monthly_conv2d.weight.copy_(model_weights['monthly_conv2d.weight'])\n",
    "    #     model.periodic_conv2d.weight.copy_(model_weights['periodic_conv2d.weight'])  \n",
    "        \n",
    "    pipe = Pipeline(train_loader, valid_loader, model, target_scaler)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    pipe.train_model(3, 5)\n",
    "    best_model, val_loss, metrics = pipe.model_checkpoints[-1]\n",
    "    \n",
    "    models.append(best_model)\n",
    "    loss_list.append(val_loss)\n",
    "    metrics_list.append(metrics)\n",
    "print('Average Val loss: {:.4f}\\tAverage Val MAE: {:.4f}'.format(np.mean(loss_list), np.mean(metrics_list)))\n",
    "print('-'*50)\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sk5qDw7Iu_bt",
    "outputId": "c519a09f-100a-4256-df1a-55bea9334f53",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 1 from 8/2021 to 1/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0147\tVal Loss: 0.0167\tVal MAE: 283600.15\telapsed: 0.00 mins\n",
      "Epoch 10: Train Loss: 0.0064\tVal Loss: 0.0106\tVal MAE: 231188.11\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0032\tVal Loss: 0.0078\tVal MAE: 205438.50\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0031\tVal Loss: 0.0079\tVal MAE: 206882.41\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 17\tVal loss: 0.0075\tVal MAE: 206882.41\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 2 from 9/2021 to 2/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0219\tVal Loss: 0.0235\tVal MAE: 361228.78\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0077\tVal Loss: 0.0117\tVal MAE: 244032.32\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0036\tVal Loss: 0.0083\tVal MAE: 215847.62\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0025\tVal Loss: 0.0074\tVal MAE: 208127.34\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0021\tVal Loss: 0.0071\tVal MAE: 206162.36\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0071\tVal MAE: 205713.52\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 3 from 10/2021 to 3/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0234\tVal Loss: 0.0247\tVal MAE: 394697.05\telapsed: 0.00 mins\n",
      "Epoch 10: Train Loss: 0.0073\tVal Loss: 0.0119\tVal MAE: 265967.10\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0034\tVal Loss: 0.0086\tVal MAE: 239308.18\telapsed: 0.01 mins\n",
      "Epoch 20: Train Loss: 0.0023\tVal Loss: 0.0078\tVal MAE: 232274.66\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0021\tVal Loss: 0.0076\tVal MAE: 231281.36\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 22\tVal loss: 0.0076\tVal MAE: 231281.36\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 4 from 1/2016 to 4/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0187\tVal Loss: 0.0210\tVal MAE: 370859.81\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0067\tVal Loss: 0.0115\tVal MAE: 286833.13\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0033\tVal Loss: 0.0088\tVal MAE: 266626.85\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0025\tVal Loss: 0.0081\tVal MAE: 261286.70\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0022\tVal Loss: 0.0079\tVal MAE: 259623.10\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 23\tVal loss: 0.0079\tVal MAE: 259184.31\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 5 from 2/2016 to 5/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0224\tVal Loss: 0.0246\tVal MAE: 421286.75\telapsed: 0.00 mins\n",
      "Epoch 10: Train Loss: 0.0078\tVal Loss: 0.0128\tVal MAE: 317707.35\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0037\tVal Loss: 0.0094\tVal MAE: 291676.00\telapsed: 0.01 mins\n",
      "Epoch 20: Train Loss: 0.0025\tVal Loss: 0.0085\tVal MAE: 285374.44\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0022\tVal Loss: 0.0082\tVal MAE: 283130.76\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0082\tVal MAE: 283265.78\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 6 from 3/2016 to 6/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0247\tVal Loss: 0.0248\tVal MAE: 419202.85\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0096\tVal Loss: 0.0125\tVal MAE: 316941.03\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0064\tVal Loss: 0.0093\tVal MAE: 295240.42\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0055\tVal Loss: 0.0085\tVal MAE: 292571.72\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0053\tVal Loss: 0.0083\tVal MAE: 291981.75\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 24\tVal loss: 0.0083\tVal MAE: 293195.83\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 7 from 4/2016 to 7/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0191\tVal Loss: 0.0209\tVal MAE: 382232.72\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0077\tVal Loss: 0.0112\tVal MAE: 298620.54\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0046\tVal Loss: 0.0087\tVal MAE: 281019.02\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0038\tVal Loss: 0.0080\tVal MAE: 276284.33\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 20\tVal loss: 0.0080\tVal MAE: 277520.24\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 8 from 5/2016 to 8/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0161\tVal Loss: 0.0178\tVal MAE: 353763.15\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0073\tVal Loss: 0.0101\tVal MAE: 283701.94\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0050\tVal Loss: 0.0082\tVal MAE: 269835.68\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0043\tVal Loss: 0.0077\tVal MAE: 266497.89\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0042\tVal Loss: 0.0075\tVal MAE: 265227.67\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 24\tVal loss: 0.0075\tVal MAE: 265642.87\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 9 from 6/2016 to 9/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0197\tVal Loss: 0.0206\tVal MAE: 381971.41\telapsed: 0.00 mins\n",
      "Epoch 10: Train Loss: 0.0085\tVal Loss: 0.0107\tVal MAE: 291361.01\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0054\tVal Loss: 0.0083\tVal MAE: 277739.28\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0046\tVal Loss: 0.0075\tVal MAE: 270849.72\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0044\tVal Loss: 0.0074\tVal MAE: 270799.64\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 23\tVal loss: 0.0074\tVal MAE: 271129.91\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 10 from 7/2016 to 10/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0230\tVal Loss: 0.0230\tVal MAE: 415721.68\telapsed: 0.00 mins\n",
      "Epoch 10: Train Loss: 0.0113\tVal Loss: 0.0121\tVal MAE: 318538.94\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0074\tVal Loss: 0.0089\tVal MAE: 297356.59\telapsed: 0.01 mins\n",
      "Epoch 20: Train Loss: 0.0059\tVal Loss: 0.0076\tVal MAE: 291306.12\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0054\tVal Loss: 0.0072\tVal MAE: 289938.24\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0053\tVal Loss: 0.0071\tVal MAE: 289417.34\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 28\tVal loss: 0.0071\tVal MAE: 289214.08\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 11 from 8/2016 to 11/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0357\tVal Loss: 0.0323\tVal MAE: 493628.27\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0112\tVal Loss: 0.0126\tVal MAE: 326057.30\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0064\tVal Loss: 0.0086\tVal MAE: 303608.14\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0050\tVal Loss: 0.0074\tVal MAE: 293290.84\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0046\tVal Loss: 0.0071\tVal MAE: 293915.24\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 23\tVal loss: 0.0071\tVal MAE: 293304.41\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 12 from 9/2016 to 12/2016:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0231\tVal Loss: 0.0227\tVal MAE: 390474.74\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0110\tVal Loss: 0.0116\tVal MAE: 291712.22\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0071\tVal Loss: 0.0083\tVal MAE: 271839.13\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0059\tVal Loss: 0.0072\tVal MAE: 265416.49\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0056\tVal Loss: 0.0070\tVal MAE: 264401.37\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0055\tVal Loss: 0.0068\tVal MAE: 262433.82\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 29\tVal loss: 0.0068\tVal MAE: 264122.62\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 13 from 10/2016 to 1/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0238\tVal Loss: 0.0239\tVal MAE: 421523.73\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0098\tVal Loss: 0.0118\tVal MAE: 321552.43\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0061\tVal Loss: 0.0086\tVal MAE: 302510.14\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0051\tVal Loss: 0.0077\tVal MAE: 297103.33\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0048\tVal Loss: 0.0075\tVal MAE: 296106.94\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0047\tVal Loss: 0.0074\tVal MAE: 295958.20\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 29\tVal loss: 0.0073\tVal MAE: 295806.71\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 14 from 11/2016 to 2/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0234\tVal Loss: 0.0236\tVal MAE: 433557.97\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0093\tVal Loss: 0.0116\tVal MAE: 339232.75\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0055\tVal Loss: 0.0084\tVal MAE: 316858.11\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0045\tVal Loss: 0.0075\tVal MAE: 311480.03\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0041\tVal Loss: 0.0073\tVal MAE: 310953.87\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0040\tVal Loss: 0.0072\tVal MAE: 310919.20\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 29\tVal loss: 0.0071\tVal MAE: 308508.19\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 15 from 12/2016 to 3/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0238\tVal Loss: 0.0239\tVal MAE: 433935.58\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0091\tVal Loss: 0.0116\tVal MAE: 326123.69\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0054\tVal Loss: 0.0083\tVal MAE: 306089.35\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0043\tVal Loss: 0.0073\tVal MAE: 297903.75\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0039\tVal Loss: 0.0071\tVal MAE: 296978.73\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 26\tVal loss: 0.0070\tVal MAE: 294825.52\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 16 from 1/2017 to 4/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0152\tVal Loss: 0.0165\tVal MAE: 398805.26\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0067\tVal Loss: 0.0094\tVal MAE: 350870.97\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0047\tVal Loss: 0.0076\tVal MAE: 339614.42\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0041\tVal Loss: 0.0071\tVal MAE: 337435.52\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0039\tVal Loss: 0.0070\tVal MAE: 338267.08\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 22\tVal loss: 0.0070\tVal MAE: 338267.08\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 17 from 2/2017 to 5/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0197\tVal Loss: 0.0200\tVal MAE: 369906.84\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0080\tVal Loss: 0.0103\tVal MAE: 290043.09\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0047\tVal Loss: 0.0074\tVal MAE: 267080.36\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0039\tVal Loss: 0.0067\tVal MAE: 264243.78\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0036\tVal Loss: 0.0065\tVal MAE: 263792.91\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 24\tVal loss: 0.0065\tVal MAE: 264051.32\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 18 from 3/2017 to 6/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0198\tVal Loss: 0.0193\tVal MAE: 354672.30\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0087\tVal Loss: 0.0094\tVal MAE: 272175.55\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0058\tVal Loss: 0.0068\tVal MAE: 256317.17\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0050\tVal Loss: 0.0061\tVal MAE: 251984.15\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0047\tVal Loss: 0.0059\tVal MAE: 248894.04\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0059\tVal MAE: 250404.22\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 19 from 4/2017 to 7/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0259\tVal Loss: 0.0242\tVal MAE: 412508.58\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0097\tVal Loss: 0.0105\tVal MAE: 291706.19\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0058\tVal Loss: 0.0072\tVal MAE: 272847.03\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0047\tVal Loss: 0.0063\tVal MAE: 266574.46\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0044\tVal Loss: 0.0060\tVal MAE: 265835.04\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0043\tVal Loss: 0.0059\tVal MAE: 264544.11\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 27\tVal loss: 0.0059\tVal MAE: 264544.11\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 20 from 5/2017 to 8/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0285\tVal Loss: 0.0262\tVal MAE: 434906.60\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0106\tVal Loss: 0.0121\tVal MAE: 322204.67\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0062\tVal Loss: 0.0081\tVal MAE: 287734.28\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0048\tVal Loss: 0.0069\tVal MAE: 281500.52\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0044\tVal Loss: 0.0065\tVal MAE: 280392.87\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0042\tVal Loss: 0.0064\tVal MAE: 279806.61\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 27\tVal loss: 0.0064\tVal MAE: 279806.61\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 21 from 6/2017 to 9/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0345\tVal Loss: 0.0298\tVal MAE: 472853.28\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0120\tVal Loss: 0.0128\tVal MAE: 342458.69\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0067\tVal Loss: 0.0084\tVal MAE: 307301.65\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0051\tVal Loss: 0.0069\tVal MAE: 296898.45\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0047\tVal Loss: 0.0066\tVal MAE: 295759.23\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0045\tVal Loss: 0.0064\tVal MAE: 295052.59\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 29\tVal loss: 0.0064\tVal MAE: 295492.41\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 22 from 7/2017 to 10/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0238\tVal Loss: 0.0230\tVal MAE: 401544.38\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0097\tVal Loss: 0.0109\tVal MAE: 306155.84\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0059\tVal Loss: 0.0077\tVal MAE: 287502.03\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0049\tVal Loss: 0.0069\tVal MAE: 281979.91\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0046\tVal Loss: 0.0065\tVal MAE: 279543.09\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0065\tVal MAE: 279990.38\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 23 from 8/2017 to 11/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0245\tVal Loss: 0.0233\tVal MAE: 390088.31\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0090\tVal Loss: 0.0109\tVal MAE: 299165.49\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0052\tVal Loss: 0.0075\tVal MAE: 280649.88\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0042\tVal Loss: 0.0067\tVal MAE: 275259.79\telapsed: 0.03 mins\n",
      "Epoch 25: Train Loss: 0.0039\tVal Loss: 0.0065\tVal MAE: 273042.19\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 22\tVal loss: 0.0065\tVal MAE: 273042.19\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 24 from 9/2017 to 12/2017:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0201\tVal Loss: 0.0194\tVal MAE: 347629.35\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0084\tVal Loss: 0.0095\tVal MAE: 259344.27\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0053\tVal Loss: 0.0067\tVal MAE: 237410.15\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0045\tVal Loss: 0.0060\tVal MAE: 235495.90\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0042\tVal Loss: 0.0058\tVal MAE: 233804.56\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0058\tVal MAE: 232479.30\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 25 from 10/2017 to 1/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0241\tVal Loss: 0.0226\tVal MAE: 356778.62\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0088\tVal Loss: 0.0097\tVal MAE: 237385.17\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0052\tVal Loss: 0.0064\tVal MAE: 214146.06\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0041\tVal Loss: 0.0055\tVal MAE: 208799.01\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0039\tVal Loss: 0.0052\tVal MAE: 204799.19\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0052\tVal MAE: 206511.73\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 26 from 11/2017 to 2/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0236\tVal Loss: 0.0227\tVal MAE: 365842.39\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0081\tVal Loss: 0.0099\tVal MAE: 245404.66\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0043\tVal Loss: 0.0064\tVal MAE: 218324.63\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0033\tVal Loss: 0.0055\tVal MAE: 210915.81\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0030\tVal Loss: 0.0052\tVal MAE: 210296.02\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0052\tVal MAE: 210216.84\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 27 from 12/2017 to 3/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0235\tVal Loss: 0.0227\tVal MAE: 367327.86\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0082\tVal Loss: 0.0100\tVal MAE: 255990.47\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0046\tVal Loss: 0.0065\tVal MAE: 229093.49\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0035\tVal Loss: 0.0056\tVal MAE: 224629.95\telapsed: 0.03 mins\n",
      "Epoch 25: Train Loss: 0.0033\tVal Loss: 0.0054\tVal MAE: 223793.94\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0054\tVal MAE: 223690.06\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 28 from 1/2018 to 4/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0235\tVal Loss: 0.0227\tVal MAE: 368419.68\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0083\tVal Loss: 0.0100\tVal MAE: 257630.96\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0046\tVal Loss: 0.0065\tVal MAE: 228052.92\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0035\tVal Loss: 0.0055\tVal MAE: 220283.38\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0033\tVal Loss: 0.0053\tVal MAE: 219697.66\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0032\tVal Loss: 0.0053\tVal MAE: 220328.06\telapsed: 0.04 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 28\tVal loss: 0.0052\tVal MAE: 219764.68\tTotal time elapsed: 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 29 from 2/2018 to 5/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0340\tVal Loss: 0.0315\tVal MAE: 452089.03\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0095\tVal Loss: 0.0110\tVal MAE: 257466.35\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0047\tVal Loss: 0.0069\tVal MAE: 227569.06\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0033\tVal Loss: 0.0057\tVal MAE: 219920.32\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0028\tVal Loss: 0.0053\tVal MAE: 217440.74\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0053\tVal MAE: 217321.28\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 30 from 3/2018 to 6/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0237\tVal Loss: 0.0227\tVal MAE: 364968.33\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0084\tVal Loss: 0.0098\tVal MAE: 243014.82\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0047\tVal Loss: 0.0064\tVal MAE: 220596.37\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0037\tVal Loss: 0.0055\tVal MAE: 214972.39\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0034\tVal Loss: 0.0053\tVal MAE: 211996.76\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 22\tVal loss: 0.0053\tVal MAE: 211996.76\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 31 from 4/2018 to 7/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0195\tVal Loss: 0.0189\tVal MAE: 307487.10\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0079\tVal Loss: 0.0089\tVal MAE: 220501.04\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0048\tVal Loss: 0.0062\tVal MAE: 201127.91\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0039\tVal Loss: 0.0054\tVal MAE: 194402.66\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0037\tVal Loss: 0.0052\tVal MAE: 191622.73\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 26\tVal loss: 0.0051\tVal MAE: 193406.29\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 32 from 5/2018 to 8/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0148\tVal Loss: 0.0151\tVal MAE: 294607.50\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0063\tVal Loss: 0.0076\tVal MAE: 227517.66\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0042\tVal Loss: 0.0059\tVal MAE: 216856.36\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0037\tVal Loss: 0.0053\tVal MAE: 213352.79\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0035\tVal Loss: 0.0052\tVal MAE: 214094.67\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 23\tVal loss: 0.0052\tVal MAE: 213092.65\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 33 from 6/2018 to 9/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0219\tVal Loss: 0.0218\tVal MAE: 364479.75\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0092\tVal Loss: 0.0103\tVal MAE: 258743.54\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0053\tVal Loss: 0.0069\tVal MAE: 237128.76\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0041\tVal Loss: 0.0059\tVal MAE: 230758.23\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0038\tVal Loss: 0.0055\tVal MAE: 228502.95\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0055\tVal MAE: 228482.38\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 34 from 7/2018 to 10/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0243\tVal Loss: 0.0226\tVal MAE: 357513.14\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0088\tVal Loss: 0.0098\tVal MAE: 246175.33\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0052\tVal Loss: 0.0064\tVal MAE: 225873.77\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0042\tVal Loss: 0.0055\tVal MAE: 222854.22\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0039\tVal Loss: 0.0053\tVal MAE: 221418.50\telapsed: 0.03 mins\n",
      "Epoch 30: Train Loss: 0.0038\tVal Loss: 0.0052\tVal MAE: 219760.76\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 27\tVal loss: 0.0052\tVal MAE: 219760.76\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 35 from 8/2018 to 11/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0331\tVal Loss: 0.0287\tVal MAE: 425572.65\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0110\tVal Loss: 0.0117\tVal MAE: 269759.22\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0058\tVal Loss: 0.0073\tVal MAE: 237794.71\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0042\tVal Loss: 0.0059\tVal MAE: 227241.18\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0038\tVal Loss: 0.0056\tVal MAE: 225944.71\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 26\tVal loss: 0.0055\tVal MAE: 225257.80\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 36 from 9/2018 to 12/2018:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0149\tVal Loss: 0.0149\tVal MAE: 282761.12\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0064\tVal Loss: 0.0076\tVal MAE: 221587.14\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0054\tVal Loss: 0.0066\tVal MAE: 215428.25\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0041\tVal Loss: 0.0056\tVal MAE: 209140.77\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0037\tVal Loss: 0.0052\tVal MAE: 206058.12\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 26\tVal loss: 0.0051\tVal MAE: 205230.13\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 37 from 10/2018 to 1/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0149\tVal Loss: 0.0149\tVal MAE: 272645.83\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0063\tVal Loss: 0.0077\tVal MAE: 209202.72\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0058\tVal Loss: 0.0070\tVal MAE: 206959.21\telapsed: 0.02 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 12\tVal loss: 0.0067\tVal MAE: 206959.21\tTotal time elapsed: 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 38 from 11/2018 to 2/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0284\tVal Loss: 0.0268\tVal MAE: 425611.95\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0095\tVal Loss: 0.0109\tVal MAE: 271427.18\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0049\tVal Loss: 0.0069\tVal MAE: 238211.45\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0035\tVal Loss: 0.0059\tVal MAE: 232583.94\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0032\tVal Loss: 0.0056\tVal MAE: 228715.17\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 25\tVal loss: 0.0056\tVal MAE: 229739.50\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 39 from 12/2018 to 3/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0235\tVal Loss: 0.0226\tVal MAE: 376651.50\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0081\tVal Loss: 0.0098\tVal MAE: 263778.74\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0044\tVal Loss: 0.0064\tVal MAE: 240516.61\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0034\tVal Loss: 0.0055\tVal MAE: 232153.60\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0031\tVal Loss: 0.0054\tVal MAE: 232723.40\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 22\tVal loss: 0.0054\tVal MAE: 232723.40\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 40 from 1/2019 to 4/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0192\tVal Loss: 0.0190\tVal MAE: 338294.51\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0077\tVal Loss: 0.0092\tVal MAE: 258497.95\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0046\tVal Loss: 0.0065\tVal MAE: 238607.12\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0038\tVal Loss: 0.0058\tVal MAE: 233331.11\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0035\tVal Loss: 0.0056\tVal MAE: 231393.26\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 24\tVal loss: 0.0056\tVal MAE: 230770.21\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 41 from 2/2019 to 5/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0185\tVal Loss: 0.0187\tVal MAE: 343456.40\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0072\tVal Loss: 0.0089\tVal MAE: 261719.75\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0042\tVal Loss: 0.0066\tVal MAE: 245278.02\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0034\tVal Loss: 0.0059\tVal MAE: 239486.93\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0032\tVal Loss: 0.0057\tVal MAE: 237212.72\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 24\tVal loss: 0.0057\tVal MAE: 237797.47\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 42 from 3/2019 to 6/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0187\tVal Loss: 0.0186\tVal MAE: 308754.15\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0075\tVal Loss: 0.0087\tVal MAE: 220131.17\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0048\tVal Loss: 0.0062\tVal MAE: 202468.46\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0040\tVal Loss: 0.0056\tVal MAE: 195749.51\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0038\tVal Loss: 0.0054\tVal MAE: 196397.76\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 26\tVal loss: 0.0053\tVal MAE: 197045.15\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 43 from 4/2019 to 7/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0148\tVal Loss: 0.0150\tVal MAE: 282121.40\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0064\tVal Loss: 0.0078\tVal MAE: 226032.46\telapsed: 0.01 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 10\tVal loss: 0.0078\tVal MAE: 274491.58\tTotal time elapsed: 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 44 from 5/2019 to 8/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0187\tVal Loss: 0.0186\tVal MAE: 322814.39\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0075\tVal Loss: 0.0089\tVal MAE: 241804.54\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0048\tVal Loss: 0.0066\tVal MAE: 229360.43\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0040\tVal Loss: 0.0060\tVal MAE: 225857.24\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0037\tVal Loss: 0.0059\tVal MAE: 224893.42\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 23\tVal loss: 0.0058\tVal MAE: 223311.35\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 45 from 6/2019 to 9/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0232\tVal Loss: 0.0230\tVal MAE: 378423.47\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0095\tVal Loss: 0.0110\tVal MAE: 271770.44\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0056\tVal Loss: 0.0075\tVal MAE: 247685.63\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0043\tVal Loss: 0.0063\tVal MAE: 239389.99\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0040\tVal Loss: 0.0061\tVal MAE: 237362.43\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 24\tVal loss: 0.0060\tVal MAE: 239474.41\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 46 from 7/2019 to 10/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0180\tVal Loss: 0.0180\tVal MAE: 321304.20\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0076\tVal Loss: 0.0088\tVal MAE: 247502.39\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0050\tVal Loss: 0.0066\tVal MAE: 231788.54\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0043\tVal Loss: 0.0061\tVal MAE: 231174.44\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0042\tVal Loss: 0.0060\tVal MAE: 229640.49\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 22\tVal loss: 0.0060\tVal MAE: 229640.49\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 47 from 8/2019 to 11/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0197\tVal Loss: 0.0189\tVal MAE: 321192.43\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0083\tVal Loss: 0.0090\tVal MAE: 227885.08\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0052\tVal Loss: 0.0064\tVal MAE: 210365.41\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0044\tVal Loss: 0.0057\tVal MAE: 207069.46\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0043\tVal Loss: 0.0056\tVal MAE: 206200.50\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 22\tVal loss: 0.0056\tVal MAE: 206200.50\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 48 from 9/2019 to 12/2019:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0195\tVal Loss: 0.0187\tVal MAE: 299061.58\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0081\tVal Loss: 0.0087\tVal MAE: 203946.88\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0049\tVal Loss: 0.0061\tVal MAE: 184436.87\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0041\tVal Loss: 0.0053\tVal MAE: 180411.41\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0039\tVal Loss: 0.0052\tVal MAE: 179944.59\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 26\tVal loss: 0.0051\tVal MAE: 178615.17\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training step 49 from 10/2019 to 1/2021:\n",
      "\n",
      "\n",
      "Epoch 5: Train Loss: 0.0145\tVal Loss: 0.0146\tVal MAE: 270472.51\telapsed: 0.01 mins\n",
      "Epoch 10: Train Loss: 0.0062\tVal Loss: 0.0074\tVal MAE: 201359.01\telapsed: 0.01 mins\n",
      "Epoch 15: Train Loss: 0.0043\tVal Loss: 0.0057\tVal MAE: 189492.43\telapsed: 0.02 mins\n",
      "Epoch 20: Train Loss: 0.0038\tVal Loss: 0.0052\tVal MAE: 185821.22\telapsed: 0.02 mins\n",
      "Epoch 25: Train Loss: 0.0036\tVal Loss: 0.0051\tVal MAE: 186114.12\telapsed: 0.03 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 22\tVal loss: 0.0051\tVal MAE: 186114.12\tTotal time elapsed: 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Average Val loss: 0.0063\tAverage Val MAE: 242794.8895\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "window = 3\n",
    "expanding = False\n",
    "metrics_list, loss_list, train_set, val_set = [], [], [], []\n",
    "dates = sorted([(m, y) for y,m in train_df.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "\n",
    "for ix in range(len(train_norm)):\n",
    "    if expanding:\n",
    "        print('Training step {} from {}/{} to {}/{}:'.format(ix+1, dates[0][0], dates[0][1], dates[ix][0], dates[ix][1]))\n",
    "        print('\\n')\n",
    "        train_set += [train_norm[ix]]\n",
    "        val_set += [val_norm[ix]]\n",
    "    else:\n",
    "        print('Training step {} from {}/{} to {}/{}:'.format(ix+1, dates[ix-window][0], dates[ix-window][1], dates[ix][0], dates[ix][1]))\n",
    "        print('\\n')\n",
    "        train_set = [train_norm[i] if i>window else train_norm[ix] for i in range(ix-window,ix)]\n",
    "        val_set = [val_norm[i] if i>window else val_norm[ix] for i in range(ix-window,ix)]\n",
    "\n",
    "    train_dataset = FossilDataset(train_set, sku_encoder)\n",
    "    val_dataset = FossilDataset(val_set, sku_encoder, False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "    valid_loader = DataLoader(val_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    # model = pipe.model if ix>0 else FossilModel(len(features), 4, len(sku_encoder)+1, N_STEPS).double()\n",
    "    \n",
    "    model = FossilModel(len(features), 4, len(sku_encoder)+1, N_STEPS).double()\n",
    "    pipe = Pipeline(train_loader, valid_loader, model, target_scaler)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    pipe.train_model(3, 5)\n",
    "    best_model, val_loss, metrics = pipe.model_checkpoints[-1]\n",
    "\n",
    "    loss_list.append(val_loss)\n",
    "    metrics_list.append(metrics)\n",
    "print('Average Val loss: {:.4f}\\tAverage Val MAE: {:.4f}'.format(np.mean(loss_list), np.mean(metrics_list)))\n",
    "print('-'*50)\n",
    "# print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prepare OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_cnn_oof(model, train_norm, val_norm):\n",
    "    \n",
    "    oof_list, train_set, val_set = [], [], []\n",
    "    pred_cols = [f'y_pred_{j+1}' for j in range(N_STEPS)]\n",
    "\n",
    "    for ix in tqdm(range(len(train_norm)), desc='creating OOF predictions'):\n",
    "        if expanding_window:\n",
    "            train_set += [train_norm[ix]]\n",
    "            val_set += [val_norm[ix]]\n",
    "            \n",
    "        else:\n",
    "            train_set = [train_norm[i] for i in range(ix-window,ix)] if ix>window else train_norm[:ix+1]\n",
    "            val_set = [val_norm[i] for i in range(ix-window,ix)] if ix>window else val_norm[:ix+1]\n",
    "\n",
    "        train_dataset = FossilDataset(train_set, sku_encoder)\n",
    "        val_dataset = FossilDataset(val_set, sku_encoder, False)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "        valid_loader = DataLoader(val_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "        # model = pipe.model if ix>0 else FossilModel(len(features), 4, len(sku_encoder)+1, N_STEPS).double()\n",
    "\n",
    "        pipe = Pipeline(train_loader, valid_loader, model, target_scaler)\n",
    "        pipe.model.load_state_dict(models[ix])\n",
    "\n",
    "        oof_preds = pipe.make_oof_preds()\n",
    "        oof_preds = target_scaler.inverse_transform(oof_preds.reshape(1, -1)).reshape(-1, N_STEPS)# (products, preds)\n",
    "        oof_list.append(pd.DataFrame(oof_preds, columns=pred_cols))\n",
    "        \n",
    "    return oof_list\n",
    "\n",
    "def update_data(oof, train_data, val_data):\n",
    "    pred_cols = [f'y_pred_{j+1}' for j in range(N_STEPS)]\n",
    "    train_list, val_list = [], []\n",
    "    \n",
    "    for ix in tqdm(range(len(train_data)), desc='updating training data with OOF predictions'):\n",
    "        x_val = val_data[ix][0][0].join(oof[ix])\n",
    "        \n",
    "        if ix==len(train_data)-1:\n",
    "            val_list.append(((x_val,val_data[ix][0][1]),val_data[ix][1]))\n",
    "            continue\n",
    "\n",
    "        if ix==0:\n",
    "            train_data[ix][0][0][pred_cols] = np.nan\n",
    "            x_train = train_data[ix][0][0].copy()\n",
    "        else:\n",
    "            x_train = train_data[ix][0][0].join(oof[ix-1])\n",
    "            \n",
    "        train_list.append(((x_train,train_data[ix][0][1]),train_data[ix][1]))\n",
    "        val_list.append(((x_val,val_data[ix][0][1]),val_data[ix][1]))\n",
    "\n",
    "    return train_list, val_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ea4fafc1524104b084fb52fe311e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "creating OOF predictions:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5e20778c5e4b5495aaeb676740afb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "updating training data with OOF predictions:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FossilModel(len(features), 4, len(sku_encoder)+1, N_STEPS).double()\n",
    "oof_list = prepare_cnn_oof(model, train_norm, val_norm)\n",
    "train_data, val_data = update_data(oof_list, train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZ9HSK4g_g3X",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training on Final Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "48e0bcd890854a18b4710ba90d643830",
      "9e0a23c6f6574573929267c26790f0e8",
      "7585eee2183f4fa99749275bef109d60",
      "d9ea1817d5e24f01a6b85ce8b7f0d697",
      "b91173e97c4247c48c741d563ef17894",
      "a8b9bbd32dfe4f569374b8b878f715c4",
      "6529afe4759e41629a9d25e0cb2c530a",
      "ba2f651c4e864397b50a669b0ae23198",
      "5f849ad724954d64aba622d3799fd6b1",
      "1e131596ef834f13b0847b6a516c2064",
      "6896dcc1086c407c9bb987d026a9b335",
      "caf599e669e84e83bd02fe59add8593d",
      "f0787d2ae1434df786ae30a5cad26238",
      "4c44a3c1654745b6bc8db58ecdd99940",
      "a21dcb1036414c74af5cf15adcb102c0",
      "7a8c942666bc41dc86885e0cafe21f2a",
      "e261b8adb1a248b9b616d3b5655061b9",
      "0abb324774c148eaa19fac4fb665fe9b",
      "c0ba0ca588d64a15829ec04cc41ebf49",
      "8d382f3aec96437ea005b17f976f811c",
      "aab41a09b24d48dc83684dbacccf245f",
      "48cc49781ad34644b06a70c602688976",
      "dbca7372a9e64e02be2f4f1975b50e9b",
      "3bcf2e19c7464341a5f0eb8eed8c9916",
      "0ef75938ce8e4a68af5e1ecc7079639c",
      "0ce3ea3d2f554ee4920633b27749cfd6",
      "e9a809c9294d40d2803d997bb6efc09b",
      "8b87b5635a1c4f20a39de50b42b97747",
      "c7fd994b956c42a28ddc0302aade887d",
      "0a0bf323f759488881cb36c6348ee711",
      "46f52779db1b4ecda445c105b6e7d939",
      "b9aeb9eb7e3649b78e206df552f0c78d",
      "9f33d91db81c4f7084a5336821d56dc6"
     ]
    },
    "id": "DR7LUn5v1uN0",
    "outputId": "d6ab677d-8d48-43af-b0e9-6e9d469b145f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9d2140d7a44059a566b126a2ed6f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizing data:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_scaler, y_scaler, target_scaler = prepare_data_scaler(x_scaler, y_scaler, target_scaler, train_data, val_data)\n",
    "train_norm, val_norm = normalize_data(train_data, val_data, x_scaler, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window = 3\n",
    "expanding_window = True\n",
    "\n",
    "ix = len(train_norm)-1\n",
    "train_set = train_norm if expanding_window else [train_norm[i] if i>window else train_norm[ix] for i in range(ix-window,ix)]\n",
    "val_set = val_norm if expanding_window else [val_norm[i] if i>window else val_norm[ix] for i in range(ix-window,ix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "topIHZ_iQRCc"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = FossilDataset(train_set, sku_encoder)\n",
    "val_dataset = FossilDataset(val_set, sku_encoder)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, collate_fn=collate_fn, shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "gfjiP7GC5lxt"
   },
   "outputs": [],
   "source": [
    "model = FossilEncoder(len(features), 4, len(sku_encoder)+1, N_STEPS).double()\n",
    "pipe = Pipeline(train_loader, valid_loader, model, target_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJr1AUJ7P5ZD",
    "outputId": "be12fac1-2f82-4279-e100-f7a461e57d91",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0062\tVal Loss: 0.0059\tVal MAE: 67146.94\telapsed: 0.02 mins\n",
      "Epoch 2: Train Loss: 0.0059\tVal Loss: 0.0056\tVal MAE: 63825.20\telapsed: 0.05 mins\n",
      "Epoch 3: Train Loss: 0.0057\tVal Loss: 0.0055\tVal MAE: 62786.01\telapsed: 0.07 mins\n",
      "Epoch 4: Train Loss: 0.0056\tVal Loss: 0.0055\tVal MAE: 61979.29\telapsed: 0.09 mins\n",
      "Epoch 5: Train Loss: 0.0056\tVal Loss: 0.0054\tVal MAE: 61607.00\telapsed: 0.11 mins\n",
      "Epoch 6: Train Loss: 0.0056\tVal Loss: 0.0054\tVal MAE: 61439.74\telapsed: 0.14 mins\n",
      "Epoch 7: Train Loss: 0.0055\tVal Loss: 0.0053\tVal MAE: 60675.97\telapsed: 0.16 mins\n",
      "Epoch 8: Train Loss: 0.0056\tVal Loss: 0.0054\tVal MAE: 61048.88\telapsed: 0.18 mins\n",
      "Epoch 9: Train Loss: 0.0056\tVal Loss: 0.0054\tVal MAE: 61490.00\telapsed: 0.20 mins\n",
      "Epoch 10: Train Loss: 0.0056\tVal Loss: 0.0054\tVal MAE: 61406.78\telapsed: 0.22 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 7\tVal loss: 0.0053\tVal MAE: 61406.78\tTotal time elapsed: 0.22 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "pipe.train_model(3, 1, save_path=f'{MODEL_CHECKPOINT_DIR}/CNNpred1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJr1AUJ7P5ZD",
    "outputId": "be12fac1-2f82-4279-e100-f7a461e57d91",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0062\tVal Loss: 0.0056\tVal MAE: 63675.58\telapsed: 0.02 mins\n",
      "Epoch 2: Train Loss: 0.0056\tVal Loss: 0.0051\tVal MAE: 57674.27\telapsed: 0.04 mins\n",
      "Epoch 3: Train Loss: 0.0054\tVal Loss: 0.0049\tVal MAE: 56434.01\telapsed: 0.05 mins\n",
      "Epoch 4: Train Loss: 0.0053\tVal Loss: 0.0048\tVal MAE: 54816.54\telapsed: 0.08 mins\n",
      "Epoch 5: Train Loss: 0.0052\tVal Loss: 0.0047\tVal MAE: 53976.31\telapsed: 0.09 mins\n",
      "Epoch 6: Train Loss: 0.0051\tVal Loss: 0.0047\tVal MAE: 53252.46\telapsed: 0.11 mins\n",
      "Epoch 7: Train Loss: 0.0051\tVal Loss: 0.0046\tVal MAE: 52882.10\telapsed: 0.13 mins\n",
      "Epoch 8: Train Loss: 0.0051\tVal Loss: 0.0046\tVal MAE: 52627.44\telapsed: 0.15 mins\n",
      "Epoch 9: Train Loss: 0.0050\tVal Loss: 0.0046\tVal MAE: 52126.96\telapsed: 0.16 mins\n",
      "Epoch 10: Train Loss: 0.0050\tVal Loss: 0.0046\tVal MAE: 52103.90\telapsed: 0.18 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 7\tVal loss: 0.0046\tVal MAE: 52103.90\tTotal time elapsed: 0.18 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "pipe.train_model(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oP5U6XD87xnu",
    "outputId": "ce72c669-d2d7-4678-bafd-56589ce19577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0100\tVal Loss: 0.0106\tVal MAE: 212158.76\telapsed: 0.03 mins\n",
      "Epoch 2: Train Loss: 0.0095\tVal Loss: 0.0102\tVal MAE: 207235.72\telapsed: 0.06 mins\n",
      "Epoch 3: Train Loss: 0.0093\tVal Loss: 0.0099\tVal MAE: 205042.51\telapsed: 0.09 mins\n",
      "Epoch 4: Train Loss: 0.0091\tVal Loss: 0.0098\tVal MAE: 202743.84\telapsed: 0.12 mins\n",
      "Epoch 5: Train Loss: 0.0091\tVal Loss: 0.0097\tVal MAE: 201606.58\telapsed: 0.15 mins\n",
      "Epoch 6: Train Loss: 0.0091\tVal Loss: 0.0097\tVal MAE: 202068.22\telapsed: 0.18 mins\n",
      "Epoch 7: Train Loss: 0.0090\tVal Loss: 0.0096\tVal MAE: 201792.26\telapsed: 0.21 mins\n",
      "Epoch 8: Train Loss: 0.0091\tVal Loss: 0.0097\tVal MAE: 202449.30\telapsed: 0.24 mins\n",
      "Epoch 9: Train Loss: 0.0091\tVal Loss: 0.0097\tVal MAE: 204582.80\telapsed: 0.27 mins\n",
      "Epoch 10: Train Loss: 0.0091\tVal Loss: 0.0097\tVal MAE: 203588.09\telapsed: 0.30 mins\n",
      "\n",
      "\n",
      "Early stopping: Best Epoch: 7\tVal loss: 0.0096\tVal MAE: 203588.09\tTotal time elapsed: 0.30 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "pipe.train_model(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "8ni2UOZsLa_V"
   },
   "outputs": [],
   "source": [
    "model_weights, loss, metrics = pipe.model_checkpoints[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5gEH5qx_ODA",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kMcvIW1lUAtw",
    "outputId": "63587449-77d0-4130-c358-3f069c0a9ac7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model.load_state_dict(pipe.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "iGV4KVcKmTL-"
   },
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv(f'{TEST_DIR}/Test.csv')\n",
    "test['sku_coded'] = test.sku_name.apply(sku_encoder)\n",
    "\n",
    "test['idx'] = SEED\n",
    "test = test.groupby('sku_name').apply(lambda x: func(x, x.name)).reset_index(drop=True)\n",
    "\n",
    "dates = sorted([(m, y) for y,m in test.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "\n",
    "month,yr = dates[0]\n",
    "test_dates = [(month-n,yr) if month>n else (12+month-n, yr-1) for n in range(1,LOOKBACK+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "lffHhZo38lMg"
   },
   "outputs": [],
   "source": [
    "test_seq = prepare_test_data(train_df, test_dates, x_scaler)\n",
    "pred_df = make_predictions(test_seq, pipe.model, test)\n",
    "sub = make_submission(pred_df, dates, target_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "cLiXzq6kmsjG"
   },
   "outputs": [],
   "source": [
    "save_name = f'fossil_{loss}_3dCNN_pred_{metrics}.csv'\n",
    "sub.to_csv(f'{OUTPUT_DIR}/{save_name}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "1ChhK46Eny0f",
    "outputId": "881687c2-9f32-483a-e505-c6873105c583"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-146d94c6-c61a-4a02-8692-7c844b012e04\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>CAT_GENDER_BOTH</th>\n",
       "      <th>CAT_GENDER_MEN</th>\n",
       "      <th>CAT_GENDER_WOMEN</th>\n",
       "      <th>sku_coded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOSHTLYNYOSHZZ</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YOSHRENECARL</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>ABEENNEARMAZZ</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528 rows Ã— 7 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-146d94c6-c61a-4a02-8692-7c844b012e04')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-146d94c6-c61a-4a02-8692-7c844b012e04 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-146d94c6-c61a-4a02-8692-7c844b012e04');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            sku_name  month  year  CAT_GENDER_BOTH  CAT_GENDER_MEN  \\\n",
       "0     YOSHTLYNYOSHZZ     11  2021                0               1   \n",
       "1     YOSHTLYNYOSHZZ     12  2021                0               1   \n",
       "2     YOSHTLYNYOSHZZ      1  2022                0               1   \n",
       "3     YOSHTLYNYOSHZZ      2  2022                0               1   \n",
       "4       YOSHRENECARL     11  2021                0               0   \n",
       "...              ...    ...   ...              ...             ...   \n",
       "1523   ABEENNEARMAZZ      2  2022                0               0   \n",
       "1524     ABEAHAMASHL     11  2021                0               0   \n",
       "1525     ABEAHAMASHL     12  2021                0               0   \n",
       "1526     ABEAHAMASHL      1  2022                0               0   \n",
       "1527     ABEAHAMASHL      2  2022                0               0   \n",
       "\n",
       "      CAT_GENDER_WOMEN  sku_coded  \n",
       "0                    0       1852  \n",
       "1                    0       1852  \n",
       "2                    0       1852  \n",
       "3                    0       1852  \n",
       "4                    1        248  \n",
       "...                ...        ...  \n",
       "1523                 1        633  \n",
       "1524                 1         74  \n",
       "1525                 1         74  \n",
       "1526                 1         74  \n",
       "1527                 1         74  \n",
       "\n",
       "[1528 rows x 7 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8M0WJkqNt2Z0",
    "outputId": "3cfff892-ea7e-4525-f641-015b271618ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-16d78500-2f68-4349-afe9-e1ba8fbfb53b\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOSHTLYNYOSHZZ_11_2021</td>\n",
       "      <td>145429.250285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOSHTLYNYOSHZZ_12_2021</td>\n",
       "      <td>191321.712680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOSHTLYNYOSHZZ_1_2022</td>\n",
       "      <td>164629.086590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOSHTLYNYOSHZZ_2_2022</td>\n",
       "      <td>188166.288759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YOSHRENECARL_11_2021</td>\n",
       "      <td>247200.255858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>ABEENNEARMAZZ_2_2022</td>\n",
       "      <td>174279.919125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>ABEAHAMASHL_11_2021</td>\n",
       "      <td>209537.057022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>ABEAHAMASHL_12_2021</td>\n",
       "      <td>112665.964162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>ABEAHAMASHL_1_2022</td>\n",
       "      <td>168520.968764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>ABEAHAMASHL_2_2022</td>\n",
       "      <td>242835.092874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528 rows Ã— 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16d78500-2f68-4349-afe9-e1ba8fbfb53b')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-16d78500-2f68-4349-afe9-e1ba8fbfb53b button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-16d78500-2f68-4349-afe9-e1ba8fbfb53b');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                     Item_ID         Target\n",
       "0     YOSHTLYNYOSHZZ_11_2021  145429.250285\n",
       "1     YOSHTLYNYOSHZZ_12_2021  191321.712680\n",
       "2      YOSHTLYNYOSHZZ_1_2022  164629.086590\n",
       "3      YOSHTLYNYOSHZZ_2_2022  188166.288759\n",
       "4       YOSHRENECARL_11_2021  247200.255858\n",
       "...                      ...            ...\n",
       "1523    ABEENNEARMAZZ_2_2022  174279.919125\n",
       "1524     ABEAHAMASHL_11_2021  209537.057022\n",
       "1525     ABEAHAMASHL_12_2021  112665.964162\n",
       "1526      ABEAHAMASHL_1_2022  168520.968764\n",
       "1527      ABEAHAMASHL_2_2022  242835.092874\n",
       "\n",
       "[1528 rows x 2 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Ax2BCwr7nKfU",
    "outputId": "fc2aa7f4-4b58-439b-f467-12c216ae1543"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>WHITESHAMITCZZ_2_2022</td>\n",
       "      <td>-1093.405051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>WALLHORARUBYZZ_12_2021</td>\n",
       "      <td>-60285.438921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>WALLHORARUBYZZ_2_2022</td>\n",
       "      <td>-12720.524744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>TERRUTHEHARLZZ_12_2021</td>\n",
       "      <td>-10616.780384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>SUNDSAACETHAZZ_1_2022</td>\n",
       "      <td>-4126.050663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>ALONETTASHERZZ_2_2022</td>\n",
       "      <td>-25396.747117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>ALBEATHAPENNZZ_1_2022</td>\n",
       "      <td>-34879.619401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>ABERTHAKEVAZZ_11_2021</td>\n",
       "      <td>-41586.152583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>ABERTHAKEVAZZ_12_2021</td>\n",
       "      <td>-181.823899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>ABEAHAMASHL_11_2021</td>\n",
       "      <td>-5610.778971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Item_ID        Target\n",
       "27     WHITESHAMITCZZ_2_2022  -1093.405051\n",
       "53    WALLHORARUBYZZ_12_2021 -60285.438921\n",
       "55     WALLHORARUBYZZ_2_2022 -12720.524744\n",
       "77    TERRUTHEHARLZZ_12_2021 -10616.780384\n",
       "90     SUNDSAACETHAZZ_1_2022  -4126.050663\n",
       "...                      ...           ...\n",
       "1483   ALONETTASHERZZ_2_2022 -25396.747117\n",
       "1506   ALBEATHAPENNZZ_1_2022 -34879.619401\n",
       "1512   ABERTHAKEVAZZ_11_2021 -41586.152583\n",
       "1513   ABERTHAKEVAZZ_12_2021   -181.823899\n",
       "1524     ABEAHAMASHL_11_2021  -5610.778971\n",
       "\n",
       "[221 rows x 2 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[sub.Target<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "aziocmRjpix0",
    "outputId": "a4e67dc7-f941-4b9d-a9e5-2264470a3905"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-2cb26a18-7e70-4547-b914-913c55ad4a9d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>YOSHILSEHOWAZZ_11_2021</td>\n",
       "      <td>-444.869258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>YOSHILSEHOWAZZ_12_2021</td>\n",
       "      <td>-387.910670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>YOSHILSEHOWAZZ_1_2022</td>\n",
       "      <td>-9674.911690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WHITOLPHKAITZZ_11_2021</td>\n",
       "      <td>-148.123507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>WHITENNEANHZZ_12_2021</td>\n",
       "      <td>-17814.987857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>ALBEATHAPENNZZ_11_2021</td>\n",
       "      <td>-10957.578400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>ALBEATHAPENNZZ_12_2021</td>\n",
       "      <td>-34150.499972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>ALBEATHAPENNZZ_1_2022</td>\n",
       "      <td>-62915.975782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>ALBEATHAPENNZZ_2_2022</td>\n",
       "      <td>-24454.112466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>ABERTHAKEVAZZ_11_2021</td>\n",
       "      <td>-30490.881438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>258 rows Ã— 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cb26a18-7e70-4547-b914-913c55ad4a9d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-2cb26a18-7e70-4547-b914-913c55ad4a9d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-2cb26a18-7e70-4547-b914-913c55ad4a9d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                     Item_ID        Target\n",
       "12    YOSHILSEHOWAZZ_11_2021   -444.869258\n",
       "13    YOSHILSEHOWAZZ_12_2021   -387.910670\n",
       "14     YOSHILSEHOWAZZ_1_2022  -9674.911690\n",
       "20    WHITOLPHKAITZZ_11_2021   -148.123507\n",
       "29     WHITENNEANHZZ_12_2021 -17814.987857\n",
       "...                      ...           ...\n",
       "1504  ALBEATHAPENNZZ_11_2021 -10957.578400\n",
       "1505  ALBEATHAPENNZZ_12_2021 -34150.499972\n",
       "1506   ALBEATHAPENNZZ_1_2022 -62915.975782\n",
       "1507   ALBEATHAPENNZZ_2_2022 -24454.112466\n",
       "1512   ABERTHAKEVAZZ_11_2021 -30490.881438\n",
       "\n",
       "[258 rows x 2 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[sub.Target<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Oyo3MTagtMn_",
    "outputId": "bcc68f99-9aa0-4003-c199-46e706283d98"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-81a5ff1c-ab7d-453e-a2da-f13a8dc19113\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOSHTLYNYOSHZZ_11_2021</td>\n",
       "      <td>-86689.314831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOSHTLYNYOSHZZ_12_2021</td>\n",
       "      <td>504469.515706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOSHTLYNYOSHZZ_1_2022</td>\n",
       "      <td>124290.917432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOSHTLYNYOSHZZ_2_2022</td>\n",
       "      <td>409444.409936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YOSHRENECARL_11_2021</td>\n",
       "      <td>189293.468312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>ABEENNEARMAZZ_2_2022</td>\n",
       "      <td>178486.522485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>ABEAHAMASHL_11_2021</td>\n",
       "      <td>221659.443293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>ABEAHAMASHL_12_2021</td>\n",
       "      <td>117745.208810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>ABEAHAMASHL_1_2022</td>\n",
       "      <td>150719.726404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>ABEAHAMASHL_2_2022</td>\n",
       "      <td>234505.197429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528 rows Ã— 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81a5ff1c-ab7d-453e-a2da-f13a8dc19113')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-81a5ff1c-ab7d-453e-a2da-f13a8dc19113 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-81a5ff1c-ab7d-453e-a2da-f13a8dc19113');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                     Item_ID         Target\n",
       "0     YOSHTLYNYOSHZZ_11_2021  -86689.314831\n",
       "1     YOSHTLYNYOSHZZ_12_2021  504469.515706\n",
       "2      YOSHTLYNYOSHZZ_1_2022  124290.917432\n",
       "3      YOSHTLYNYOSHZZ_2_2022  409444.409936\n",
       "4       YOSHRENECARL_11_2021  189293.468312\n",
       "...                      ...            ...\n",
       "1523    ABEENNEARMAZZ_2_2022  178486.522485\n",
       "1524     ABEAHAMASHL_11_2021  221659.443293\n",
       "1525     ABEAHAMASHL_12_2021  117745.208810\n",
       "1526      ABEAHAMASHL_1_2022  150719.726404\n",
       "1527      ABEAHAMASHL_2_2022  234505.197429\n",
       "\n",
       "[1528 rows x 2 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNXWBBQ5LTy1"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(f'lgbm_importances{rmse}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCbgp2mtsmd5",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training on Final Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "UZgFemV-fIwX"
   },
   "outputs": [],
   "source": [
    "def train_step(train_loader, model, optimizer, criterion, total_epochs, verbose):\n",
    "    for epoch in range(total_epochs):\n",
    "        for batch_idx, (inputs,labels, y_) in enumerate(train_loader):\n",
    "            inputs = inputs.to(ModelsConfig.device)\n",
    "            labels = labels.to(ModelsConfig.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs)\n",
    "\n",
    "            loss = criterion(preds,labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        if epoch % verbose==0 or epoch==(total_epochs-1):\n",
    "            print('Epoch {}\\tloss: {:.4f}'.format(epoch+1, loss.item()))\n",
    "\n",
    "    return model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "lJXD9mxWl9qE"
   },
   "outputs": [],
   "source": [
    "def train_model(train_loader, num_features, num_products, epochs, verbose=5):\n",
    "    weights = []\n",
    "\n",
    "    for i,total_epochs in enumerate(epochs):\n",
    "        model = FossilModel(num_features, 4, num_products, N_STEPS).double().to(ModelsConfig.device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=ModelsConfig.learning_rate)\n",
    "        criterion = nn.L1Loss()\n",
    "    \n",
    "        print(f'Training model {i+1}:')\n",
    "        weights.append(train_step(train_loader, model, optimizer, criterion, total_epochs, verbose))\n",
    "        print('-'*50)\n",
    "        print('\\n')\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "xKSj4VsAVJUI"
   },
   "outputs": [],
   "source": [
    "\n",
    "dates = sorted([(m, y) for y,m in train_df.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "# start_ix = len(dates)-2*LOOKBACK-N_STEPS\n",
    "train_ix = len(dates)-LOOKBACK-N_STEPS\n",
    "test_ix = len(dates)-LOOKBACK\n",
    "\n",
    "x_train_date = dates[: train_ix]\n",
    "y_train_date = dates[train_ix:test_ix]\n",
    "test_date = dates[test_ix:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "jLMJgsVnVXjC"
   },
   "outputs": [],
   "source": [
    "target_cols = ['sku_name', 'sellin', 'month', 'year', 'sku_coded']\n",
    "\n",
    "x_train = train_df[train_df[['month', 'year']].apply(tuple, axis=1).isin(x_train_date)].reset_index(drop=True)\n",
    "y_train = train_df[train_df[['month', 'year']].apply(tuple, axis=1).isin(y_train_date)].reset_index(drop=True)\n",
    "\n",
    "x_train['sku_coded'] = x_train['sku_name'].apply(sku_encoder).reset_index(drop=True)\n",
    "y_train['sku_coded'] = y_train['sku_name'].apply(sku_encoder).reset_index(drop=True)\n",
    "\n",
    "y_train = _pad_target(x_train, y_train, sku_encoder)\n",
    "x_train = x_train.groupby(['year', 'month']).apply(pad_sku_sequence, encoder=sku_encoder).reset_index(drop=True)\n",
    "\n",
    "y_train['sellin'] = y_train.groupby(['sku_name'])['sellin'].transform(lambda x: x.fillna(x.mean()))\n",
    "y_train['sellin'] = y_train.groupby(['year', 'month'])['sellin'].transform(lambda x: x.fillna(x.mean()))\n",
    "y_train = y_train.groupby(['sku_name','year','month']).mean().reset_index()[target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "PhJVDUNGcVi1"
   },
   "outputs": [],
   "source": [
    "\n",
    "x_cols = [c for c in x_train.columns if c not in ['sku_name','sku_coded']]\n",
    "y_cols = [c for c in y_train.columns if c not in ['sku_name','sku_coded']]\n",
    "\n",
    "x_scaler.fit(x_train.drop(columns=['sku_name','sku_coded']))\n",
    "y_scaler.fit(y_train.drop(columns=['sku_name','sku_coded']))\n",
    "target_scaler.fit(x_train['sellin'].values.reshape(-1,1))\n",
    "\n",
    "x_train[x_cols] = x_scaler.transform(x_train[x_cols])\n",
    "y_train[y_cols] = y_scaler.transform(y_train[y_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "yr1cq2wddaN_"
   },
   "outputs": [],
   "source": [
    "features = [c for c in x_train.columns if c not in ['sku_name','year','month','sku_coded']]\n",
    "feat_group = x_train.sort_values(['sku_name','year','month']).groupby(['month','year'])\n",
    "\n",
    "seq = np.array([v.drop(columns=['sku_name','year','month','sku_coded']) for k,v in feat_group]) #(timesteps, products, features)\n",
    "seq = torch.as_tensor(np.transpose(seq, (2,0,1))).unsqueeze(0) #(features, timesteps, products)\n",
    "\n",
    "targets = y_train.sort_values(['sku_name','year','month'])\n",
    "targets = torch.as_tensor(targets.drop(columns=['month', 'year', 'sku_name', 'sku_coded']).values).transpose(0,1) #(timesteps*products, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "48e0bcd890854a18b4710ba90d643830",
      "9e0a23c6f6574573929267c26790f0e8",
      "7585eee2183f4fa99749275bef109d60",
      "d9ea1817d5e24f01a6b85ce8b7f0d697",
      "b91173e97c4247c48c741d563ef17894",
      "a8b9bbd32dfe4f569374b8b878f715c4",
      "6529afe4759e41629a9d25e0cb2c530a",
      "ba2f651c4e864397b50a669b0ae23198",
      "5f849ad724954d64aba622d3799fd6b1",
      "1e131596ef834f13b0847b6a516c2064",
      "6896dcc1086c407c9bb987d026a9b335",
      "caf599e669e84e83bd02fe59add8593d",
      "f0787d2ae1434df786ae30a5cad26238",
      "4c44a3c1654745b6bc8db58ecdd99940",
      "a21dcb1036414c74af5cf15adcb102c0",
      "7a8c942666bc41dc86885e0cafe21f2a",
      "e261b8adb1a248b9b616d3b5655061b9",
      "0abb324774c148eaa19fac4fb665fe9b",
      "c0ba0ca588d64a15829ec04cc41ebf49",
      "8d382f3aec96437ea005b17f976f811c",
      "aab41a09b24d48dc83684dbacccf245f",
      "48cc49781ad34644b06a70c602688976",
      "dbca7372a9e64e02be2f4f1975b50e9b",
      "3bcf2e19c7464341a5f0eb8eed8c9916",
      "0ef75938ce8e4a68af5e1ecc7079639c",
      "0ce3ea3d2f554ee4920633b27749cfd6",
      "e9a809c9294d40d2803d997bb6efc09b",
      "8b87b5635a1c4f20a39de50b42b97747",
      "c7fd994b956c42a28ddc0302aade887d",
      "0a0bf323f759488881cb36c6348ee711",
      "46f52779db1b4ecda445c105b6e7d939",
      "b9aeb9eb7e3649b78e206df552f0c78d",
      "9f33d91db81c4f7084a5336821d56dc6"
     ]
    },
    "id": "DR7LUn5v1uN0",
    "outputId": "d6ab677d-8d48-43af-b0e9-6e9d469b145f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab88954765d419994ab4cb4004a303b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizing data:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_scaler, y_scaler, target_scaler = prepare_data_scaler(x_scaler, y_scaler, target_scaler, train_data, val_data)\n",
    "train_norm, val_norm = normalize_data(train_data, val_data, x_scaler, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_norm + [((x_train,y_train),y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "train_dataset = FossilDataset(train_set, sku_encoder)\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "YZDEcp_gf7Y2"
   },
   "outputs": [],
   "source": [
    "\n",
    "inputs = seq.to(ModelsConfig.device)\n",
    "labels = targets.to(ModelsConfig.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>sellin</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11143.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12156.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>41533.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15363</th>\n",
       "      <td>3840</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15364</th>\n",
       "      <td>3841</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15365</th>\n",
       "      <td>3841</td>\n",
       "      <td>11143.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15366</th>\n",
       "      <td>3841</td>\n",
       "      <td>12156.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15367</th>\n",
       "      <td>3841</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2021.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15368 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sku_name   sellin  month    year\n",
       "0             0  14182.0    7.0  2021.0\n",
       "1             0  11143.0    8.0  2021.0\n",
       "2             0  12156.0    9.0  2021.0\n",
       "3             0   5065.0   10.0  2021.0\n",
       "4             1  41533.0    7.0  2021.0\n",
       "...         ...      ...    ...     ...\n",
       "15363      3840   5065.0   10.0  2021.0\n",
       "15364      3841  14182.0    7.0  2021.0\n",
       "15365      3841  11143.0    8.0  2021.0\n",
       "15366      3841  12156.0    9.0  2021.0\n",
       "15367      3841   5065.0   10.0  2021.0\n",
       "\n",
       "[15368 rows x 4 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[-1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 2021), (7, 2021), (8, 2021), (9, 2021), (10, 2021)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6txV5sToZ1j",
    "outputId": "713e45d2-9d69-482f-a0b3-4a76d612fa2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x212 and 4x28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [134]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m model_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msku_encoder\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [129]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, num_features, num_products, epochs, verbose)\u001b[0m\n\u001b[1;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m weights\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [133]\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(train_loader, model, optimizer, criterion, total_epochs, verbose)\u001b[0m\n\u001b[1;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(ModelsConfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds,labels)\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mFossilModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# x = self.max_pool2d(x)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# x = self.max_pool2d(x)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x212 and 4x28)"
     ]
    }
   ],
   "source": [
    "epochs = [5]\n",
    "model_weights = train_model(train_loader, len(features), len(sku_encoder)+1, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 34, 53, 3842])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4fDY3bqxzHt"
   },
   "outputs": [],
   "source": [
    "def test_model(test_seq, model, model_weights):\n",
    "    preds = []\n",
    "\n",
    "    for i,weights in enumerate(model_weights):\n",
    "        model.load_state_dict(weights)\n",
    "        preds.append(model(torch.as_tensor(test_seq).unsqueeze(0).to(ModelsConfig.device)).detach().numpy().reshape(-1, N_STEPS))\n",
    "    \n",
    "    return np.mean(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4VanVgFZvuX"
   },
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv(f'{TEST_DIR}/Test.csv')\n",
    "test['sku_coded'] = test.sku_name.apply(sku_encoder)\n",
    "\n",
    "test['idx'] = SEED\n",
    "test = test.groupby('sku_name').apply(lambda x: func(x, x.name)).reset_index(drop=True)\n",
    "\n",
    "test_seq = prepare_test_data(train_df, test_date, x_scaler)\n",
    "model = FossilModel(len(features), 4, len(sku_encoder)+1, N_STEPS).double().to(ModelsConfig.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkDHUTrYziB3"
   },
   "outputs": [],
   "source": [
    "preds = test_model(test_seq, model, model_weights)\n",
    "test['sellin'] = preds[test.sku_coded.values, test.idx.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79nTqeuK04f_"
   },
   "outputs": [],
   "source": [
    "dates = sorted([(m,y) for y,m in test.groupby(['year','month']).groups.keys()], key= lambda g: (g[1],g[0]))\n",
    "sub = make_submission(test, dates, target_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBv3DNmc4CkF"
   },
   "outputs": [],
   "source": [
    "save_name = f'fossil_{0.0024}_3dCNN_pred_{174729.0105}.csv'\n",
    "sub.to_csv(save_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "fH6BEvTC4YF6",
    "outputId": "bd822b0b-5db4-4299-fbd3-794e9f73e395"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_8c61e856-49ce-4779-aa49-d0afa82af768\", \"fossil_0.0024_3dCNN_pred_174729.0105.csv\", 61063)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2Q5WThw78yc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vnb709ys2ktD",
    "outputId": "bc60f9ff-d698-4880-e08d-b9836e3bb22b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-eee5def0-15be-4ffc-a806-003c08bbf2de\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>WALLUTHEGRAHZZ_1_2022</td>\n",
       "      <td>-392.400952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>SUNDRENEASHLZZ_12_2021</td>\n",
       "      <td>-3577.196945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>SUNDBARTLEATZZ_12_2021</td>\n",
       "      <td>-3158.495167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>SIGRERRYARTIZZ_2_2022</td>\n",
       "      <td>-557.118437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>SHERTTNYNOLAZZ_12_2021</td>\n",
       "      <td>-410.538957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>SHERANDARIGOZZ_2_2022</td>\n",
       "      <td>-1699.748038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>SAMABARTRUTHZZ_2_2022</td>\n",
       "      <td>-3394.795845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>ROSEERRYHANGZZ_11_2021</td>\n",
       "      <td>-1303.555183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>RIGOANNALAKEZZ_11_2021</td>\n",
       "      <td>-358.877303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>RICKONIOCATHZZ_12_2021</td>\n",
       "      <td>-1071.753557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>REYRUBYDONAZZ_11_2021</td>\n",
       "      <td>-894.058806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>REVALIZAMICHZZ_11_2021</td>\n",
       "      <td>-4969.493140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>REVAJAYENOLA_2_2022</td>\n",
       "      <td>-3980.418022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>PAMULONAJAYEZZ_12_2021</td>\n",
       "      <td>-7889.297747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>NOVERENEETHAZZ_2_2022</td>\n",
       "      <td>-2504.323226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>NOVEATHAASHLZZ_11_2021</td>\n",
       "      <td>-1047.078180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>MICHEDESKAIT_12_2021</td>\n",
       "      <td>-4107.147002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>MERCTTNYLEAT_11_2021</td>\n",
       "      <td>-6954.364575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>MERCTTNYLEAT_2_2022</td>\n",
       "      <td>-1676.440380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>MERCSAACJULI_2_2022</td>\n",
       "      <td>-1642.783959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>MERCHLIEHARLZZ_2_2022</td>\n",
       "      <td>-1331.713635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>MAXIEENAKATHZZ_12_2021</td>\n",
       "      <td>-425.292250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>MALIHEBAYOSHZZ_12_2021</td>\n",
       "      <td>-2413.392774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>MALIERTORIGOZZ_11_2021</td>\n",
       "      <td>-641.487903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>LILLATHARIGO_1_2022</td>\n",
       "      <td>-3227.113729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>LAVENORAKAIT_1_2022</td>\n",
       "      <td>-1731.026042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>LAVEHALARLEZZ_12_2021</td>\n",
       "      <td>-619.928622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>KEVAROBRIGO_11_2021</td>\n",
       "      <td>-2174.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>KEVAROBRIGO_12_2021</td>\n",
       "      <td>-799.548392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>KATHTNEYMICH_2_2022</td>\n",
       "      <td>-230.950108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>KATHABEMERC_2_2022</td>\n",
       "      <td>-391.916862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>KAITVETADUANZZ_1_2022</td>\n",
       "      <td>-5595.994223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>HOWAUREAJULIZZ_11_2021</td>\n",
       "      <td>-2524.236586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>ETHAMULABRITZZ_1_2022</td>\n",
       "      <td>-2616.352750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>ETHAKEVASHER_2_2022</td>\n",
       "      <td>-3273.957636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>ETHAICKIDONOZZ_12_2021</td>\n",
       "      <td>-1980.643946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>ELIZHLIEAURE_2_2022</td>\n",
       "      <td>-480.215024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>DONAAHAMRHEBZZ_11_2021</td>\n",
       "      <td>-3899.346037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>DESMSHIAASHLZZ_1_2022</td>\n",
       "      <td>-3066.352146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>CINDIBALDONOZZ_1_2022</td>\n",
       "      <td>-2479.805268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>CARLERINCINDZZ_11_2021</td>\n",
       "      <td>-1117.915572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>CARLERINCINDZZ_1_2022</td>\n",
       "      <td>-4459.936261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>CANDETTEMAXIZZ_12_2021</td>\n",
       "      <td>-24.856518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>BRITABEREEN_11_2021</td>\n",
       "      <td>-541.778317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>BRITABEREEN_12_2021</td>\n",
       "      <td>-2559.435938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>ASHLNALDJULI_11_2021</td>\n",
       "      <td>-1458.216563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>ARTIERINILSE_11_2021</td>\n",
       "      <td>-946.460082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>ARTIERINILSE_1_2022</td>\n",
       "      <td>-66.047552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>ARLEENNYARTI_11_2021</td>\n",
       "      <td>-2544.582944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>ANHEENARUDOZZ_1_2022</td>\n",
       "      <td>-1462.531567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>AMBEELIABRITZZ_12_2021</td>\n",
       "      <td>-5572.684549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>ALONRTIEARLEZZ_11_2021</td>\n",
       "      <td>-120.789590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eee5def0-15be-4ffc-a806-003c08bbf2de')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-eee5def0-15be-4ffc-a806-003c08bbf2de button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-eee5def0-15be-4ffc-a806-003c08bbf2de');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                     Item_ID       Target\n",
       "38     WALLUTHEGRAHZZ_1_2022  -392.400952\n",
       "97    SUNDRENEASHLZZ_12_2021 -3577.196945\n",
       "101   SUNDBARTLEATZZ_12_2021 -3158.495167\n",
       "135    SIGRERRYARTIZZ_2_2022  -557.118437\n",
       "137   SHERTTNYNOLAZZ_12_2021  -410.538957\n",
       "143    SHERANDARIGOZZ_2_2022 -1699.748038\n",
       "163    SAMABARTRUTHZZ_2_2022 -3394.795845\n",
       "216   ROSEERRYHANGZZ_11_2021 -1303.555183\n",
       "260   RIGOANNALAKEZZ_11_2021  -358.877303\n",
       "269   RICKONIOCATHZZ_12_2021 -1071.753557\n",
       "288    REYRUBYDONAZZ_11_2021  -894.058806\n",
       "316   REVALIZAMICHZZ_11_2021 -4969.493140\n",
       "323      REVAJAYENOLA_2_2022 -3980.418022\n",
       "381   PAMULONAJAYEZZ_12_2021 -7889.297747\n",
       "407    NOVERENEETHAZZ_2_2022 -2504.323226\n",
       "420   NOVEATHAASHLZZ_11_2021 -1047.078180\n",
       "465     MICHEDESKAIT_12_2021 -4107.147002\n",
       "472     MERCTTNYLEAT_11_2021 -6954.364575\n",
       "475      MERCTTNYLEAT_2_2022 -1676.440380\n",
       "479      MERCSAACJULI_2_2022 -1642.783959\n",
       "495    MERCHLIEHARLZZ_2_2022 -1331.713635\n",
       "537   MAXIEENAKATHZZ_12_2021  -425.292250\n",
       "561   MALIHEBAYOSHZZ_12_2021 -2413.392774\n",
       "564   MALIERTORIGOZZ_11_2021  -641.487903\n",
       "642      LILLATHARIGO_1_2022 -3227.113729\n",
       "694      LAVENORAKAIT_1_2022 -1731.026042\n",
       "701    LAVEHALARLEZZ_12_2021  -619.928622\n",
       "764      KEVAROBRIGO_11_2021 -2174.278600\n",
       "765      KEVAROBRIGO_12_2021  -799.548392\n",
       "775      KATHTNEYMICH_2_2022  -230.950108\n",
       "783       KATHABEMERC_2_2022  -391.916862\n",
       "810    KAITVETADUANZZ_1_2022 -5595.994223\n",
       "916   HOWAUREAJULIZZ_11_2021 -2524.236586\n",
       "1018   ETHAMULABRITZZ_1_2022 -2616.352750\n",
       "1023     ETHAKEVASHER_2_2022 -3273.957636\n",
       "1029  ETHAICKIDONOZZ_12_2021 -1980.643946\n",
       "1099     ELIZHLIEAURE_2_2022  -480.215024\n",
       "1160  DONAAHAMRHEBZZ_11_2021 -3899.346037\n",
       "1166   DESMSHIAASHLZZ_1_2022 -3066.352146\n",
       "1206   CINDIBALDONOZZ_1_2022 -2479.805268\n",
       "1236  CARLERINCINDZZ_11_2021 -1117.915572\n",
       "1238   CARLERINCINDZZ_1_2022 -4459.936261\n",
       "1241  CANDETTEMAXIZZ_12_2021   -24.856518\n",
       "1272     BRITABEREEN_11_2021  -541.778317\n",
       "1273     BRITABEREEN_12_2021 -2559.435938\n",
       "1296    ASHLNALDJULI_11_2021 -1458.216563\n",
       "1332    ARTIERINILSE_11_2021  -946.460082\n",
       "1334     ARTIERINILSE_1_2022   -66.047552\n",
       "1380    ARLEENNYARTI_11_2021 -2544.582944\n",
       "1446    ANHEENARUDOZZ_1_2022 -1462.531567\n",
       "1461  AMBEELIABRITZZ_12_2021 -5572.684549\n",
       "1468  ALONRTIEARLEZZ_11_2021  -120.789590"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[sub.Target<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "NldUcoXg7Vya",
    "outputId": "8e9059d5-675a-4199-fae5-a251180a681e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-89138507-ca64-45af-8dd7-c3b8d016b726\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>WALLHAELARLEZZ_12_2021</td>\n",
       "      <td>-5963.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>SUNDSAACETHAZZ_12_2021</td>\n",
       "      <td>-4940.276403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>SIGRERRYARTIZZ_2_2022</td>\n",
       "      <td>-10042.744408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>SHERTTNYNOLAZZ_12_2021</td>\n",
       "      <td>-11454.637798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>ROSEINDIPENNZZ_1_2022</td>\n",
       "      <td>-37445.711867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>ARTIERINILSE_2_2022</td>\n",
       "      <td>-33696.514132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>ARLEENNYARTI_11_2021</td>\n",
       "      <td>-14409.352859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>ANIBMBERJULI_1_2022</td>\n",
       "      <td>-154.905098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>ANHANHILSEZZ_12_2021</td>\n",
       "      <td>-251.972520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>ALBERRONMAYN_1_2022</td>\n",
       "      <td>-29664.136040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows Ã— 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89138507-ca64-45af-8dd7-c3b8d016b726')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-89138507-ca64-45af-8dd7-c3b8d016b726 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-89138507-ca64-45af-8dd7-c3b8d016b726');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                     Item_ID        Target\n",
       "57    WALLHAELARLEZZ_12_2021  -5963.760600\n",
       "89    SUNDSAACETHAZZ_12_2021  -4940.276403\n",
       "135    SIGRERRYARTIZZ_2_2022 -10042.744408\n",
       "137   SHERTTNYNOLAZZ_12_2021 -11454.637798\n",
       "214    ROSEINDIPENNZZ_1_2022 -37445.711867\n",
       "...                      ...           ...\n",
       "1335     ARTIERINILSE_2_2022 -33696.514132\n",
       "1380    ARLEENNYARTI_11_2021 -14409.352859\n",
       "1418     ANIBMBERJULI_1_2022   -154.905098\n",
       "1449    ANHANHILSEZZ_12_2021   -251.972520\n",
       "1494     ALBERRONMAYN_1_2022 -29664.136040\n",
       "\n",
       "[63 rows x 2 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[sub.Target<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "JRAmoMbP6YHH",
    "outputId": "39bee2de-b34e-4441-9510-79d6baab2810"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-07a4f4c7-b850-4d6a-b407-679fcacec1ab\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>WALLNALDDUAN_11_2021</td>\n",
       "      <td>-74723.751976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>WALLNALDDUAN_12_2021</td>\n",
       "      <td>-15544.969020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>WALLNALDDUAN_2_2022</td>\n",
       "      <td>-54897.061613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>WALLHAELARLEZZ_1_2022</td>\n",
       "      <td>-33625.251960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>TRANETTEALONZZ_2_2022</td>\n",
       "      <td>-46340.884279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>AMBEELIABRITZZ_12_2021</td>\n",
       "      <td>-49568.355077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>AMBEELIABRITZZ_1_2022</td>\n",
       "      <td>-31328.971367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>AMBEELIABRITZZ_2_2022</td>\n",
       "      <td>-67644.973320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>ABEAHAMASHL_12_2021</td>\n",
       "      <td>-49614.639129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>ABEAHAMASHL_1_2022</td>\n",
       "      <td>-21506.159428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166 rows Ã— 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07a4f4c7-b850-4d6a-b407-679fcacec1ab')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-07a4f4c7-b850-4d6a-b407-679fcacec1ab button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-07a4f4c7-b850-4d6a-b407-679fcacec1ab');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                     Item_ID        Target\n",
       "44      WALLNALDDUAN_11_2021 -74723.751976\n",
       "45      WALLNALDDUAN_12_2021 -15544.969020\n",
       "47       WALLNALDDUAN_2_2022 -54897.061613\n",
       "58     WALLHAELARLEZZ_1_2022 -33625.251960\n",
       "71     TRANETTEALONZZ_2_2022 -46340.884279\n",
       "...                      ...           ...\n",
       "1461  AMBEELIABRITZZ_12_2021 -49568.355077\n",
       "1462   AMBEELIABRITZZ_1_2022 -31328.971367\n",
       "1463   AMBEELIABRITZZ_2_2022 -67644.973320\n",
       "1525     ABEAHAMASHL_12_2021 -49614.639129\n",
       "1526      ABEAHAMASHL_1_2022 -21506.159428\n",
       "\n",
       "[166 rows x 2 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBZLG3x_LjrK"
   },
   "outputs": [],
   "source": [
    "test['sku_coded'] = test.sku_name.apply(sku_encoder)\n",
    "dates = sorted([(m,y) for y,m in test.groupby(['year','month']).groups.keys()], key= lambda g: (g[1],g[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pgu4M9KCWyiV",
    "tags": []
   },
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prereqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lju143vVHUHH"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import lightgbm as lgb \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9nWLuxB5LOLu"
   },
   "outputs": [],
   "source": [
    "#function to make feature-importance plot\n",
    "def plot_Imp_lgb(model, X, score):\n",
    "    '''\n",
    "    A funtion to make feature importance plot for LGBM.\n",
    "    Returns feature-impotance plot.\n",
    "\n",
    "    args: model -trained model\n",
    "    args: X     - features used to train model\n",
    "    \n",
    "    '''\n",
    "    feature_imp = pd.DataFrame({'Value':model.feature_importance(),'Feature':X.columns})\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    sns.set(font_scale = 1)\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                        ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{PLOTS_DIR}/lgbm_importances{score}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Favcih0eHZ6E"
   },
   "outputs": [],
   "source": [
    "def update_feature_importances(x_train, model, fold, feature_importance):\n",
    "    # feature importance\n",
    "    fold_importance = pd.DataFrame()\n",
    "    fold_importance[\"feature\"] = x_train.columns\n",
    "    fold_importance[\"importance\"] = model.feature_importance()\n",
    "    fold_importance[\"fold\"] = fold + 1\n",
    "    \n",
    "    return pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "def cv_feature_importance_plot(train_data: list, folds: int, cv_models: list, plot_feature_importance: bool=True):\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # print(f'Training fold {fold+1}')\n",
    "    print('\\n')\n",
    "    kf = GroupKFold(folds)\n",
    "\n",
    "    features = train_data[[c for c in train_data.columns if 'target' not in c]]\n",
    "    group = features['month'].astype(str)+'_'+features['year'].astype(str)\n",
    "    targets = train_data[[c for c in train_data.columns if 'target' in c]]\n",
    "\n",
    "\n",
    "    for fold, (train_id, val_id) in enumerate(kf.split(features, targets, group)):\n",
    "        for i,model in enumerate(cv_models[fold]):\n",
    "            feature_importance = update_feature_importances(features.iloc[train_id].drop(columns=['sku_name', 'sku_coded']), model, fold, feature_importance)\n",
    "            if plot_feature_importance:\n",
    "                plot_Imp_lgb(model, features.iloc[train_id].drop(columns=['sku_name', 'sku_coded']), f'fold_{fold+1}_step_{i+1}')\n",
    "\n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NrFDg6V8Hhi7"
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def evaluate(df, oof): \n",
    "    target_cols = [c for c in df.columns if 'target' in c]\n",
    "    pred_df = pd.DataFrame(np.repeat(df['sku_name'], N_STEPS), columns=['sku_name']).reset_index(drop=True)\n",
    "    pred_df['oof'] = oof.reshape(-1,1)\n",
    "    pred_df['target'] = df[target_cols].values.reshape(-1,1)\n",
    "    oof_mae = np.absolute(np.subtract(pred_df['target'], pred_df['oof'])).mean()\n",
    "    \n",
    "    return oof_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xor7xPJ4HqtY"
   },
   "outputs": [],
   "source": [
    "def eval_step(df, oof, target_col):    \n",
    "    metrics = evaluate(df, oof, target_col)\n",
    "    print(f'OOF MAE: %.4F'%metrics)\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "cQjuIY85H0gQ"
   },
   "outputs": [],
   "source": [
    "def multi_step_test(x_test, models):\n",
    "    oof = []\n",
    "    channel_cols = [c for c in x_test.columns if any(f'channel_{i}' in c for i in range(1,11)) and c not in CAT+['sku_name','sku_coded']]\n",
    "\n",
    "    for step in range(N_STEPS):\n",
    "        oof.append(models[step].predict(x_test.drop(columns=['sku_name', 'sku_coded']+channel_cols), start_iteration=-1))\n",
    "\n",
    "    return oof\n",
    "\n",
    "def test_model(train_data:pd.DataFrame, folds:int, cv_models:list):\n",
    "    mae = []\n",
    "    start_time = time()   \n",
    "    kf = GroupKFold(folds)\n",
    "    oof = np.zeros((len(train_data),N_STEPS))\n",
    "\n",
    "    features = train_data[[c for c in train_data.columns if 'target' not in c]]\n",
    "    group = features['month'].astype(str)+'_'+features['year'].astype(str)\n",
    "    targets = train_data[[c for c in train_data.columns if 'target' in c]]\n",
    "    channel_cols = [c for c in features.columns if any(f'channel_{i}' in c for i in range(1,11)) and c not in CAT+['sku_name','sku_coded']]\n",
    "\n",
    "    for fold, (train_id, val_id) in enumerate(kf.split(features, targets, group)):\n",
    "        print(f\"Making fold {fold+1} predictions\")\n",
    "        # predict oof\n",
    "        for step in range(N_STEPS):\n",
    "            oof[val_id, step] = cv_models[fold][step].predict(features.iloc[val_id].drop(columns=['sku_name','sku_coded']), start_iteration=-1)\n",
    "    \n",
    "        # mae.append(evaluate(test, oof_preds, 'sellin'))\n",
    "        # print(\"valid MAE {:.4f}\\tElapsed {:.2f} mins\".format(mae[-1], (time() - start_time)/60))\n",
    "        # print('-'*50)\n",
    "        print('\\n')\n",
    "\n",
    "    val_mae = evaluate(train_data, oof)\n",
    "    print(f'Average Val MAE: {val_mae}')\n",
    "    # del(x_test)\n",
    "    train\n",
    "    gc.collect()\n",
    "    \n",
    "    return val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kQE5BI4rCP-V"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "def cv_train(params, xy_train, xy_val): \n",
    "    train_set = lgb.Dataset(*xy_train)\n",
    "    val_set = lgb.Dataset(*xy_val)\n",
    "\n",
    "    # train model\n",
    "    return lgb.train(params, \n",
    "                    train_set,\n",
    "                    num_boost_round = 100000,\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=5),lgb.log_evaluation(period=10)],\n",
    "                    valid_sets = [train_set, val_set], \n",
    "                    )\n",
    "\n",
    "def model_train(params, xy_train, num_rounds):\n",
    "    train_set = lgb.Dataset(*xy_train)\n",
    "    # train model\n",
    "    return lgb.train(params, \n",
    "                    train_set,\n",
    "                    num_boost_round = num_rounds,\n",
    "                    callbacks=[lgb.log_evaluation(period=10)],\n",
    "                    # valid_sets = [train_set, val_set], \n",
    "                    )\n",
    "    \n",
    "def multi_step_train(params, \n",
    "                     x_train, \n",
    "                     y_train, \n",
    "                     train_cv,\n",
    "                     x_val=None, \n",
    "                     y_val=None, \n",
    "                     store_cv_models=False, \n",
    "                     fold=None, \n",
    "                     num_rounds=None):\n",
    "    models = []\n",
    "    channel_cols = [c for c in x_train.columns if any(f'channel_{i}' in c for i in range(1,11)) and c not in CAT+['sku_name','sku_coded']]\n",
    "    \n",
    "    for step in range(N_STEPS):\n",
    "        print(f'Training model for timestep {step+1} forecasting')\n",
    "        feature_importance = pd.DataFrame()\n",
    "        \n",
    "        if train_cv:\n",
    "            assert x_val is not None and y_val is not None, 'validation data required for CV training'\n",
    "            model = cv_train(params,\n",
    "                             (x_train.drop(columns=['sku_name','sku_coded']),\n",
    "                              y_train[f'target_{step}']),\n",
    "                             (x_val.drop(columns=['sku_name','sku_coded']),\n",
    "                              y_val[f'target_{step}']))\n",
    "            \n",
    "            if store_cv_models:\n",
    "                model.save_model(f'lgb_cv_model_step{step+1}_{fold+1}')\n",
    "        else:\n",
    "            assert num_rounds is not None, 'Please specifiy number of training rounds'\n",
    "            model = model_train(params, \n",
    "                                (x_train.drop(columns=['sku_name','sku_coded']+channel_cols),\n",
    "                                y_train[f'target_{step}']),\n",
    "                                 num_rounds)\n",
    "        models.append(model)\n",
    "        \n",
    "        print('\\n')\n",
    "\n",
    "    return models\n",
    "\n",
    "    \n",
    "def train_model(train_data: pd.DataFrame, folds: int, store_cv_models: bool=False):\n",
    "    cv_models = []\n",
    "    start_time = time()\n",
    "    kf = GroupKFold(folds)\n",
    "    \n",
    "\n",
    "    features = train_data[[c for c in train_data.columns if 'target' not in c]]\n",
    "    group = features['month'].astype(str)+'_'+features['year'].astype(str)\n",
    "    targets = train_data[[c for c in train_data.columns if 'target' in c]]\n",
    "\n",
    "\n",
    "    for fold, (train_id, val_id) in enumerate(kf.split(features, targets, group)):\n",
    "        print(f'Training fold {fold+1}')\n",
    "        print('\\n')\n",
    "        models = multi_step_train(params,\n",
    "                                  features.iloc[train_id],\n",
    "                                  targets.iloc[train_id],\n",
    "                                  True,\n",
    "                                  features.iloc[val_id],\n",
    "                                  targets.iloc[val_id],\n",
    "                                  store_cv_models,\n",
    "                                  fold)\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Elapsed {:.2f} mins\".format((time() - start_time)/60))\n",
    "        print('-'*50)\n",
    "        print('\\n')\n",
    "              \n",
    "        gc.collect()\n",
    "        cv_models.append(models)\n",
    "\n",
    "    return cv_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(df, encoder):\n",
    "    non_missing = df['sku_coded'].unique()\n",
    "    # print(df)\n",
    "    df = df.groupby(['month','year']).apply(pad_sku_sequence, encoder=encoder).reset_index(drop=True)\n",
    "    \n",
    "    target_cols = [f'target_{i}' for i in range(N_STEPS)]\n",
    "    cols = [c for c in df.columns if c not in ['sku_coded','month','year', 'sku_name']+target_cols]\n",
    "    df.loc[df['sku_coded']==len(encoder), cols] = np.nan\n",
    "    \n",
    "    return df[df['sku_coded'].isin(non_missing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(df, dates):   \n",
    "    non_missing = [(m, y) for y,m in df.groupby(['year', 'month']).groups.keys()]\n",
    "    df = pad_targets(df, dates).sort_values(by=['year','month'])\n",
    "\n",
    "    for step in range(N_STEPS):\n",
    "        df[f'target_{step}'] = df['sellin'].shift(-(step+1))\n",
    "    df = _add_periodic_indices(df)\n",
    "        \n",
    "    return df[df[['month', 'year']].apply(tuple, axis=1).isin(non_missing)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "L8bT88rdz-xj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "tqdm.pandas()\n",
    "train = pd.read_csv(f'{TRAIN_DIR}/Train.csv')\n",
    "desc = pd.read_csv(f'{DATA_DIR}/DataDictionary.csv')\n",
    "\n",
    "CAT = desc[36:]['Column Name'].tolist()\n",
    "channel_cols = [c for c in train.columns if all(f'_{i}' not in c for i in range(1,11)) and c not in CAT+['sku_name','month','year','sku_coded']]\n",
    "train_df = train.drop(columns=CAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7afc65cec548d4b5bbcdfc2890d2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f323521825294596bf329e2026312f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "np.random.seed(SEED)\n",
    "sku_encoder = LabelEncoder(train.sku_name.sample(frac=0.95).unique())\n",
    "\n",
    "dates = sorted([(m, y) for y,m in train_df.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "train_data = train_df.groupby('sku_name').progress_apply(prepare_data, dates=dates).reset_index(drop=True)\n",
    "\n",
    "train_data['sku_coded'] = train_data['sku_name'].apply(sku_encoder)\n",
    "train_data = train_data.groupby(['month','year']).progress_apply(clean_data, encoder=sku_encoder).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyc_cols = [c for c in train_data.columns if 'seires' in c or 'month' in c]\n",
    "# train_data = cyclic_encode(train_data, cyc_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# channel_cols = [c for c in train_data.columns if all(f'_{i}' not in c for i in range(1,11)) and c not in CAT+['sku_name','month','year','sku_coded'] and 'target' not in c]\n",
    "# train_data = lag_shift(train_data, 'sku_name', channel_cols, [3,6,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_1 = [c for c in train_data.columns if c not in ['sku_name','sku_coded']]\n",
    "cols_2 = [c for c in train_data.columns if c not in ['month','year', 'sku_name', 'sku_coded']]\n",
    "\n",
    "train_data[cols_1] = train_data.groupby('sku_name')[cols_1].transform(lambda x: x.fillna(x.median()))\n",
    "train_data[cols_2] = train_data.groupby(['month','year'])[cols_2].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuBCgvTjM6NP",
    "outputId": "ba82b130-fa0b-48de-a5c1-d926bc190b5a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 237799\tvalid_1's l1: 246382\n",
      "[20]\ttraining's l1: 194735\tvalid_1's l1: 204246\n",
      "[30]\ttraining's l1: 164267\tvalid_1's l1: 178719\n",
      "[40]\ttraining's l1: 142435\tvalid_1's l1: 163324\n",
      "[50]\ttraining's l1: 126835\tvalid_1's l1: 153900\n",
      "[60]\ttraining's l1: 115734\tvalid_1's l1: 147543\n",
      "[70]\ttraining's l1: 107641\tvalid_1's l1: 143654\n",
      "[80]\ttraining's l1: 101614\tvalid_1's l1: 141279\n",
      "[90]\ttraining's l1: 96841.7\tvalid_1's l1: 139889\n",
      "[100]\ttraining's l1: 93030.3\tvalid_1's l1: 138770\n",
      "[110]\ttraining's l1: 89848\tvalid_1's l1: 138203\n",
      "[120]\ttraining's l1: 87205.2\tvalid_1's l1: 137871\n",
      "[130]\ttraining's l1: 84847.6\tvalid_1's l1: 137805\n",
      "Early stopping, best iteration is:\n",
      "[125]\ttraining's l1: 85989.5\tvalid_1's l1: 137785\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 226513\tvalid_1's l1: 249276\n",
      "[20]\ttraining's l1: 188162\tvalid_1's l1: 214108\n",
      "[30]\ttraining's l1: 160801\tvalid_1's l1: 190050\n",
      "[40]\ttraining's l1: 141316\tvalid_1's l1: 173544\n",
      "[50]\ttraining's l1: 127367\tvalid_1's l1: 162469\n",
      "[60]\ttraining's l1: 117259\tvalid_1's l1: 154686\n",
      "[70]\ttraining's l1: 109724\tvalid_1's l1: 149063\n",
      "[80]\ttraining's l1: 103909\tvalid_1's l1: 145282\n",
      "[90]\ttraining's l1: 99295.3\tvalid_1's l1: 142334\n",
      "[100]\ttraining's l1: 95642.6\tvalid_1's l1: 140249\n",
      "[110]\ttraining's l1: 92708.2\tvalid_1's l1: 138614\n",
      "[120]\ttraining's l1: 90098.6\tvalid_1's l1: 137560\n",
      "[130]\ttraining's l1: 87757.2\tvalid_1's l1: 136875\n",
      "[140]\ttraining's l1: 85684.7\tvalid_1's l1: 136449\n",
      "[150]\ttraining's l1: 83794.7\tvalid_1's l1: 136064\n",
      "[160]\ttraining's l1: 82116.9\tvalid_1's l1: 135955\n",
      "[170]\ttraining's l1: 80544.5\tvalid_1's l1: 135811\n",
      "[180]\ttraining's l1: 79068.5\tvalid_1's l1: 135757\n",
      "Early stopping, best iteration is:\n",
      "[176]\ttraining's l1: 79654.3\tvalid_1's l1: 135722\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 226497\tvalid_1's l1: 231273\n",
      "[20]\ttraining's l1: 188098\tvalid_1's l1: 198993\n",
      "[30]\ttraining's l1: 160846\tvalid_1's l1: 176617\n",
      "[40]\ttraining's l1: 141370\tvalid_1's l1: 161832\n",
      "[50]\ttraining's l1: 127339\tvalid_1's l1: 151296\n",
      "[60]\ttraining's l1: 117013\tvalid_1's l1: 144111\n",
      "[70]\ttraining's l1: 109313\tvalid_1's l1: 138917\n",
      "[80]\ttraining's l1: 103431\tvalid_1's l1: 135218\n",
      "[90]\ttraining's l1: 98799.3\tvalid_1's l1: 132542\n",
      "[100]\ttraining's l1: 95061.8\tvalid_1's l1: 130904\n",
      "[110]\ttraining's l1: 91921.9\tvalid_1's l1: 129712\n",
      "[120]\ttraining's l1: 89115.9\tvalid_1's l1: 128904\n",
      "[130]\ttraining's l1: 86843.6\tvalid_1's l1: 128259\n",
      "[140]\ttraining's l1: 84667.1\tvalid_1's l1: 127569\n",
      "[150]\ttraining's l1: 82794.5\tvalid_1's l1: 127149\n",
      "[160]\ttraining's l1: 81089.7\tvalid_1's l1: 126924\n",
      "[170]\ttraining's l1: 79584.5\tvalid_1's l1: 126806\n",
      "[180]\ttraining's l1: 78236.9\tvalid_1's l1: 126623\n",
      "[190]\ttraining's l1: 76866.9\tvalid_1's l1: 126645\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's l1: 77395.2\tvalid_1's l1: 126569\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 235218\tvalid_1's l1: 212844\n",
      "[20]\ttraining's l1: 195523\tvalid_1's l1: 180681\n",
      "[30]\ttraining's l1: 167106\tvalid_1's l1: 159907\n",
      "[40]\ttraining's l1: 146894\tvalid_1's l1: 146616\n",
      "[50]\ttraining's l1: 132170\tvalid_1's l1: 137508\n",
      "[60]\ttraining's l1: 121405\tvalid_1's l1: 131734\n",
      "[70]\ttraining's l1: 113389\tvalid_1's l1: 127772\n",
      "[80]\ttraining's l1: 107080\tvalid_1's l1: 124962\n",
      "[90]\ttraining's l1: 102193\tvalid_1's l1: 123247\n",
      "[100]\ttraining's l1: 98138.6\tvalid_1's l1: 121872\n",
      "[110]\ttraining's l1: 94808.7\tvalid_1's l1: 121004\n",
      "[120]\ttraining's l1: 91972.6\tvalid_1's l1: 120322\n",
      "[130]\ttraining's l1: 89442.4\tvalid_1's l1: 119750\n",
      "[140]\ttraining's l1: 87283.5\tvalid_1's l1: 119232\n",
      "[150]\ttraining's l1: 85266.5\tvalid_1's l1: 118869\n",
      "[160]\ttraining's l1: 83459.2\tvalid_1's l1: 118596\n",
      "[170]\ttraining's l1: 81862.8\tvalid_1's l1: 118365\n",
      "[180]\ttraining's l1: 80424.8\tvalid_1's l1: 118305\n",
      "Early stopping, best iteration is:\n",
      "[179]\ttraining's l1: 80568.4\tvalid_1's l1: 118304\n",
      "\n",
      "\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 244612\tvalid_1's l1: 242280\n",
      "[20]\ttraining's l1: 199949\tvalid_1's l1: 199940\n",
      "[30]\ttraining's l1: 168525\tvalid_1's l1: 171859\n",
      "[40]\ttraining's l1: 146591\tvalid_1's l1: 153647\n",
      "[50]\ttraining's l1: 131023\tvalid_1's l1: 141924\n",
      "[60]\ttraining's l1: 119838\tvalid_1's l1: 134541\n",
      "[70]\ttraining's l1: 111620\tvalid_1's l1: 129685\n",
      "[80]\ttraining's l1: 105422\tvalid_1's l1: 126212\n",
      "[90]\ttraining's l1: 100588\tvalid_1's l1: 124016\n",
      "[100]\ttraining's l1: 96601.9\tvalid_1's l1: 122555\n",
      "[110]\ttraining's l1: 93284.9\tvalid_1's l1: 121710\n",
      "[120]\ttraining's l1: 90528.3\tvalid_1's l1: 121147\n",
      "[130]\ttraining's l1: 88202.1\tvalid_1's l1: 120716\n",
      "[140]\ttraining's l1: 86199.2\tvalid_1's l1: 120243\n",
      "[150]\ttraining's l1: 84382.4\tvalid_1's l1: 120108\n",
      "[160]\ttraining's l1: 82668.1\tvalid_1's l1: 119938\n",
      "[170]\ttraining's l1: 81033.1\tvalid_1's l1: 119744\n",
      "[180]\ttraining's l1: 79647.3\tvalid_1's l1: 119703\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttraining's l1: 80069.7\tvalid_1's l1: 119661\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 220289\tvalid_1's l1: 254712\n",
      "[20]\ttraining's l1: 184124\tvalid_1's l1: 217976\n",
      "[30]\ttraining's l1: 158319\tvalid_1's l1: 192605\n",
      "[40]\ttraining's l1: 139886\tvalid_1's l1: 174973\n",
      "[50]\ttraining's l1: 126780\tvalid_1's l1: 162802\n",
      "[60]\ttraining's l1: 117236\tvalid_1's l1: 154441\n",
      "[70]\ttraining's l1: 109845\tvalid_1's l1: 148672\n",
      "[80]\ttraining's l1: 104151\tvalid_1's l1: 144221\n",
      "[90]\ttraining's l1: 99687.7\tvalid_1's l1: 141139\n",
      "[100]\ttraining's l1: 96134.4\tvalid_1's l1: 139032\n",
      "[110]\ttraining's l1: 93044.5\tvalid_1's l1: 137636\n",
      "[120]\ttraining's l1: 90378.4\tvalid_1's l1: 136424\n",
      "[130]\ttraining's l1: 88067.8\tvalid_1's l1: 135652\n",
      "[140]\ttraining's l1: 85888.7\tvalid_1's l1: 135106\n",
      "[150]\ttraining's l1: 84011\tvalid_1's l1: 134802\n",
      "[160]\ttraining's l1: 82286\tvalid_1's l1: 134512\n",
      "[170]\ttraining's l1: 80734.2\tvalid_1's l1: 134333\n",
      "Early stopping, best iteration is:\n",
      "[170]\ttraining's l1: 80734.2\tvalid_1's l1: 134333\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 235783\tvalid_1's l1: 220650\n",
      "[20]\ttraining's l1: 196029\tvalid_1's l1: 184746\n",
      "[30]\ttraining's l1: 167838\tvalid_1's l1: 161732\n",
      "[40]\ttraining's l1: 147793\tvalid_1's l1: 147450\n",
      "[50]\ttraining's l1: 133530\tvalid_1's l1: 138700\n",
      "[60]\ttraining's l1: 122946\tvalid_1's l1: 133037\n",
      "[70]\ttraining's l1: 115145\tvalid_1's l1: 129524\n",
      "[80]\ttraining's l1: 109068\tvalid_1's l1: 127215\n",
      "[90]\ttraining's l1: 104293\tvalid_1's l1: 125510\n",
      "[100]\ttraining's l1: 100372\tvalid_1's l1: 124434\n",
      "[110]\ttraining's l1: 97114.3\tvalid_1's l1: 123626\n",
      "[120]\ttraining's l1: 94334\tvalid_1's l1: 122989\n",
      "[130]\ttraining's l1: 91902.7\tvalid_1's l1: 122580\n",
      "[140]\ttraining's l1: 89739.1\tvalid_1's l1: 122270\n",
      "[150]\ttraining's l1: 87824.2\tvalid_1's l1: 122067\n",
      "Early stopping, best iteration is:\n",
      "[154]\ttraining's l1: 87091.6\tvalid_1's l1: 121952\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 214626\tvalid_1's l1: 239113\n",
      "[20]\ttraining's l1: 180055\tvalid_1's l1: 204972\n",
      "[30]\ttraining's l1: 155497\tvalid_1's l1: 181728\n",
      "[40]\ttraining's l1: 137805\tvalid_1's l1: 165555\n",
      "[50]\ttraining's l1: 125007\tvalid_1's l1: 154569\n",
      "[60]\ttraining's l1: 115664\tvalid_1's l1: 147006\n",
      "[70]\ttraining's l1: 108685\tvalid_1's l1: 141587\n",
      "[80]\ttraining's l1: 103249\tvalid_1's l1: 137712\n",
      "[90]\ttraining's l1: 98921.1\tvalid_1's l1: 134888\n",
      "[100]\ttraining's l1: 95204.1\tvalid_1's l1: 133009\n",
      "[110]\ttraining's l1: 92108.2\tvalid_1's l1: 131668\n",
      "[120]\ttraining's l1: 89561\tvalid_1's l1: 130754\n",
      "[130]\ttraining's l1: 87258.6\tvalid_1's l1: 130073\n",
      "[140]\ttraining's l1: 85151.4\tvalid_1's l1: 129498\n",
      "[150]\ttraining's l1: 83187.8\tvalid_1's l1: 128966\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's l1: 82651\tvalid_1's l1: 128810\n",
      "\n",
      "\n",
      "Elapsed 0.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 246114\tvalid_1's l1: 245717\n",
      "[20]\ttraining's l1: 200244\tvalid_1's l1: 207541\n",
      "[30]\ttraining's l1: 168049\tvalid_1's l1: 181764\n",
      "[40]\ttraining's l1: 145261\tvalid_1's l1: 164542\n",
      "[50]\ttraining's l1: 129056\tvalid_1's l1: 152783\n",
      "[60]\ttraining's l1: 117438\tvalid_1's l1: 144720\n",
      "[70]\ttraining's l1: 108963\tvalid_1's l1: 139219\n",
      "[80]\ttraining's l1: 102529\tvalid_1's l1: 135078\n",
      "[90]\ttraining's l1: 97417\tvalid_1's l1: 132388\n",
      "[100]\ttraining's l1: 93472.4\tvalid_1's l1: 130434\n",
      "[110]\ttraining's l1: 90238.4\tvalid_1's l1: 128996\n",
      "[120]\ttraining's l1: 87538.5\tvalid_1's l1: 127960\n",
      "[130]\ttraining's l1: 85189.5\tvalid_1's l1: 127370\n",
      "[140]\ttraining's l1: 83145\tvalid_1's l1: 126838\n",
      "[150]\ttraining's l1: 81393\tvalid_1's l1: 126490\n",
      "[160]\ttraining's l1: 79693.1\tvalid_1's l1: 126253\n",
      "[170]\ttraining's l1: 78164.5\tvalid_1's l1: 126083\n",
      "[180]\ttraining's l1: 76831.9\tvalid_1's l1: 125955\n",
      "[190]\ttraining's l1: 75546\tvalid_1's l1: 125838\n",
      "[200]\ttraining's l1: 74366.3\tvalid_1's l1: 125664\n",
      "[210]\ttraining's l1: 73254\tvalid_1's l1: 125567\n",
      "[220]\ttraining's l1: 72151.6\tvalid_1's l1: 125544\n",
      "[230]\ttraining's l1: 71084\tvalid_1's l1: 125474\n",
      "[240]\ttraining's l1: 70117.9\tvalid_1's l1: 125393\n",
      "Early stopping, best iteration is:\n",
      "[243]\ttraining's l1: 69834.5\tvalid_1's l1: 125350\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 256840\tvalid_1's l1: 215300\n",
      "[20]\ttraining's l1: 211666\tvalid_1's l1: 180926\n",
      "[30]\ttraining's l1: 179652\tvalid_1's l1: 159903\n",
      "[40]\ttraining's l1: 156742\tvalid_1's l1: 147822\n",
      "[50]\ttraining's l1: 140430\tvalid_1's l1: 140860\n",
      "[60]\ttraining's l1: 128428\tvalid_1's l1: 136531\n",
      "[70]\ttraining's l1: 119665\tvalid_1's l1: 133665\n",
      "[80]\ttraining's l1: 112865\tvalid_1's l1: 131847\n",
      "[90]\ttraining's l1: 107571\tvalid_1's l1: 130525\n",
      "[100]\ttraining's l1: 103301\tvalid_1's l1: 129592\n",
      "[110]\ttraining's l1: 99784.2\tvalid_1's l1: 129009\n",
      "[120]\ttraining's l1: 96880.4\tvalid_1's l1: 128618\n",
      "[130]\ttraining's l1: 94365.6\tvalid_1's l1: 128448\n",
      "[140]\ttraining's l1: 92158.8\tvalid_1's l1: 128146\n",
      "[150]\ttraining's l1: 90195.6\tvalid_1's l1: 127983\n",
      "[160]\ttraining's l1: 88318.7\tvalid_1's l1: 127766\n",
      "[170]\ttraining's l1: 86545.8\tvalid_1's l1: 127650\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttraining's l1: 87253.1\tvalid_1's l1: 127617\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 218515\tvalid_1's l1: 241585\n",
      "[20]\ttraining's l1: 182243\tvalid_1's l1: 208287\n",
      "[30]\ttraining's l1: 156423\tvalid_1's l1: 185205\n",
      "[40]\ttraining's l1: 137935\tvalid_1's l1: 169231\n",
      "[50]\ttraining's l1: 124738\tvalid_1's l1: 158591\n",
      "[60]\ttraining's l1: 115134\tvalid_1's l1: 150829\n",
      "[70]\ttraining's l1: 107813\tvalid_1's l1: 145550\n",
      "[80]\ttraining's l1: 102259\tvalid_1's l1: 141803\n",
      "[90]\ttraining's l1: 97812\tvalid_1's l1: 139327\n",
      "[100]\ttraining's l1: 94261.5\tvalid_1's l1: 137438\n",
      "[110]\ttraining's l1: 91296.7\tvalid_1's l1: 136159\n",
      "[120]\ttraining's l1: 88714.2\tvalid_1's l1: 135226\n",
      "[130]\ttraining's l1: 86503.7\tvalid_1's l1: 134637\n",
      "[140]\ttraining's l1: 84517.2\tvalid_1's l1: 134206\n",
      "[150]\ttraining's l1: 82720.8\tvalid_1's l1: 133942\n",
      "[160]\ttraining's l1: 81041.7\tvalid_1's l1: 133715\n",
      "[170]\ttraining's l1: 79547.7\tvalid_1's l1: 133613\n",
      "Early stopping, best iteration is:\n",
      "[170]\ttraining's l1: 79547.7\tvalid_1's l1: 133613\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 219793\tvalid_1's l1: 226430\n",
      "[20]\ttraining's l1: 182982\tvalid_1's l1: 192299\n",
      "[30]\ttraining's l1: 156865\tvalid_1's l1: 170654\n",
      "[40]\ttraining's l1: 138288\tvalid_1's l1: 156644\n",
      "[50]\ttraining's l1: 124821\tvalid_1's l1: 147832\n",
      "[60]\ttraining's l1: 114978\tvalid_1's l1: 142163\n",
      "[70]\ttraining's l1: 107590\tvalid_1's l1: 138142\n",
      "[80]\ttraining's l1: 101775\tvalid_1's l1: 135400\n",
      "[90]\ttraining's l1: 97165.3\tvalid_1's l1: 133548\n",
      "[100]\ttraining's l1: 93399.7\tvalid_1's l1: 132035\n",
      "[110]\ttraining's l1: 90215.5\tvalid_1's l1: 131127\n",
      "[120]\ttraining's l1: 87569.2\tvalid_1's l1: 130492\n",
      "[130]\ttraining's l1: 85124.7\tvalid_1's l1: 130072\n",
      "[140]\ttraining's l1: 83062.2\tvalid_1's l1: 129629\n",
      "[150]\ttraining's l1: 81200\tvalid_1's l1: 129307\n",
      "[160]\ttraining's l1: 79490.6\tvalid_1's l1: 129046\n",
      "[170]\ttraining's l1: 78086\tvalid_1's l1: 128896\n",
      "[180]\ttraining's l1: 76656.6\tvalid_1's l1: 128762\n",
      "[190]\ttraining's l1: 75354.1\tvalid_1's l1: 128582\n",
      "[200]\ttraining's l1: 74123.5\tvalid_1's l1: 128521\n",
      "Early stopping, best iteration is:\n",
      "[196]\ttraining's l1: 74615\tvalid_1's l1: 128486\n",
      "\n",
      "\n",
      "Elapsed 0.14 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds = 3\n",
    "use_gpu = False\n",
    "\n",
    "params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'device_type': 'gpu' if use_gpu else 'cpu',\n",
    "                #'gpu_use_dp': 'true',\n",
    "                'n_jobs': -1,\n",
    "                'learning_rate': 0.03,\n",
    "                #'bagging_fraction': 0.85,\n",
    "                #'bagging_freq': 10, \n",
    "                'colsample_bytree': 0.85,\n",
    "                'colsample_bynode': 0.85,\n",
    "                'min_data_per_leaf': 25,\n",
    "                'max_bin':63,\n",
    "                'num_leaves': 125,\n",
    "                'lambda_l1': 0.5,\n",
    "                'lambda_l2': 0.5,\n",
    "                \"metric\": \"mae\",\n",
    "#                 \"min_delta\":0.0003,\n",
    "                'seed': SEED,\n",
    "                'verbose':-1\n",
    "              }  \n",
    "cv_models = train_model(train_data, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHlBjEK_Y9Jq",
    "outputId": "84b05bd9-7679-45dd-a7e4-6f7e6cc12b84",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "Average Val MAE: 128194.74215690416\n"
     ]
    }
   ],
   "source": [
    "val_mae = test_model(train_data, folds, cv_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuBCgvTjM6NP",
    "outputId": "ba82b130-fa0b-48de-a5c1-d926bc190b5a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 237735\tvalid_1's l1: 246615\n",
      "[20]\ttraining's l1: 194757\tvalid_1's l1: 204461\n",
      "[30]\ttraining's l1: 164355\tvalid_1's l1: 178536\n",
      "[40]\ttraining's l1: 142535\tvalid_1's l1: 162918\n",
      "[50]\ttraining's l1: 126980\tvalid_1's l1: 153320\n",
      "[60]\ttraining's l1: 115870\tvalid_1's l1: 147585\n",
      "[70]\ttraining's l1: 107860\tvalid_1's l1: 143704\n",
      "[80]\ttraining's l1: 101637\tvalid_1's l1: 141159\n",
      "[90]\ttraining's l1: 96890.9\tvalid_1's l1: 139369\n",
      "[100]\ttraining's l1: 93101.4\tvalid_1's l1: 138360\n",
      "[110]\ttraining's l1: 89960.3\tvalid_1's l1: 137970\n",
      "[120]\ttraining's l1: 87444.8\tvalid_1's l1: 137554\n",
      "[130]\ttraining's l1: 85232\tvalid_1's l1: 137240\n",
      "[140]\ttraining's l1: 83188\tvalid_1's l1: 136923\n",
      "[150]\ttraining's l1: 81315.8\tvalid_1's l1: 136658\n",
      "[160]\ttraining's l1: 79641.9\tvalid_1's l1: 136516\n",
      "Early stopping, best iteration is:\n",
      "[159]\ttraining's l1: 79805.1\tvalid_1's l1: 136509\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 226637\tvalid_1's l1: 249616\n",
      "[20]\ttraining's l1: 188311\tvalid_1's l1: 214490\n",
      "[30]\ttraining's l1: 160994\tvalid_1's l1: 190711\n",
      "[40]\ttraining's l1: 141458\tvalid_1's l1: 173841\n",
      "[50]\ttraining's l1: 127299\tvalid_1's l1: 162482\n",
      "[60]\ttraining's l1: 117097\tvalid_1's l1: 154807\n",
      "[70]\ttraining's l1: 109507\tvalid_1's l1: 149263\n",
      "[80]\ttraining's l1: 103692\tvalid_1's l1: 145392\n",
      "[90]\ttraining's l1: 99107.2\tvalid_1's l1: 142602\n",
      "[100]\ttraining's l1: 95561.3\tvalid_1's l1: 140548\n",
      "[110]\ttraining's l1: 92540.3\tvalid_1's l1: 139045\n",
      "[120]\ttraining's l1: 89926.1\tvalid_1's l1: 137961\n",
      "[130]\ttraining's l1: 87670.8\tvalid_1's l1: 137155\n",
      "[140]\ttraining's l1: 85658\tvalid_1's l1: 136648\n",
      "[150]\ttraining's l1: 83855.9\tvalid_1's l1: 136416\n",
      "[160]\ttraining's l1: 82154.2\tvalid_1's l1: 136256\n",
      "[170]\ttraining's l1: 80549.5\tvalid_1's l1: 135958\n",
      "[180]\ttraining's l1: 79118.2\tvalid_1's l1: 135813\n",
      "[190]\ttraining's l1: 77733.6\tvalid_1's l1: 135758\n",
      "Early stopping, best iteration is:\n",
      "[188]\ttraining's l1: 78027.2\tvalid_1's l1: 135738\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 226648\tvalid_1's l1: 231748\n",
      "[20]\ttraining's l1: 188304\tvalid_1's l1: 198088\n",
      "[30]\ttraining's l1: 160868\tvalid_1's l1: 176090\n",
      "[40]\ttraining's l1: 141306\tvalid_1's l1: 160943\n",
      "[50]\ttraining's l1: 127220\tvalid_1's l1: 150747\n",
      "[60]\ttraining's l1: 116911\tvalid_1's l1: 143338\n",
      "[70]\ttraining's l1: 109337\tvalid_1's l1: 138395\n",
      "[80]\ttraining's l1: 103463\tvalid_1's l1: 135007\n",
      "[90]\ttraining's l1: 98919.4\tvalid_1's l1: 132574\n",
      "[100]\ttraining's l1: 95129.3\tvalid_1's l1: 130794\n",
      "[110]\ttraining's l1: 91999\tvalid_1's l1: 129460\n",
      "[120]\ttraining's l1: 89333.7\tvalid_1's l1: 128550\n",
      "[130]\ttraining's l1: 87003.9\tvalid_1's l1: 127995\n",
      "[140]\ttraining's l1: 84897.2\tvalid_1's l1: 127483\n",
      "[150]\ttraining's l1: 83106.7\tvalid_1's l1: 127148\n",
      "[160]\ttraining's l1: 81469.5\tvalid_1's l1: 126884\n",
      "[170]\ttraining's l1: 79946.1\tvalid_1's l1: 126775\n",
      "[180]\ttraining's l1: 78487.1\tvalid_1's l1: 126677\n",
      "Early stopping, best iteration is:\n",
      "[183]\ttraining's l1: 78073.3\tvalid_1's l1: 126613\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 235379\tvalid_1's l1: 213548\n",
      "[20]\ttraining's l1: 195607\tvalid_1's l1: 181343\n",
      "[30]\ttraining's l1: 167093\tvalid_1's l1: 160529\n",
      "[40]\ttraining's l1: 146826\tvalid_1's l1: 146969\n",
      "[50]\ttraining's l1: 132300\tvalid_1's l1: 138192\n",
      "[60]\ttraining's l1: 121601\tvalid_1's l1: 132129\n",
      "[70]\ttraining's l1: 113579\tvalid_1's l1: 128098\n",
      "[80]\ttraining's l1: 107423\tvalid_1's l1: 125244\n",
      "[90]\ttraining's l1: 102387\tvalid_1's l1: 123411\n",
      "[100]\ttraining's l1: 98423.5\tvalid_1's l1: 122050\n",
      "[110]\ttraining's l1: 95020.5\tvalid_1's l1: 121138\n",
      "[120]\ttraining's l1: 92200.6\tvalid_1's l1: 120662\n",
      "[130]\ttraining's l1: 89509.9\tvalid_1's l1: 120205\n",
      "[140]\ttraining's l1: 87304.4\tvalid_1's l1: 119760\n",
      "[150]\ttraining's l1: 85283.5\tvalid_1's l1: 119481\n",
      "[160]\ttraining's l1: 83573.1\tvalid_1's l1: 119307\n",
      "Early stopping, best iteration is:\n",
      "[160]\ttraining's l1: 83573.1\tvalid_1's l1: 119307\n",
      "\n",
      "\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 244688\tvalid_1's l1: 242454\n",
      "[20]\ttraining's l1: 200067\tvalid_1's l1: 200522\n",
      "[30]\ttraining's l1: 168865\tvalid_1's l1: 172527\n",
      "[40]\ttraining's l1: 146864\tvalid_1's l1: 154232\n",
      "[50]\ttraining's l1: 131414\tvalid_1's l1: 142026\n",
      "[60]\ttraining's l1: 120244\tvalid_1's l1: 134539\n",
      "[70]\ttraining's l1: 111995\tvalid_1's l1: 129534\n",
      "[80]\ttraining's l1: 105767\tvalid_1's l1: 126169\n",
      "[90]\ttraining's l1: 100956\tvalid_1's l1: 123770\n",
      "[100]\ttraining's l1: 96889\tvalid_1's l1: 122328\n",
      "[110]\ttraining's l1: 93645.6\tvalid_1's l1: 121398\n",
      "[120]\ttraining's l1: 90807.7\tvalid_1's l1: 120622\n",
      "[130]\ttraining's l1: 88397.9\tvalid_1's l1: 120122\n",
      "[140]\ttraining's l1: 86341.3\tvalid_1's l1: 119768\n",
      "[150]\ttraining's l1: 84472.8\tvalid_1's l1: 119431\n",
      "Early stopping, best iteration is:\n",
      "[151]\ttraining's l1: 84287.3\tvalid_1's l1: 119419\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 220352\tvalid_1's l1: 254872\n",
      "[20]\ttraining's l1: 183974\tvalid_1's l1: 217972\n",
      "[30]\ttraining's l1: 158279\tvalid_1's l1: 192584\n",
      "[40]\ttraining's l1: 139842\tvalid_1's l1: 174803\n",
      "[50]\ttraining's l1: 126692\tvalid_1's l1: 162921\n",
      "[60]\ttraining's l1: 117065\tvalid_1's l1: 154623\n",
      "[70]\ttraining's l1: 109727\tvalid_1's l1: 148431\n",
      "[80]\ttraining's l1: 104054\tvalid_1's l1: 144094\n",
      "[90]\ttraining's l1: 99565\tvalid_1's l1: 141143\n",
      "[100]\ttraining's l1: 95930.2\tvalid_1's l1: 139027\n",
      "[110]\ttraining's l1: 92974.9\tvalid_1's l1: 137568\n",
      "[120]\ttraining's l1: 90351.3\tvalid_1's l1: 136573\n",
      "[130]\ttraining's l1: 88065.8\tvalid_1's l1: 136085\n",
      "[140]\ttraining's l1: 85938.7\tvalid_1's l1: 135523\n",
      "[150]\ttraining's l1: 84086.1\tvalid_1's l1: 135240\n",
      "[160]\ttraining's l1: 82355.5\tvalid_1's l1: 135043\n",
      "Early stopping, best iteration is:\n",
      "[159]\ttraining's l1: 82526.7\tvalid_1's l1: 134990\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 235655\tvalid_1's l1: 220593\n",
      "[20]\ttraining's l1: 196004\tvalid_1's l1: 184976\n",
      "[30]\ttraining's l1: 167858\tvalid_1's l1: 161615\n",
      "[40]\ttraining's l1: 147878\tvalid_1's l1: 147174\n",
      "[50]\ttraining's l1: 133525\tvalid_1's l1: 138445\n",
      "[60]\ttraining's l1: 122890\tvalid_1's l1: 132720\n",
      "[70]\ttraining's l1: 115029\tvalid_1's l1: 129152\n",
      "[80]\ttraining's l1: 108939\tvalid_1's l1: 126824\n",
      "[90]\ttraining's l1: 104161\tvalid_1's l1: 125233\n",
      "[100]\ttraining's l1: 100271\tvalid_1's l1: 123992\n",
      "[110]\ttraining's l1: 96969.6\tvalid_1's l1: 123207\n",
      "[120]\ttraining's l1: 94204.8\tvalid_1's l1: 122743\n",
      "[130]\ttraining's l1: 91796.1\tvalid_1's l1: 122288\n",
      "[140]\ttraining's l1: 89676.8\tvalid_1's l1: 122084\n",
      "[150]\ttraining's l1: 87689.8\tvalid_1's l1: 121845\n",
      "[160]\ttraining's l1: 85917.3\tvalid_1's l1: 121811\n",
      "Early stopping, best iteration is:\n",
      "[157]\ttraining's l1: 86448.2\tvalid_1's l1: 121760\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 214689\tvalid_1's l1: 239146\n",
      "[20]\ttraining's l1: 180040\tvalid_1's l1: 204716\n",
      "[30]\ttraining's l1: 155497\tvalid_1's l1: 181628\n",
      "[40]\ttraining's l1: 137857\tvalid_1's l1: 165937\n",
      "[50]\ttraining's l1: 125000\tvalid_1's l1: 155301\n",
      "[60]\ttraining's l1: 115686\tvalid_1's l1: 147876\n",
      "[70]\ttraining's l1: 108762\tvalid_1's l1: 142703\n",
      "[80]\ttraining's l1: 103348\tvalid_1's l1: 138911\n",
      "[90]\ttraining's l1: 99077.3\tvalid_1's l1: 136198\n",
      "[100]\ttraining's l1: 95392.7\tvalid_1's l1: 134142\n",
      "[110]\ttraining's l1: 92296.6\tvalid_1's l1: 132645\n",
      "[120]\ttraining's l1: 89681.1\tvalid_1's l1: 131598\n",
      "[130]\ttraining's l1: 87329\tvalid_1's l1: 130864\n",
      "[140]\ttraining's l1: 85218.1\tvalid_1's l1: 130297\n",
      "[150]\ttraining's l1: 83337.7\tvalid_1's l1: 129773\n",
      "[160]\ttraining's l1: 81619.8\tvalid_1's l1: 129573\n",
      "[170]\ttraining's l1: 80098.7\tvalid_1's l1: 129464\n",
      "[180]\ttraining's l1: 78666\tvalid_1's l1: 129300\n",
      "[190]\ttraining's l1: 77320.6\tvalid_1's l1: 129210\n",
      "[200]\ttraining's l1: 76050.4\tvalid_1's l1: 129149\n",
      "[210]\ttraining's l1: 74927.9\tvalid_1's l1: 129049\n",
      "Early stopping, best iteration is:\n",
      "[209]\ttraining's l1: 75045\tvalid_1's l1: 129034\n",
      "\n",
      "\n",
      "Elapsed 0.10 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 246176\tvalid_1's l1: 245720\n",
      "[20]\ttraining's l1: 200345\tvalid_1's l1: 207271\n",
      "[30]\ttraining's l1: 168105\tvalid_1's l1: 181263\n",
      "[40]\ttraining's l1: 145319\tvalid_1's l1: 163988\n",
      "[50]\ttraining's l1: 129169\tvalid_1's l1: 152389\n",
      "[60]\ttraining's l1: 117468\tvalid_1's l1: 144522\n",
      "[70]\ttraining's l1: 108848\tvalid_1's l1: 138994\n",
      "[80]\ttraining's l1: 102502\tvalid_1's l1: 135238\n",
      "[90]\ttraining's l1: 97452.8\tvalid_1's l1: 132583\n",
      "[100]\ttraining's l1: 93422.5\tvalid_1's l1: 130472\n",
      "[110]\ttraining's l1: 90236.8\tvalid_1's l1: 128967\n",
      "[120]\ttraining's l1: 87512.2\tvalid_1's l1: 127894\n",
      "[130]\ttraining's l1: 85220.3\tvalid_1's l1: 127215\n",
      "[140]\ttraining's l1: 83121.3\tvalid_1's l1: 126598\n",
      "[150]\ttraining's l1: 81338.7\tvalid_1's l1: 126275\n",
      "[160]\ttraining's l1: 79707\tvalid_1's l1: 125958\n",
      "[170]\ttraining's l1: 78187.4\tvalid_1's l1: 125840\n",
      "[180]\ttraining's l1: 76795.6\tvalid_1's l1: 125669\n",
      "[190]\ttraining's l1: 75489.8\tvalid_1's l1: 125597\n",
      "[200]\ttraining's l1: 74314.8\tvalid_1's l1: 125472\n",
      "[210]\ttraining's l1: 73193\tvalid_1's l1: 125428\n",
      "Early stopping, best iteration is:\n",
      "[213]\ttraining's l1: 72847.8\tvalid_1's l1: 125408\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 256735\tvalid_1's l1: 215230\n",
      "[20]\ttraining's l1: 211469\tvalid_1's l1: 180958\n",
      "[30]\ttraining's l1: 179347\tvalid_1's l1: 159815\n",
      "[40]\ttraining's l1: 156446\tvalid_1's l1: 148015\n",
      "[50]\ttraining's l1: 140083\tvalid_1's l1: 141109\n",
      "[60]\ttraining's l1: 128279\tvalid_1's l1: 136714\n",
      "[70]\ttraining's l1: 119462\tvalid_1's l1: 133881\n",
      "[80]\ttraining's l1: 112727\tvalid_1's l1: 132247\n",
      "[90]\ttraining's l1: 107521\tvalid_1's l1: 131115\n",
      "[100]\ttraining's l1: 103215\tvalid_1's l1: 130285\n",
      "[110]\ttraining's l1: 99697.9\tvalid_1's l1: 129603\n",
      "[120]\ttraining's l1: 96805.8\tvalid_1's l1: 128905\n",
      "[130]\ttraining's l1: 94235.4\tvalid_1's l1: 128553\n",
      "[140]\ttraining's l1: 91943.3\tvalid_1's l1: 128252\n",
      "[150]\ttraining's l1: 89984.6\tvalid_1's l1: 128047\n",
      "[160]\ttraining's l1: 88125.7\tvalid_1's l1: 128005\n",
      "[170]\ttraining's l1: 86428.7\tvalid_1's l1: 127867\n",
      "Early stopping, best iteration is:\n",
      "[169]\ttraining's l1: 86625.5\tvalid_1's l1: 127858\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 218616\tvalid_1's l1: 241875\n",
      "[20]\ttraining's l1: 182312\tvalid_1's l1: 209029\n",
      "[30]\ttraining's l1: 156562\tvalid_1's l1: 185818\n",
      "[40]\ttraining's l1: 138055\tvalid_1's l1: 169466\n",
      "[50]\ttraining's l1: 124861\tvalid_1's l1: 158893\n",
      "[60]\ttraining's l1: 115172\tvalid_1's l1: 151111\n",
      "[70]\ttraining's l1: 107940\tvalid_1's l1: 145636\n",
      "[80]\ttraining's l1: 102397\tvalid_1's l1: 141713\n",
      "[90]\ttraining's l1: 97935.8\tvalid_1's l1: 138963\n",
      "[100]\ttraining's l1: 94406.4\tvalid_1's l1: 137176\n",
      "[110]\ttraining's l1: 91452.1\tvalid_1's l1: 135727\n",
      "[120]\ttraining's l1: 88928.7\tvalid_1's l1: 134759\n",
      "[130]\ttraining's l1: 86653\tvalid_1's l1: 134157\n",
      "[140]\ttraining's l1: 84719.4\tvalid_1's l1: 133896\n",
      "[150]\ttraining's l1: 82930.9\tvalid_1's l1: 133691\n",
      "[160]\ttraining's l1: 81283\tvalid_1's l1: 133402\n",
      "[170]\ttraining's l1: 79816.6\tvalid_1's l1: 133208\n",
      "[180]\ttraining's l1: 78411.6\tvalid_1's l1: 133111\n",
      "[190]\ttraining's l1: 77104.9\tvalid_1's l1: 132985\n",
      "[200]\ttraining's l1: 75873.1\tvalid_1's l1: 132939\n",
      "Early stopping, best iteration is:\n",
      "[203]\ttraining's l1: 75516.2\tvalid_1's l1: 132902\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 219670\tvalid_1's l1: 226214\n",
      "[20]\ttraining's l1: 183028\tvalid_1's l1: 192432\n",
      "[30]\ttraining's l1: 156817\tvalid_1's l1: 170653\n",
      "[40]\ttraining's l1: 138100\tvalid_1's l1: 156987\n",
      "[50]\ttraining's l1: 124684\tvalid_1's l1: 148171\n",
      "[60]\ttraining's l1: 114822\tvalid_1's l1: 142535\n",
      "[70]\ttraining's l1: 107482\tvalid_1's l1: 138890\n",
      "[80]\ttraining's l1: 101782\tvalid_1's l1: 136274\n",
      "[90]\ttraining's l1: 97227.2\tvalid_1's l1: 134427\n",
      "[100]\ttraining's l1: 93295.9\tvalid_1's l1: 133056\n",
      "[110]\ttraining's l1: 90162.1\tvalid_1's l1: 131999\n",
      "[120]\ttraining's l1: 87484.3\tvalid_1's l1: 131131\n",
      "[130]\ttraining's l1: 85132.8\tvalid_1's l1: 130490\n",
      "[140]\ttraining's l1: 83077.8\tvalid_1's l1: 130136\n",
      "[150]\ttraining's l1: 81164.2\tvalid_1's l1: 129721\n",
      "[160]\ttraining's l1: 79441\tvalid_1's l1: 129325\n",
      "[170]\ttraining's l1: 77943.4\tvalid_1's l1: 129122\n",
      "[180]\ttraining's l1: 76514.9\tvalid_1's l1: 129011\n",
      "[190]\ttraining's l1: 75304.9\tvalid_1's l1: 128867\n",
      "Early stopping, best iteration is:\n",
      "[193]\ttraining's l1: 74914.4\tvalid_1's l1: 128798\n",
      "\n",
      "\n",
      "Elapsed 0.16 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds = 3\n",
    "use_gpu = False\n",
    "\n",
    "params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'device_type': 'gpu' if use_gpu else 'cpu',\n",
    "                #'gpu_use_dp': 'true',\n",
    "                'n_jobs': -1,\n",
    "                'learning_rate': 0.03,\n",
    "                #'bagging_fraction': 0.85,\n",
    "                #'bagging_freq': 10, \n",
    "                'colsample_bytree': 0.85,\n",
    "                'colsample_bynode': 0.85,\n",
    "                'min_data_per_leaf': 25,\n",
    "                'max_bin':63,\n",
    "                'num_leaves': 125,\n",
    "                'lambda_l1': 0.5,\n",
    "                'lambda_l2': 0.5,\n",
    "                \"metric\": \"mae\",\n",
    "#                 \"min_delta\":0.0003,\n",
    "                'seed': SEED,\n",
    "                'verbose':-1\n",
    "              }  \n",
    "cv_models = train_model(train_data, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHlBjEK_Y9Jq",
    "outputId": "84b05bd9-7679-45dd-a7e4-6f7e6cc12b84",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "Average Val MAE: 128205.37652001537\n"
     ]
    }
   ],
   "source": [
    "val_mae = test_model(train_data, folds, cv_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lxV6wYtqO84D",
    "outputId": "603a71b3-dc8e-4e8c-b9b0-b45eb0b87bed",
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance = cv_feature_importance_plot(train_data, folds, cv_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32db4232347346569d843e136ea349ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3868 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08adfc8b09de4125bcad09f09fba2fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "np.random.seed(SEED)\n",
    "sku_encoder = LabelEncoder(train.sku_name.sample(frac=0.95).unique())\n",
    "\n",
    "dates = sorted([(m, y) for y,m in train_df.groupby(['year', 'month']).groups.keys()][:-1], key=lambda d: (d[1], d[0]))\n",
    "train_data = train_df.groupby('sku_name').progress_apply(prepare_data, dates=dates).reset_index(drop=True)\n",
    "\n",
    "train_data['sku_coded'] = train_data['sku_name'].apply(sku_encoder)\n",
    "train_data = train_data.groupby(['month','year']).progress_apply(clean_data, encoder=sku_encoder).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_1 = [c for c in train_data.columns if c not in ['sku_name','sku_coded']]\n",
    "cols_2 = [c for c in train_data.columns if c not in ['month','year', 'sku_name', 'sku_coded']]\n",
    "\n",
    "train_data[cols_1] = train_data.groupby('sku_name')[cols_1].transform(lambda x: x.fillna(x.median()))\n",
    "train_data[cols_2] = train_data.groupby(['month','year'])[cols_2].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_data[[c for c in train_data.columns if 'target' not in c]]\n",
    "y_train = train_data[[c for c in train_data.columns if 'target' in c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuBCgvTjM6NP",
    "outputId": "ba82b130-fa0b-48de-a5c1-d926bc190b5a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for timestep 1 forecasting\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_gpu = False\n",
    "\n",
    "params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'device_type': 'gpu' if use_gpu else 'cpu',\n",
    "                #'gpu_use_dp': 'true',\n",
    "                'n_jobs': -1,\n",
    "                'learning_rate': 0.03,\n",
    "                #'bagging_fraction': 0.85,\n",
    "                #'bagging_freq': 10, \n",
    "                'colsample_bytree': 0.85,\n",
    "                'colsample_bynode': 0.85,\n",
    "                'min_data_per_leaf': 25,\n",
    "                'max_bin':63,\n",
    "                'num_leaves': 125,\n",
    "                'lambda_l1': 0.5,\n",
    "                'lambda_l2': 0.5,\n",
    "                \"metric\": \"mae\",\n",
    "#                 \"min_delta\":0.0003,\n",
    "                'seed': SEED,\n",
    "                'verbose':-1\n",
    "              }  \n",
    "\n",
    "models = multi_step_train(params, x_train, y_train, False, num_rounds=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_context(x, missing_sku):\n",
    "    missing_df = pd.DataFrame(np.nan, index=range(len(missing_sku)), columns=x.columns)\n",
    "    missing_df['sku_name'] = missing_sku\n",
    "    missing_df['month'] = x['month'].iloc[0]    \n",
    "    missing_df['year'] = x['year'].iloc[0]     \n",
    "    \n",
    "    return x.append(missing_df)\n",
    "\n",
    "# def pad_test_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv(f'{TEST_DIR}/Test.csv')\n",
    "dates = sorted([(m, y) for y,m in test.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "\n",
    "month,yr = dates[0]\n",
    "test_dates = [(month-n,yr) if month>n else (12+month-n, yr-1) for n in range(1,LOOKBACK+1)]\n",
    "\n",
    "test_context = train_df[train_df[['month', 'year']].apply(tuple, axis=1).isin(test_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_sku = list(set(test.sku_name.unique()).difference(test_context.sku_name.unique()))\n",
    "test_context = test_context.groupby(['month','year']).apply(add_missing_context, missing_sku=missing_sku).reset_index(drop=True)\n",
    "# test_context = test_context[test_context.sku_name.isin(test.sku_name.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4545870a3b9348d6bb64b5989c16ebb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5916a97f20467aad9c3f8fb33997dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dates = sorted([(m, y) for y,m in train_df.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "\n",
    "test_data = test_context.groupby('sku_name').progress_apply(prepare_data, dates=test_dates).reset_index(drop=True)\n",
    "\n",
    "test_data['sku_coded'] = test_data['sku_name'].apply(sku_encoder)\n",
    "test_data = test_data.groupby(['month','year']).progress_apply(clean_data, encoder=sku_encoder).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyc_cols = [c for c in test_data.columns if 'seires' in c or 'month' in c]\n",
    "train_data = cyclic_encode(test_data, cyc_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20408/2718114800.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[forward] = group.shift(lp)\n",
      "/tmp/ipykernel_20408/2718114800.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[forw_diff] = df[feat] - df[forward]\n",
      "/tmp/ipykernel_20408/2718114800.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[forward] = group.shift(lp)\n",
      "/tmp/ipykernel_20408/2718114800.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[forw_diff] = df[feat] - df[forward]\n"
     ]
    }
   ],
   "source": [
    "channel_cols = [c for c in test_data.columns if all(f'_{i}' not in c for i in range(1,11)) and c not in CAT+['sku_name','month','year','sku_coded'] and 'target' not in c]\n",
    "test_data = lag_shift(test_data, 'sku_name', channel_cols, [3,6,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_1 = [c for c in test_data.columns if c not in ['sku_name','sku_coded']]\n",
    "cols_2 = [c for c in test_data.columns if c not in ['month','year','sku_name','sku_coded']]\n",
    "\n",
    "test_data[cols_1] = test_data.groupby('sku_name')[cols_1].transform(lambda x: x.fillna(x.median()))\n",
    "test_data[cols_2] = test_data.groupby(['month','year'])[cols_2].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test_data:pd.DataFrame,\n",
    "                     cv=True,\n",
    "                     models:list=None,\n",
    "                     folds:int=None):\n",
    "    \n",
    "    target_cols = [f'target_{i}' for i in range(N_STEPS)]\n",
    "    channel_cols = [c for c in test_data.columns if any(f'channel_{i}' in c for i in range(1,11)) and c not in CAT+['sku_name','sku_coded']]\n",
    "    \n",
    "    features = test_data.drop(columns=['sku_name','sku_coded']+target_cols)\n",
    "    test_data[target_cols] = 0\n",
    "    if cv:\n",
    "        assert folds is not None, 'specify number of folds used in CV training'\n",
    "        for fold in range(folds):\n",
    "            for step in range(N_STEPS):\n",
    "                test_data[f'target_{step}'] += models[fold][step].predict(features)/folds\n",
    "    else:\n",
    "        for step in range(N_STEPS):\n",
    "            test_data[f'target_{step}'] = models[step].predict(features)\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = make_predictions(test_data, True, cv_models, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df = make_predictions(test_data, False, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_df = pred_df[pred_df.sku_name.isin(test.sku_name.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,y = test_dates[-1]\n",
    "pred_dates = [(m+n,yr) if m+n<=12 else (m+n-12, yr+1) for n in range(1,N_STEPS+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [c for c in pred_df if 'target_' in c]\n",
    "pred_arr = pred_df[target_cols].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(np.repeat(pred_df['sku_name'], N_STEPS), columns=['sku_name']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['month'], sub_df['year'] = pd.DataFrame(pred_dates*int(len(sub_df)/N_STEPS)).loc[:, 0].values, pd.DataFrame(pred_dates*int(len(sub_df)/N_STEPS)).loc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['Target'] = pred_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['Item_ID'] = sub_df['sku_name'].astype(str)+'_'+sub_df['month'].astype(str)+'_'+sub_df['year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = f'{OUTPUT_DIR}/fossil_lgbm_{val_mae}.csv'\n",
    "sub_df[['Item_ID','Target']].to_csv(save_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127758.13389548851"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuBCgvTjM6NP",
    "outputId": "ba82b130-fa0b-48de-a5c1-d926bc190b5a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 221222\tvalid_1's l1: 209058\n",
      "[20]\ttraining's l1: 182855\tvalid_1's l1: 175264\n",
      "[30]\ttraining's l1: 155836\tvalid_1's l1: 152520\n",
      "[40]\ttraining's l1: 137280\tvalid_1's l1: 136662\n",
      "[50]\ttraining's l1: 124329\tvalid_1's l1: 126308\n",
      "[60]\ttraining's l1: 115079\tvalid_1's l1: 119014\n",
      "[70]\ttraining's l1: 108418\tvalid_1's l1: 113822\n",
      "[80]\ttraining's l1: 103695\tvalid_1's l1: 110294\n",
      "[90]\ttraining's l1: 99752.7\tvalid_1's l1: 107749\n",
      "[100]\ttraining's l1: 96664.6\tvalid_1's l1: 106250\n",
      "[110]\ttraining's l1: 94322\tvalid_1's l1: 105124\n",
      "[120]\ttraining's l1: 92198.1\tvalid_1's l1: 104314\n",
      "[130]\ttraining's l1: 90454.6\tvalid_1's l1: 103746\n",
      "[140]\ttraining's l1: 88989.5\tvalid_1's l1: 103590\n",
      "[150]\ttraining's l1: 87574.5\tvalid_1's l1: 103501\n",
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's l1: 88223.9\tvalid_1's l1: 103484\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 216497\tvalid_1's l1: 194286\n",
      "[20]\ttraining's l1: 181267\tvalid_1's l1: 165018\n",
      "[30]\ttraining's l1: 156503\tvalid_1's l1: 145646\n",
      "[40]\ttraining's l1: 139232\tvalid_1's l1: 134409\n",
      "[50]\ttraining's l1: 127113\tvalid_1's l1: 128002\n",
      "[60]\ttraining's l1: 118211\tvalid_1's l1: 124431\n",
      "[70]\ttraining's l1: 111675\tvalid_1's l1: 122697\n",
      "[80]\ttraining's l1: 106744\tvalid_1's l1: 122168\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's l1: 107600\tvalid_1's l1: 122073\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 199535\tvalid_1's l1: 227162\n",
      "[20]\ttraining's l1: 166615\tvalid_1's l1: 196474\n",
      "[30]\ttraining's l1: 143408\tvalid_1's l1: 175017\n",
      "[40]\ttraining's l1: 127104\tvalid_1's l1: 160572\n",
      "[50]\ttraining's l1: 115582\tvalid_1's l1: 150558\n",
      "[60]\ttraining's l1: 107420\tvalid_1's l1: 143379\n",
      "[70]\ttraining's l1: 101653\tvalid_1's l1: 138356\n",
      "[80]\ttraining's l1: 97225.7\tvalid_1's l1: 134663\n",
      "[90]\ttraining's l1: 93910.2\tvalid_1's l1: 132205\n",
      "[100]\ttraining's l1: 91268\tvalid_1's l1: 130529\n",
      "[110]\ttraining's l1: 89129.6\tvalid_1's l1: 128932\n",
      "[120]\ttraining's l1: 87224.9\tvalid_1's l1: 128051\n",
      "[130]\ttraining's l1: 85535.6\tvalid_1's l1: 127358\n",
      "[140]\ttraining's l1: 84068.2\tvalid_1's l1: 127145\n",
      "[150]\ttraining's l1: 82739.9\tvalid_1's l1: 127017\n",
      "Early stopping, best iteration is:\n",
      "[146]\ttraining's l1: 83213.8\tvalid_1's l1: 126869\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 193675\tvalid_1's l1: 243447\n",
      "[20]\ttraining's l1: 162347\tvalid_1's l1: 215151\n",
      "[30]\ttraining's l1: 140087\tvalid_1's l1: 195452\n",
      "[40]\ttraining's l1: 124261\tvalid_1's l1: 181151\n",
      "[50]\ttraining's l1: 113121\tvalid_1's l1: 170900\n",
      "[60]\ttraining's l1: 105173\tvalid_1's l1: 163654\n",
      "[70]\ttraining's l1: 99568.1\tvalid_1's l1: 158206\n",
      "[80]\ttraining's l1: 95418.4\tvalid_1's l1: 154367\n",
      "[90]\ttraining's l1: 91992.1\tvalid_1's l1: 151656\n",
      "[100]\ttraining's l1: 89301.5\tvalid_1's l1: 149779\n",
      "[110]\ttraining's l1: 87085.2\tvalid_1's l1: 147587\n",
      "[120]\ttraining's l1: 85225.7\tvalid_1's l1: 146175\n",
      "[130]\ttraining's l1: 83560.6\tvalid_1's l1: 145193\n",
      "[140]\ttraining's l1: 82202.9\tvalid_1's l1: 144643\n",
      "[150]\ttraining's l1: 80993.8\tvalid_1's l1: 144334\n",
      "[160]\ttraining's l1: 79919.5\tvalid_1's l1: 144145\n",
      "[170]\ttraining's l1: 78909\tvalid_1's l1: 144012\n",
      "[180]\ttraining's l1: 77943.9\tvalid_1's l1: 143996\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttraining's l1: 78251.9\tvalid_1's l1: 143891\n",
      "\n",
      "\n",
      "Elapsed 0.03 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 209324\tvalid_1's l1: 262218\n",
      "[20]\ttraining's l1: 173348\tvalid_1's l1: 226876\n",
      "[30]\ttraining's l1: 148540\tvalid_1's l1: 203429\n",
      "[40]\ttraining's l1: 131257\tvalid_1's l1: 187984\n",
      "[50]\ttraining's l1: 119207\tvalid_1's l1: 177138\n",
      "[60]\ttraining's l1: 110833\tvalid_1's l1: 170021\n",
      "[70]\ttraining's l1: 104854\tvalid_1's l1: 165085\n",
      "[80]\ttraining's l1: 100310\tvalid_1's l1: 161813\n",
      "[90]\ttraining's l1: 96928.7\tvalid_1's l1: 159452\n",
      "[100]\ttraining's l1: 94417.5\tvalid_1's l1: 158029\n",
      "[110]\ttraining's l1: 92240.7\tvalid_1's l1: 156971\n",
      "[120]\ttraining's l1: 90389.3\tvalid_1's l1: 156159\n",
      "[130]\ttraining's l1: 88870.3\tvalid_1's l1: 155487\n",
      "[140]\ttraining's l1: 87556.8\tvalid_1's l1: 155050\n",
      "[150]\ttraining's l1: 86362.7\tvalid_1's l1: 154887\n",
      "[160]\ttraining's l1: 85315.4\tvalid_1's l1: 154674\n",
      "Early stopping, best iteration is:\n",
      "[161]\ttraining's l1: 85217.7\tvalid_1's l1: 154645\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 198998\tvalid_1's l1: 295406\n",
      "[20]\ttraining's l1: 167267\tvalid_1's l1: 264010\n",
      "[30]\ttraining's l1: 144731\tvalid_1's l1: 241445\n",
      "[40]\ttraining's l1: 129225\tvalid_1's l1: 225601\n",
      "[50]\ttraining's l1: 118342\tvalid_1's l1: 214343\n",
      "[60]\ttraining's l1: 110589\tvalid_1's l1: 206073\n",
      "[70]\ttraining's l1: 105017\tvalid_1's l1: 200368\n",
      "[80]\ttraining's l1: 100837\tvalid_1's l1: 196610\n",
      "[90]\ttraining's l1: 97677.7\tvalid_1's l1: 193934\n",
      "[100]\ttraining's l1: 95118.4\tvalid_1's l1: 192137\n",
      "[110]\ttraining's l1: 92995.4\tvalid_1's l1: 190530\n",
      "[120]\ttraining's l1: 91241\tvalid_1's l1: 189535\n",
      "[130]\ttraining's l1: 89678.7\tvalid_1's l1: 188905\n",
      "[140]\ttraining's l1: 88384.6\tvalid_1's l1: 188616\n",
      "[150]\ttraining's l1: 87308.3\tvalid_1's l1: 188160\n",
      "[160]\ttraining's l1: 86311.4\tvalid_1's l1: 187861\n",
      "[170]\ttraining's l1: 85365.3\tvalid_1's l1: 187797\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttraining's l1: 85726.2\tvalid_1's l1: 187697\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 212761\tvalid_1's l1: 269109\n",
      "[20]\ttraining's l1: 177864\tvalid_1's l1: 230821\n",
      "[30]\ttraining's l1: 153028\tvalid_1's l1: 205316\n",
      "[40]\ttraining's l1: 135667\tvalid_1's l1: 191997\n",
      "[50]\ttraining's l1: 123463\tvalid_1's l1: 185646\n",
      "[60]\ttraining's l1: 114829\tvalid_1's l1: 182643\n",
      "[70]\ttraining's l1: 108793\tvalid_1's l1: 180988\n",
      "[80]\ttraining's l1: 104301\tvalid_1's l1: 180522\n",
      "Early stopping, best iteration is:\n",
      "[77]\ttraining's l1: 105536\tvalid_1's l1: 180476\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 220185\tvalid_1's l1: 264888\n",
      "[20]\ttraining's l1: 184878\tvalid_1's l1: 234570\n",
      "[30]\ttraining's l1: 159443\tvalid_1's l1: 215628\n",
      "[40]\ttraining's l1: 141845\tvalid_1's l1: 203736\n",
      "[50]\ttraining's l1: 129510\tvalid_1's l1: 195973\n",
      "[60]\ttraining's l1: 120869\tvalid_1's l1: 190877\n",
      "[70]\ttraining's l1: 114566\tvalid_1's l1: 187514\n",
      "[80]\ttraining's l1: 109833\tvalid_1's l1: 185283\n",
      "[90]\ttraining's l1: 106117\tvalid_1's l1: 183934\n",
      "[100]\ttraining's l1: 103163\tvalid_1's l1: 183157\n",
      "[110]\ttraining's l1: 100791\tvalid_1's l1: 182614\n",
      "[120]\ttraining's l1: 98826.5\tvalid_1's l1: 182126\n",
      "[130]\ttraining's l1: 97137.6\tvalid_1's l1: 181889\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's l1: 96839.8\tvalid_1's l1: 181833\n",
      "\n",
      "\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 230487\tvalid_1's l1: 275625\n",
      "[20]\ttraining's l1: 190803\tvalid_1's l1: 241180\n",
      "[30]\ttraining's l1: 162922\tvalid_1's l1: 220780\n",
      "[40]\ttraining's l1: 143711\tvalid_1's l1: 207494\n",
      "[50]\ttraining's l1: 130470\tvalid_1's l1: 198615\n",
      "[60]\ttraining's l1: 121416\tvalid_1's l1: 193080\n",
      "[70]\ttraining's l1: 114870\tvalid_1's l1: 189090\n",
      "[80]\ttraining's l1: 110095\tvalid_1's l1: 186429\n",
      "[90]\ttraining's l1: 106560\tvalid_1's l1: 184754\n",
      "[100]\ttraining's l1: 103665\tvalid_1's l1: 183290\n",
      "[110]\ttraining's l1: 101442\tvalid_1's l1: 182325\n",
      "[120]\ttraining's l1: 99591.2\tvalid_1's l1: 181894\n",
      "Early stopping, best iteration is:\n",
      "[119]\ttraining's l1: 99777\tvalid_1's l1: 181874\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 237731\tvalid_1's l1: 237832\n",
      "[20]\ttraining's l1: 199062\tvalid_1's l1: 220022\n",
      "[30]\ttraining's l1: 172020\tvalid_1's l1: 212712\n",
      "[40]\ttraining's l1: 153151\tvalid_1's l1: 210489\n",
      "[50]\ttraining's l1: 139972\tvalid_1's l1: 209855\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttraining's l1: 141094\tvalid_1's l1: 209709\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 242876\tvalid_1's l1: 207662\n",
      "[20]\ttraining's l1: 203868\tvalid_1's l1: 193658\n",
      "[30]\ttraining's l1: 176257\tvalid_1's l1: 192635\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttraining's l1: 181005\tvalid_1's l1: 192247\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttraining's l1: 241755\tvalid_1's l1: 208446\n",
      "[20]\ttraining's l1: 204843\tvalid_1's l1: 185201\n",
      "[30]\ttraining's l1: 178543\tvalid_1's l1: 170224\n",
      "[40]\ttraining's l1: 160034\tvalid_1's l1: 160448\n",
      "[50]\ttraining's l1: 147173\tvalid_1's l1: 154344\n",
      "[60]\ttraining's l1: 138002\tvalid_1's l1: 150686\n",
      "[70]\ttraining's l1: 131239\tvalid_1's l1: 147892\n",
      "[80]\ttraining's l1: 126147\tvalid_1's l1: 145757\n",
      "[90]\ttraining's l1: 122118\tvalid_1's l1: 144252\n",
      "[100]\ttraining's l1: 118870\tvalid_1's l1: 143530\n",
      "[110]\ttraining's l1: 116234\tvalid_1's l1: 143140\n",
      "[120]\ttraining's l1: 114106\tvalid_1's l1: 142894\n",
      "[130]\ttraining's l1: 112355\tvalid_1's l1: 142886\n",
      "Early stopping, best iteration is:\n",
      "[126]\ttraining's l1: 112987\tvalid_1's l1: 142648\n",
      "\n",
      "\n",
      "Elapsed 0.06 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds = 3\n",
    "use_gpu = False\n",
    "\n",
    "params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'device_type': 'gpu' if use_gpu else 'cpu',\n",
    "                #'gpu_use_dp': 'true',\n",
    "                'n_jobs': -1,\n",
    "                'learning_rate': 0.03,\n",
    "                #'bagging_fraction': 0.85,\n",
    "                #'bagging_freq': 10, \n",
    "                'colsample_bytree': 0.85,\n",
    "                'colsample_bynode': 0.85,\n",
    "                'min_data_per_leaf': 25,\n",
    "                'max_bin':63,\n",
    "                'num_leaves': 125,\n",
    "                'lambda_l1': 0.5,\n",
    "                'lambda_l2': 0.5,\n",
    "                \"metric\": \"mae\",\n",
    "#                 \"min_delta\":0.0003,\n",
    "                'seed': SEED,\n",
    "                'verbose':-1\n",
    "              }  \n",
    "cv_models = train_model(train_data, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHlBjEK_Y9Jq",
    "outputId": "84b05bd9-7679-45dd-a7e4-6f7e6cc12b84",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making fold 1 predictions\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "\n",
      "\n",
      "Average Val MAE: 170126.54224123203\n"
     ]
    }
   ],
   "source": [
    "val_mae = test_model(train_data, folds, cv_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_func(x):\n",
    "    # print(x)\n",
    "    for i in range(1, N_STEPS):\n",
    "        x.loc[i, 'target_0'] = x.reset_index(drop=True).loc[i, f'target_{i}'] \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stretched_df = pd.DataFrame(np.repeat(train_data.values, N_STEPS, axis=0), columns=train_data.columns)\n",
    "stretched_df['target'] = stretched_df.groupby(['sku_name']).cumcount()//N_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>starting_inventory</th>\n",
       "      <th>sellin</th>\n",
       "      <th>sellin_channel_1</th>\n",
       "      <th>sellin_channel_2</th>\n",
       "      <th>sellin_channel_3</th>\n",
       "      <th>sellin_channel_4</th>\n",
       "      <th>sellin_channel_5</th>\n",
       "      <th>sellin_channel_6</th>\n",
       "      <th>sellin_channel_7</th>\n",
       "      <th>...</th>\n",
       "      <th>onhand_inventory_channel_9</th>\n",
       "      <th>onhand_inventory_channel_10</th>\n",
       "      <th>price</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>target_0</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>9117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229951.0</td>\n",
       "      <td>110417.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>358602.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>9117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229951.0</td>\n",
       "      <td>110417.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>358602.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>9117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229951.0</td>\n",
       "      <td>110417.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>358602.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>9117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229951.0</td>\n",
       "      <td>110417.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>358602.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3480</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>1576228.0</td>\n",
       "      <td>650346.0</td>\n",
       "      <td>380888.0</td>\n",
       "      <td>150937.0</td>\n",
       "      <td>70910.0</td>\n",
       "      <td>2026.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3039.0</td>\n",
       "      <td>21273.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16208.0</td>\n",
       "      <td>81040.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>416343.0</td>\n",
       "      <td>618943.0</td>\n",
       "      <td>285666.0</td>\n",
       "      <td>395070.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166931</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>1040351.0</td>\n",
       "      <td>349485.0</td>\n",
       "      <td>257302.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>11143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>95222.0</td>\n",
       "      <td>106365.0</td>\n",
       "      <td>138781.0</td>\n",
       "      <td>48624.0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169944</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>379875.0</td>\n",
       "      <td>219821.0</td>\n",
       "      <td>142833.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15195.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>28364.0</td>\n",
       "      <td>12156.0</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>7091.0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169945</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>379875.0</td>\n",
       "      <td>219821.0</td>\n",
       "      <td>142833.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15195.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>28364.0</td>\n",
       "      <td>12156.0</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>7091.0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169946</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>379875.0</td>\n",
       "      <td>219821.0</td>\n",
       "      <td>142833.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15195.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>28364.0</td>\n",
       "      <td>12156.0</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>7091.0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169947</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>379875.0</td>\n",
       "      <td>219821.0</td>\n",
       "      <td>142833.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15195.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>28364.0</td>\n",
       "      <td>12156.0</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>7091.0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sku_name starting_inventory    sellin sellin_channel_1  \\\n",
       "0       ABEAHAMASHL          6844841.0  902583.0         542968.0   \n",
       "1       ABEAHAMASHL          6844841.0  902583.0         542968.0   \n",
       "2       ABEAHAMASHL          6844841.0  902583.0         542968.0   \n",
       "3       ABEAHAMASHL          6844841.0  902583.0         542968.0   \n",
       "3480    ABEAHAMASHL          1576228.0  650346.0         380888.0   \n",
       "...             ...                ...       ...              ...   \n",
       "166931  ABEAHAMASHL          1040351.0  349485.0         257302.0   \n",
       "169944  ABEAHAMASHL           379875.0  219821.0         142833.0   \n",
       "169945  ABEAHAMASHL           379875.0  219821.0         142833.0   \n",
       "169946  ABEAHAMASHL           379875.0  219821.0         142833.0   \n",
       "169947  ABEAHAMASHL           379875.0  219821.0         142833.0   \n",
       "\n",
       "       sellin_channel_2 sellin_channel_3 sellin_channel_4 sellin_channel_5  \\\n",
       "0              236029.0          34442.0              0.0          14182.0   \n",
       "1              236029.0          34442.0              0.0          14182.0   \n",
       "2              236029.0          34442.0              0.0          14182.0   \n",
       "3              236029.0          34442.0              0.0          14182.0   \n",
       "3480           150937.0          70910.0           2026.0              0.0   \n",
       "...                 ...              ...              ...              ...   \n",
       "166931          65845.0          11143.0              0.0              0.0   \n",
       "169944          65845.0              0.0              0.0              0.0   \n",
       "169945          65845.0              0.0              0.0              0.0   \n",
       "169946          65845.0              0.0              0.0              0.0   \n",
       "169947          65845.0              0.0              0.0              0.0   \n",
       "\n",
       "       sellin_channel_6 sellin_channel_7  ... onhand_inventory_channel_9  \\\n",
       "0                5065.0           9117.0  ...                   229951.0   \n",
       "1                5065.0           9117.0  ...                   229951.0   \n",
       "2                5065.0           9117.0  ...                   229951.0   \n",
       "3                5065.0           9117.0  ...                   229951.0   \n",
       "3480             3039.0          21273.0  ...                    16208.0   \n",
       "...                 ...              ...  ...                        ...   \n",
       "166931           6078.0              0.0  ...                        0.0   \n",
       "169944           6078.0              0.0  ...                        0.0   \n",
       "169945           6078.0              0.0  ...                        0.0   \n",
       "169946           6078.0              0.0  ...                        0.0   \n",
       "169947           6078.0              0.0  ...                        0.0   \n",
       "\n",
       "       onhand_inventory_channel_10  price month    year  target_0   target_1  \\\n",
       "0                         110417.0  145.0   1.0  2016.0  358602.0  1177106.0   \n",
       "1                         110417.0  145.0   1.0  2016.0  358602.0  1177106.0   \n",
       "2                         110417.0  145.0   1.0  2016.0  358602.0  1177106.0   \n",
       "3                         110417.0  145.0   1.0  2016.0  358602.0  1177106.0   \n",
       "3480                       81040.0  145.0   1.0  2017.0  416343.0   618943.0   \n",
       "...                            ...    ...   ...     ...       ...        ...   \n",
       "166931                     65845.0  145.0  12.0  2018.0   95222.0   106365.0   \n",
       "169944                     15195.0  149.0  12.0  2019.0   28364.0    12156.0   \n",
       "169945                     15195.0  149.0  12.0  2019.0   28364.0    12156.0   \n",
       "169946                     15195.0  149.0  12.0  2019.0   28364.0    12156.0   \n",
       "169947                     15195.0  149.0  12.0  2019.0   28364.0    12156.0   \n",
       "\n",
       "        target_2  target_3 target  \n",
       "0       968428.0  470032.0      0  \n",
       "1       968428.0  470032.0      0  \n",
       "2       968428.0  470032.0      0  \n",
       "3       968428.0  470032.0      0  \n",
       "3480    285666.0  395070.0      1  \n",
       "...          ...       ...    ...  \n",
       "166931  138781.0   48624.0     55  \n",
       "169944   42546.0    7091.0     56  \n",
       "169945   42546.0    7091.0     56  \n",
       "169946   42546.0    7091.0     56  \n",
       "169947   42546.0    7091.0     56  \n",
       "\n",
       "[228 rows x 42 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stretched_df[stretched_df.sku_name=='ABEAHAMASHL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = stretched_df.groupby(['sku_name','month','year','target']).apply(expand_func).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>starting_inventory</th>\n",
       "      <th>sellin</th>\n",
       "      <th>sellin_channel_1</th>\n",
       "      <th>sellin_channel_2</th>\n",
       "      <th>sellin_channel_3</th>\n",
       "      <th>sellin_channel_4</th>\n",
       "      <th>sellin_channel_5</th>\n",
       "      <th>sellin_channel_6</th>\n",
       "      <th>sellin_channel_7</th>\n",
       "      <th>...</th>\n",
       "      <th>onhand_inventory_channel_9</th>\n",
       "      <th>onhand_inventory_channel_10</th>\n",
       "      <th>price</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>target_0</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>9117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229951.0</td>\n",
       "      <td>110417.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>358602.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>9117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229951.0</td>\n",
       "      <td>110417.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>9117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229951.0</td>\n",
       "      <td>110417.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>6844841.0</td>\n",
       "      <td>902583.0</td>\n",
       "      <td>542968.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>34442.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14182.0</td>\n",
       "      <td>5065.0</td>\n",
       "      <td>9117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229951.0</td>\n",
       "      <td>110417.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>1177106.0</td>\n",
       "      <td>968428.0</td>\n",
       "      <td>470032.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>1576228.0</td>\n",
       "      <td>650346.0</td>\n",
       "      <td>380888.0</td>\n",
       "      <td>150937.0</td>\n",
       "      <td>70910.0</td>\n",
       "      <td>2026.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3039.0</td>\n",
       "      <td>21273.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16208.0</td>\n",
       "      <td>81040.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>416343.0</td>\n",
       "      <td>618943.0</td>\n",
       "      <td>285666.0</td>\n",
       "      <td>395070.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>1040351.0</td>\n",
       "      <td>349485.0</td>\n",
       "      <td>257302.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>11143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>95222.0</td>\n",
       "      <td>106365.0</td>\n",
       "      <td>138781.0</td>\n",
       "      <td>48624.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>379875.0</td>\n",
       "      <td>219821.0</td>\n",
       "      <td>142833.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15195.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>28364.0</td>\n",
       "      <td>12156.0</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>7091.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>379875.0</td>\n",
       "      <td>219821.0</td>\n",
       "      <td>142833.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15195.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>28364.0</td>\n",
       "      <td>12156.0</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>7091.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>379875.0</td>\n",
       "      <td>219821.0</td>\n",
       "      <td>142833.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15195.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>28364.0</td>\n",
       "      <td>12156.0</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>7091.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>ABEAHAMASHL</td>\n",
       "      <td>379875.0</td>\n",
       "      <td>219821.0</td>\n",
       "      <td>142833.0</td>\n",
       "      <td>65845.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15195.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>28364.0</td>\n",
       "      <td>12156.0</td>\n",
       "      <td>42546.0</td>\n",
       "      <td>7091.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sku_name starting_inventory    sellin sellin_channel_1  \\\n",
       "0    ABEAHAMASHL          6844841.0  902583.0         542968.0   \n",
       "1    ABEAHAMASHL          6844841.0  902583.0         542968.0   \n",
       "2    ABEAHAMASHL          6844841.0  902583.0         542968.0   \n",
       "3    ABEAHAMASHL          6844841.0  902583.0         542968.0   \n",
       "4    ABEAHAMASHL          1576228.0  650346.0         380888.0   \n",
       "..           ...                ...       ...              ...   \n",
       "385  ABEAHAMASHL          1040351.0  349485.0         257302.0   \n",
       "389  ABEAHAMASHL           379875.0  219821.0         142833.0   \n",
       "390  ABEAHAMASHL           379875.0  219821.0         142833.0   \n",
       "391  ABEAHAMASHL           379875.0  219821.0         142833.0   \n",
       "392  ABEAHAMASHL           379875.0  219821.0         142833.0   \n",
       "\n",
       "    sellin_channel_2 sellin_channel_3 sellin_channel_4 sellin_channel_5  \\\n",
       "0           236029.0          34442.0              0.0          14182.0   \n",
       "1           236029.0          34442.0              0.0          14182.0   \n",
       "2           236029.0          34442.0              0.0          14182.0   \n",
       "3           236029.0          34442.0              0.0          14182.0   \n",
       "4           150937.0          70910.0           2026.0              0.0   \n",
       "..               ...              ...              ...              ...   \n",
       "385          65845.0          11143.0              0.0              0.0   \n",
       "389          65845.0              0.0              0.0              0.0   \n",
       "390          65845.0              0.0              0.0              0.0   \n",
       "391          65845.0              0.0              0.0              0.0   \n",
       "392          65845.0              0.0              0.0              0.0   \n",
       "\n",
       "    sellin_channel_6 sellin_channel_7  ... onhand_inventory_channel_9  \\\n",
       "0             5065.0           9117.0  ...                   229951.0   \n",
       "1             5065.0           9117.0  ...                   229951.0   \n",
       "2             5065.0           9117.0  ...                   229951.0   \n",
       "3             5065.0           9117.0  ...                   229951.0   \n",
       "4             3039.0          21273.0  ...                    16208.0   \n",
       "..               ...              ...  ...                        ...   \n",
       "385           6078.0              0.0  ...                        0.0   \n",
       "389           6078.0              0.0  ...                        0.0   \n",
       "390           6078.0              0.0  ...                        0.0   \n",
       "391           6078.0              0.0  ...                        0.0   \n",
       "392           6078.0              0.0  ...                        0.0   \n",
       "\n",
       "    onhand_inventory_channel_10  price month    year   target_0   target_1  \\\n",
       "0                      110417.0  145.0   1.0  2016.0   358602.0  1177106.0   \n",
       "1                      110417.0  145.0   1.0  2016.0  1177106.0  1177106.0   \n",
       "2                      110417.0  145.0   1.0  2016.0   968428.0  1177106.0   \n",
       "3                      110417.0  145.0   1.0  2016.0   470032.0  1177106.0   \n",
       "4                       81040.0  145.0   1.0  2017.0   416343.0   618943.0   \n",
       "..                          ...    ...   ...     ...        ...        ...   \n",
       "385                     65845.0  145.0  12.0  2018.0    95222.0   106365.0   \n",
       "389                     15195.0  149.0  12.0  2019.0    28364.0    12156.0   \n",
       "390                     15195.0  149.0  12.0  2019.0    28364.0    12156.0   \n",
       "391                     15195.0  149.0  12.0  2019.0    28364.0    12156.0   \n",
       "392                     15195.0  149.0  12.0  2019.0    28364.0    12156.0   \n",
       "\n",
       "     target_2  target_3 target  \n",
       "0    968428.0  470032.0    0.0  \n",
       "1    968428.0  470032.0    0.0  \n",
       "2    968428.0  470032.0    0.0  \n",
       "3    968428.0  470032.0    0.0  \n",
       "4    285666.0  395070.0    1.0  \n",
       "..        ...       ...    ...  \n",
       "385  138781.0   48624.0   55.0  \n",
       "389   42546.0    7091.0   56.0  \n",
       "390   42546.0    7091.0   56.0  \n",
       "391   42546.0    7091.0   56.0  \n",
       "392   42546.0    7091.0   56.0  \n",
       "\n",
       "[228 rows x 42 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep[rep.sku_name=='ABEAHAMASHL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuBCgvTjM6NP",
    "outputId": "ba82b130-fa0b-48de-a5c1-d926bc190b5a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[50]\ttraining's l1: 74825.6\tvalid_1's l1: 60646.2\n",
      "[100]\ttraining's l1: 68342.8\tvalid_1's l1: 58537\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's l1: 67025.4\tvalid_1's l1: 58412.8\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[50]\ttraining's l1: 72010.7\tvalid_1's l1: 61319.1\n",
      "[100]\ttraining's l1: 65845.2\tvalid_1's l1: 58768.8\n",
      "Early stopping, best iteration is:\n",
      "[112]\ttraining's l1: 65032.9\tvalid_1's l1: 58709.4\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[50]\ttraining's l1: 70703.1\tvalid_1's l1: 54269.7\n",
      "[100]\ttraining's l1: 61761\tvalid_1's l1: 52025.9\n",
      "Early stopping, best iteration is:\n",
      "[106]\ttraining's l1: 61261.6\tvalid_1's l1: 52022.4\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[50]\ttraining's l1: 49400.2\tvalid_1's l1: 44322.5\n",
      "Early stopping, best iteration is:\n",
      "[87]\ttraining's l1: 42703.9\tvalid_1's l1: 43318.1\n",
      "\n",
      "\n",
      "Elapsed 0.02 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttraining's l1: 93028.2\tvalid_1's l1: 81810.4\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[50]\ttraining's l1: 84132.2\tvalid_1's l1: 76164.8\n",
      "Early stopping, best iteration is:\n",
      "[84]\ttraining's l1: 82902.2\tvalid_1's l1: 75937.4\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[50]\ttraining's l1: 87335.1\tvalid_1's l1: 81619.4\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttraining's l1: 87384.1\tvalid_1's l1: 81604.7\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's l1: 76663.8\tvalid_1's l1: 64231.2\n",
      "\n",
      "\n",
      "Elapsed 0.04 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "\n",
      "Training model for timestep 1 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's l1: 86833.5\tvalid_1's l1: 63783.8\n",
      "\n",
      "\n",
      "Training model for timestep 2 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l1: 81614.7\tvalid_1's l1: 63435.7\n",
      "\n",
      "\n",
      "Training model for timestep 3 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\ttraining's l1: 86434\tvalid_1's l1: 61504.5\n",
      "\n",
      "\n",
      "Training model for timestep 4 forecasting\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's l1: 68826.3\tvalid_1's l1: 46100\n",
      "\n",
      "\n",
      "Elapsed 0.05 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds = len(train_data)\n",
    "use_gpu = False\n",
    "\n",
    "params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'device_type': 'gpu' if use_gpu else 'cpu',\n",
    "                #'gpu_use_dp': 'true',\n",
    "                'n_jobs': -1,\n",
    "                'learning_rate': 0.03,\n",
    "                #'bagging_fraction': 0.85,\n",
    "                #'bagging_freq': 10, \n",
    "                'colsample_bytree': 0.85,\n",
    "                'colsample_bynode': 0.85,\n",
    "                'min_data_per_leaf': 25,\n",
    "                'max_bin':63,\n",
    "                'num_leaves': 125,\n",
    "                'lambda_l1': 0.5,\n",
    "                'lambda_l2': 0.5,\n",
    "                \"metric\": \"mae\",\n",
    "#                 \"min_delta\":0.0003,\n",
    "                'seed': SEED,\n",
    "                'verbose':-1\n",
    "              }  \n",
    "cv_models = train_model(train_data, val_data, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHlBjEK_Y9Jq",
    "outputId": "84b05bd9-7679-45dd-a7e4-6f7e6cc12b84",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making fold 1 predictions\n",
      "valid MAE 273330.1023\tElapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Making fold 2 predictions\n",
      "valid MAE 208414.7877\tElapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Making fold 3 predictions\n",
      "valid MAE 148976.9905\tElapsed 0.01 mins\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Average Val MAE: 210240.6268305067\n"
     ]
    }
   ],
   "source": [
    "val_mae = test_model(train_data, val_data, folds, cv_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "iGV4KVcKmTL-"
   },
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv(f'{TEST_DIR}/Test.csv')\n",
    "test['sku_coded'] = test.sku_name.apply(sku_encoder)\n",
    "\n",
    "test['idx'] = SEED\n",
    "test = test.groupby('sku_name').apply(lambda x: func(x, x.name)).reset_index(drop=True)\n",
    "\n",
    "dates = sorted([(m, y) for y,m in test.groupby(['year', 'month']).groups.keys()], key=lambda d: (d[1], d[0]))\n",
    "\n",
    "month,yr = dates[0]\n",
    "test_dates = [(month-n,yr) if month>n else (12+month-n, yr-1) for n in range(1,LOOKBACK+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = train_df[train_df[['month','year']].apply(tuple, axis=1).isin(test_dates)].reset_index(drop=True)\n",
    "test_context['sku_name'] = test_context['sku_name'].apply(sku_encoder)\n",
    "test_context = test_context.groupby(['year', 'month']).apply(pad_sku_sequence, encoder=sku_encoder).reset_index(drop=True)\n",
    "x_test = test_context.sort_values(by=['year','month','sku_name']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] The number of features in data (8) is not the same as it was in training data (12).\n",
      "You can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "The number of features in data (8) is not the same as it was in training data (12).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [249]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\u001b[43mmulti_step_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, N_STEPS)\n\u001b[1;32m      2\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mrepeat(test\u001b[38;5;241m.\u001b[39mvalues, LOOKBACK, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), columns\u001b[38;5;241m=\u001b[39mtest\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m      3\u001b[0m pred_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msellin\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m preds[pred_df\u001b[38;5;241m.\u001b[39msku_coded\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues, pred_df\u001b[38;5;241m.\u001b[39midx\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues]\n",
      "Input \u001b[0;32mIn [241]\u001b[0m, in \u001b[0;36mmulti_step_test\u001b[0;34m(x_test, models)\u001b[0m\n\u001b[1;32m      3\u001b[0m channel_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m x_test\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m c \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m11\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m CAT\u001b[38;5;241m+\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msku_name\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_STEPS):\n\u001b[0;32m----> 6\u001b[0m     oof\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msku_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mchannel_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m oof\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:3538\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[1;32m   3536\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3537\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 3538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3539\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3540\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:848\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[1;32m    846\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_csc(data, start_iteration, num_iteration, predict_type)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 848\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:938\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:908\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d.<locals>.inner_predict\u001b[0;34m(mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length of pre-allocated predict array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    907\u001b[0m out_num_preds \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int64(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 908\u001b[0m \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterPredictForMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_API_IS_ROW_MAJOR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_preds \u001b[38;5;241m!=\u001b[39m out_num_preds\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length for predict results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py:125\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mLightGBMError\u001b[0m: The number of features in data (8) is not the same as it was in training data (12).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing."
     ]
    }
   ],
   "source": [
    "preds = np.concatenate(multi_step_test(x_test, cv_models[folds-1])).reshape(-1, N_STEPS)\n",
    "pred_df = pd.DataFrame(np.repeat(test.values, LOOKBACK, axis=0), columns=test.columns)\n",
    "pred_df['sellin'] = preds[pred_df.sku_coded.astype(int).values, pred_df.idx.astype(int).values]\n",
    "pred_df = pred_df.groupby(['sku_name','month','year']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = make_submission(pred_df, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "cLiXzq6kmsjG"
   },
   "outputs": [],
   "source": [
    "save_name = f'{OUTPUT_DIR}/fossil_lgbm_{val_mae}.csv'\n",
    "sub.to_csv(save_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_name</th>\n",
       "      <th>starting_inventory</th>\n",
       "      <th>sellin</th>\n",
       "      <th>sellin_channel_1</th>\n",
       "      <th>sellin_channel_2</th>\n",
       "      <th>sellin_channel_3</th>\n",
       "      <th>sellin_channel_4</th>\n",
       "      <th>sellin_channel_5</th>\n",
       "      <th>sellin_channel_6</th>\n",
       "      <th>sellin_channel_7</th>\n",
       "      <th>...</th>\n",
       "      <th>onhand_inventory_channel_8</th>\n",
       "      <th>onhand_inventory_channel_9</th>\n",
       "      <th>onhand_inventory_channel_10</th>\n",
       "      <th>price</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>y_pred_1</th>\n",
       "      <th>y_pred_2</th>\n",
       "      <th>y_pred_3</th>\n",
       "      <th>y_pred_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1782880.0</td>\n",
       "      <td>1258146.00</td>\n",
       "      <td>195509.0</td>\n",
       "      <td>68884.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31403.0</td>\n",
       "      <td>17221.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8104.00</td>\n",
       "      <td>614891.0</td>\n",
       "      <td>236029.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2041195.0</td>\n",
       "      <td>1041364.0</td>\n",
       "      <td>891440.00</td>\n",
       "      <td>20260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50650.00</td>\n",
       "      <td>4052.0</td>\n",
       "      <td>6078.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>379875.0</td>\n",
       "      <td>162080.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5502616.0</td>\n",
       "      <td>2478811.0</td>\n",
       "      <td>2050312.00</td>\n",
       "      <td>159041.0</td>\n",
       "      <td>61793.0</td>\n",
       "      <td>4052.0</td>\n",
       "      <td>1013.00</td>\n",
       "      <td>15195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20260.0</td>\n",
       "      <td>110417.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3837</th>\n",
       "      <td>3837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3838</th>\n",
       "      <td>3838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3839</th>\n",
       "      <td>3839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3840</th>\n",
       "      <td>3840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3841</th>\n",
       "      <td>3841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1519.5</td>\n",
       "      <td>253.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>506.5</td>\n",
       "      <td>759.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>253.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3842 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sku_name  starting_inventory     sellin  sellin_channel_1  \\\n",
       "0            0                 0.0        0.0              0.00   \n",
       "1            1                 0.0  1782880.0        1258146.00   \n",
       "2            2           2041195.0  1041364.0         891440.00   \n",
       "3            3                 0.0        0.0              0.00   \n",
       "4            4           5502616.0  2478811.0        2050312.00   \n",
       "...        ...                 ...        ...               ...   \n",
       "3837      3837                 0.0        0.0              0.00   \n",
       "3838      3838                 0.0        0.0              0.00   \n",
       "3839      3839                 0.0        0.0              0.00   \n",
       "3840      3840                 0.0        0.0              0.00   \n",
       "3841      3841                 0.0     1519.5            253.25   \n",
       "\n",
       "      sellin_channel_2  sellin_channel_3  sellin_channel_4  sellin_channel_5  \\\n",
       "0                  0.0               0.0               0.0              0.00   \n",
       "1             195509.0           68884.0               0.0              0.00   \n",
       "2              20260.0               0.0               0.0          50650.00   \n",
       "3                  0.0               0.0               0.0              0.00   \n",
       "4             159041.0           61793.0            4052.0           1013.00   \n",
       "...                ...               ...               ...               ...   \n",
       "3837               0.0               0.0               0.0              0.00   \n",
       "3838               0.0               0.0               0.0              0.00   \n",
       "3839               0.0               0.0               0.0              0.00   \n",
       "3840               0.0               0.0               0.0              0.00   \n",
       "3841               0.0               0.0             506.5            759.75   \n",
       "\n",
       "      sellin_channel_6  sellin_channel_7  ...  onhand_inventory_channel_8  \\\n",
       "0                  0.0               0.0  ...                        0.00   \n",
       "1              31403.0           17221.0  ...                     8104.00   \n",
       "2               4052.0            6078.0  ...                        0.00   \n",
       "3                  0.0               0.0  ...                        0.00   \n",
       "4              15195.0               0.0  ...                        0.00   \n",
       "...                ...               ...  ...                         ...   \n",
       "3837               0.0               0.0  ...                        0.00   \n",
       "3838               0.0               0.0  ...                        0.00   \n",
       "3839               0.0               0.0  ...                        0.00   \n",
       "3840               0.0               0.0  ...                        0.00   \n",
       "3841               0.0               0.0  ...                      253.25   \n",
       "\n",
       "      onhand_inventory_channel_9  onhand_inventory_channel_10  price  month  \\\n",
       "0                            0.0                          0.0    0.0    1.0   \n",
       "1                       614891.0                     236029.0  125.0    1.0   \n",
       "2                       379875.0                     162080.0   95.0    1.0   \n",
       "3                            0.0                          0.0    0.0    1.0   \n",
       "4                        20260.0                     110417.0  145.0    1.0   \n",
       "...                          ...                          ...    ...    ...   \n",
       "3837                         0.0                          0.0    0.0    1.0   \n",
       "3838                         0.0                          0.0    0.0    1.0   \n",
       "3839                         0.0                          0.0    0.0    1.0   \n",
       "3840                         0.0                          0.0    0.0    1.0   \n",
       "3841                         0.0                          0.0  132.5    1.0   \n",
       "\n",
       "        year  y_pred_1  y_pred_2  y_pred_3  y_pred_4  \n",
       "0     2016.0       NaN       NaN       NaN       NaN  \n",
       "1     2016.0       NaN       NaN       NaN       NaN  \n",
       "2     2016.0       NaN       NaN       NaN       NaN  \n",
       "3     2016.0       NaN       NaN       NaN       NaN  \n",
       "4     2016.0       NaN       NaN       NaN       NaN  \n",
       "...      ...       ...       ...       ...       ...  \n",
       "3837  2016.0       NaN       NaN       NaN       NaN  \n",
       "3838  2016.0       NaN       NaN       NaN       NaN  \n",
       "3839  2016.0       NaN       NaN       NaN       NaN  \n",
       "3840  2016.0       NaN       NaN       NaN       NaN  \n",
       "3841  2016.0       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[3842 rows x 41 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOSHTLYNYOSHZZ_11_2021</td>\n",
       "      <td>57035.260394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOSHTLYNYOSHZZ_12_2021</td>\n",
       "      <td>57035.260394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOSHTLYNYOSHZZ_1_2022</td>\n",
       "      <td>57035.260394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOSHTLYNYOSHZZ_2_2022</td>\n",
       "      <td>57035.260394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YOSHRENECARL_11_2021</td>\n",
       "      <td>57035.260394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>ABEENNEARMAZZ_2_2022</td>\n",
       "      <td>57035.260394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>ABEAHAMASHL_11_2021</td>\n",
       "      <td>57035.260394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>ABEAHAMASHL_12_2021</td>\n",
       "      <td>57035.260394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>ABEAHAMASHL_1_2022</td>\n",
       "      <td>65605.281441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>ABEAHAMASHL_2_2022</td>\n",
       "      <td>57035.260394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Item_ID        Target\n",
       "0     YOSHTLYNYOSHZZ_11_2021  57035.260394\n",
       "1     YOSHTLYNYOSHZZ_12_2021  57035.260394\n",
       "2      YOSHTLYNYOSHZZ_1_2022  57035.260394\n",
       "3      YOSHTLYNYOSHZZ_2_2022  57035.260394\n",
       "4       YOSHRENECARL_11_2021  57035.260394\n",
       "...                      ...           ...\n",
       "1523    ABEENNEARMAZZ_2_2022  57035.260394\n",
       "1524     ABEAHAMASHL_11_2021  57035.260394\n",
       "1525     ABEAHAMASHL_12_2021  57035.260394\n",
       "1526      ABEAHAMASHL_1_2022  65605.281441\n",
       "1527      ABEAHAMASHL_2_2022  57035.260394\n",
       "\n",
       "[1528 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOSHTLYNYOSHZZ_11_2021</td>\n",
       "      <td>30149.691267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOSHTLYNYOSHZZ_12_2021</td>\n",
       "      <td>30149.691267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YOSHTLYNYOSHZZ_1_2022</td>\n",
       "      <td>30149.691267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YOSHTLYNYOSHZZ_2_2022</td>\n",
       "      <td>30149.691267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YOSHRENECARL_11_2021</td>\n",
       "      <td>34624.701924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>ABEENNEARMAZZ_2_2022</td>\n",
       "      <td>198208.985307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>ABEAHAMASHL_11_2021</td>\n",
       "      <td>33511.029644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>ABEAHAMASHL_12_2021</td>\n",
       "      <td>34624.701924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>ABEAHAMASHL_1_2022</td>\n",
       "      <td>745801.630574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>ABEAHAMASHL_2_2022</td>\n",
       "      <td>34624.701924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1528 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Item_ID         Target\n",
       "0     YOSHTLYNYOSHZZ_11_2021   30149.691267\n",
       "1     YOSHTLYNYOSHZZ_12_2021   30149.691267\n",
       "2      YOSHTLYNYOSHZZ_1_2022   30149.691267\n",
       "3      YOSHTLYNYOSHZZ_2_2022   30149.691267\n",
       "4       YOSHRENECARL_11_2021   34624.701924\n",
       "...                      ...            ...\n",
       "1523    ABEENNEARMAZZ_2_2022  198208.985307\n",
       "1524     ABEAHAMASHL_11_2021   33511.029644\n",
       "1525     ABEAHAMASHL_12_2021   34624.701924\n",
       "1526      ABEAHAMASHL_1_2022  745801.630574\n",
       "1527      ABEAHAMASHL_2_2022   34624.701924\n",
       "\n",
       "[1528 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lxV6wYtqO84D",
    "outputId": "603a71b3-dc8e-4e8c-b9b0-b45eb0b87bed",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = cv_feature_importance_plot(train_data, val_data, folds, cv_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JnoxK49T7rQX"
   },
   "outputs": [],
   "source": [
    "def prepare_fold_data(dataset):\n",
    "    x, y, te = [], [], []\n",
    "    \n",
    "    for d in dataset:\n",
    "        x.append(d[0][0])\n",
    "        te.append(d[1])\n",
    "        \n",
    "        d[0][1]['target'] = 'target_'+d[0][1].groupby('sku_name').cumcount().astype(str)\n",
    "        # print(t[0][1],t[0][1].pivot_table(columns='target', index='sku_name', values ='sellin').reset_index())\n",
    "        y.append(d[0][1].pivot_table(columns='target', index='sku_name', values ='sellin').reset_index())\n",
    "        \n",
    "    features, targets = pd.concat(x, axis=0).sort_values(by=['sku_name','year','month']), pd.DataFrame(np.repeat(pd.concat(y, axis=0).values, LOOKBACK, axis=0), columns=pd.concat(y, axis=0).columns)\n",
    "    \n",
    "    test = pd.concat(te, axis=0)\n",
    "    return features, targets, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kQE5BI4rCP-V"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def train_model(train_data: list, val_data:list, folds: int, store_cv_models: bool=False):\n",
    "    cv_models = []\n",
    "    size = len(train_data)//folds\n",
    "\n",
    "    start_time = time()\n",
    "    for fold in range(folds):\n",
    "        print(f'Training fold {fold+1}')\n",
    "        print('\\n')\n",
    "        train_set = train_data[:(fold+1)*size]\n",
    "        val_set = [val_data[((fold+1)*size)-1]]\n",
    "        \n",
    "        x_train, y_train, _ = prepare_fold_data(train_set)\n",
    "        x_val, y_val, _ = prepare_fold_data(val_set)\n",
    "        \n",
    "        models = multi_step_train(params, x_train, y_train, x_val, y_val, store_cv_models, fold)\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Elapsed {:.2f} mins\".format((time() - start_time)/60))\n",
    "        print('-'*50)\n",
    "        print('\\n')\n",
    "              \n",
    "        del(x_train, y_train)\n",
    "        gc.collect()\n",
    "        cv_models.append(models)\n",
    "\n",
    "    return cv_models"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "fossil starter",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "099cda2b44d040b9b461c2107d3d9ff3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a0bf323f759488881cb36c6348ee711": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0abb324774c148eaa19fac4fb665fe9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ce3ea3d2f554ee4920633b27749cfd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9aeb9eb7e3649b78e206df552f0c78d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9f33d91db81c4f7084a5336821d56dc6",
      "value": " 53/53 [00:00&lt;00:00, 67.63it/s]"
     }
    },
    "0d771d3e5b064dae886a3e1a3eed17e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0ef75938ce8e4a68af5e1ecc7079639c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a0bf323f759488881cb36c6348ee711",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46f52779db1b4ecda445c105b6e7d939",
      "value": 53
     }
    },
    "1e131596ef834f13b0847b6a516c2064": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23225794fe3e491d9d6db3360b01eb3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b0d4258562746aa9e6bf280d3735b2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bcf2e19c7464341a5f0eb8eed8c9916": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b87b5635a1c4f20a39de50b42b97747",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c7fd994b956c42a28ddc0302aade887d",
      "value": "normalizing data: 100%"
     }
    },
    "3bd6fed385dd4b299aa56026411c783e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46f52779db1b4ecda445c105b6e7d939": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "48cc49781ad34644b06a70c602688976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48e0bcd890854a18b4710ba90d643830": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e0a23c6f6574573929267c26790f0e8",
       "IPY_MODEL_7585eee2183f4fa99749275bef109d60",
       "IPY_MODEL_d9ea1817d5e24f01a6b85ce8b7f0d697"
      ],
      "layout": "IPY_MODEL_b91173e97c4247c48c741d563ef17894"
     }
    },
    "4ba922a6711241b2991cfef8bbd3c5e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c44a3c1654745b6bc8db58ecdd99940": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0ba0ca588d64a15829ec04cc41ebf49",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8d382f3aec96437ea005b17f976f811c",
      "value": 53
     }
    },
    "50d6d46313cc4c26babac4203ec07c3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f849ad724954d64aba622d3799fd6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5f8adbd613c14b518b7571eea5612929": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61183c0aced54263988111244d5955e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6529afe4759e41629a9d25e0cb2c530a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6896dcc1086c407c9bb987d026a9b335": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72229050ac7b44a1a9d98d22671b84ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7585eee2183f4fa99749275bef109d60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba2f651c4e864397b50a669b0ae23198",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5f849ad724954d64aba622d3799fd6b1",
      "value": 53
     }
    },
    "7671bf9c19494a2b9f0ee5e2337f48c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a8c942666bc41dc86885e0cafe21f2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b87b5635a1c4f20a39de50b42b97747": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d382f3aec96437ea005b17f976f811c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "959bb1bd1291458c8e7a6b86f7ed6d2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9e0a23c6f6574573929267c26790f0e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8b9bbd32dfe4f569374b8b878f715c4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6529afe4759e41629a9d25e0cb2c530a",
      "value": "imputing missing values: 100%"
     }
    },
    "9f33d91db81c4f7084a5336821d56dc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a21dcb1036414c74af5cf15adcb102c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aab41a09b24d48dc83684dbacccf245f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_48cc49781ad34644b06a70c602688976",
      "value": " 53/53 [00:58&lt;00:00,  1.10s/it]"
     }
    },
    "a8b9bbd32dfe4f569374b8b878f715c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aab41a09b24d48dc83684dbacccf245f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aee2972fb8204b8c8e4b1cac5bb5aadf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bd6fed385dd4b299aa56026411c783e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7671bf9c19494a2b9f0ee5e2337f48c9",
      "value": "preparing sequences: 100%"
     }
    },
    "b91173e97c4247c48c741d563ef17894": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9aeb9eb7e3649b78e206df552f0c78d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba0f0f6acac149afb42db0931a177b90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_099cda2b44d040b9b461c2107d3d9ff3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e517d5baae4242cfbda65f877f0bba46",
      "value": " 53/53 [02:38&lt;00:00,  2.15s/it]"
     }
    },
    "ba2f651c4e864397b50a669b0ae23198": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0ba0ca588d64a15829ec04cc41ebf49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c632e524e100468885fd0d0f939ed3ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61183c0aced54263988111244d5955e8",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0d771d3e5b064dae886a3e1a3eed17e2",
      "value": 53
     }
    },
    "c7fd994b956c42a28ddc0302aade887d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "caf599e669e84e83bd02fe59add8593d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f0787d2ae1434df786ae30a5cad26238",
       "IPY_MODEL_4c44a3c1654745b6bc8db58ecdd99940",
       "IPY_MODEL_a21dcb1036414c74af5cf15adcb102c0"
      ],
      "layout": "IPY_MODEL_7a8c942666bc41dc86885e0cafe21f2a"
     }
    },
    "d7a120e280fa438482f09c7cd79abc89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9ea1817d5e24f01a6b85ce8b7f0d697": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e131596ef834f13b0847b6a516c2064",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6896dcc1086c407c9bb987d026a9b335",
      "value": " 53/53 [00:58&lt;00:00,  1.12s/it]"
     }
    },
    "dbca7372a9e64e02be2f4f1975b50e9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3bcf2e19c7464341a5f0eb8eed8c9916",
       "IPY_MODEL_0ef75938ce8e4a68af5e1ecc7079639c",
       "IPY_MODEL_0ce3ea3d2f554ee4920633b27749cfd6"
      ],
      "layout": "IPY_MODEL_e9a809c9294d40d2803d997bb6efc09b"
     }
    },
    "ddfc896a0bfb4981a8cce9e6ed1a094a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7a120e280fa438482f09c7cd79abc89",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4ba922a6711241b2991cfef8bbd3c5e8",
      "value": " 53/58 [01:17&lt;00:07,  1.41s/it]"
     }
    },
    "e2517ad8550645a786cfb47ef7c7b25d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23225794fe3e491d9d6db3360b01eb3e",
      "max": 58,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_959bb1bd1291458c8e7a6b86f7ed6d2e",
      "value": 53
     }
    },
    "e261b8adb1a248b9b616d3b5655061b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e359990d14324ff9adf74dd51ed53584": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aee2972fb8204b8c8e4b1cac5bb5aadf",
       "IPY_MODEL_c632e524e100468885fd0d0f939ed3ec",
       "IPY_MODEL_ba0f0f6acac149afb42db0931a177b90"
      ],
      "layout": "IPY_MODEL_5f8adbd613c14b518b7571eea5612929"
     }
    },
    "e517d5baae4242cfbda65f877f0bba46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9a809c9294d40d2803d997bb6efc09b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0787d2ae1434df786ae30a5cad26238": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e261b8adb1a248b9b616d3b5655061b9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0abb324774c148eaa19fac4fb665fe9b",
      "value": "imputing missing values: 100%"
     }
    },
    "f25671d37804416f9dc918126c34d515": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa725963d10448d3a1b49e160ebe7814",
       "IPY_MODEL_e2517ad8550645a786cfb47ef7c7b25d",
       "IPY_MODEL_ddfc896a0bfb4981a8cce9e6ed1a094a"
      ],
      "layout": "IPY_MODEL_72229050ac7b44a1a9d98d22671b84ee"
     }
    },
    "fa725963d10448d3a1b49e160ebe7814": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b0d4258562746aa9e6bf280d3735b2b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_50d6d46313cc4c26babac4203ec07c3d",
      "value": "retrieving walk forward indices:  91%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
